Abstract
Purpose
Accurate segmentation of lung and infection in COVID-19 computed tomography (CT) scans plays an important role in the quantitative management of patients. Most of the existing studies are based on large and private annotated datasets that are impractical to obtain from a single institution, especially when radiologists are busy fighting the coronavirus disease. Furthermore, it is hard to compare current COVID-19 CT segmentation methods as they are developed on different datasets, trained in different settings, and evaluated with different metrics.

Methods
To promote the development of data-efficient deep learning methods, in this paper, we built three benchmarks for lung and infection segmentation based on 70 annotated COVID-19 cases, which contain current active research areas, for example, few-shot learning, domain generalization, and knowledge transfer. For a fair comparison among different segmentation methods, we also provide standard training, validation and testing splits, evaluation metrics and, the corresponding code.

Results
Based on the state-of-the-art network, we provide more than 40 pretrained baseline models, which not only serve as out-of-the-box segmentation tools but also save computational time for researchers who are interested in COVID-19 lung and infection segmentation. We achieve average dice similarity coefficient (DSC) scores of 97.3%, 97.7%, and 67.3% and average normalized surface dice (NSD) scores of 90.6%, 91.4%, and 70.0% for left lung, right lung, and infection, respectively.

Conclusions
To the best of our knowledge, this work presents the first data-efficient learning benchmark for medical image segmentation, and the largest number of pretrained models up to now. All these resources are publicly available, and our work lays the foundation for promoting the development of deep learning methods for efficient COVID-19 CT segmentation with limited data.

1 INTRODUCTION
Coronavirus disease 2019 (COVID-19 ) has spread all over the world during the past few months and caused over 61 000 000 people infected as of November 30, 2020 according to WHO statistics* . Computed tomography (CT) is playing an important role in the fight against COVID-19.1-3 Computed tomography is shown to be more sensitive in the early diagnosis of COVID-19 infection compared to reverse transcription-polymerase chain reaction (RT-PCR) tests.4 Wang et al. trained a deep learning model on 325 COVID-19 CT scans and 740 typical pneumonia scans. Their model can identify 46 COVID-19 cases that were previously missed by the RT-PCR test.5 Furthermore, quantitative information from CT images, such as the lung burden, the percentage of high opacity, and the lung severity score, can be used to monitor the disease progression and help us understand the course of COVID-19.6, 7

Artificial Intelligence (AI) methods, especially deep learning-based methods, have been widely applied in medical image analysis to combat COVID-19.8 For example, AI can be used for building a contactless imaging workflow to prevent transmission from patients to health care providers.9 In addition, most screening and segmentation algorithms for COVID-19 are developed with deep learning models, and the automatic diagnosis and COVID-19 infection quantification systems usually rely on the segmentation results generated by deep neural networks.10-13

Although several studies show that deep learning methods have potential for providing accurate and quantitative assessment of COVID-19 infection in CT images,8 the solutions mainly rely on large private datasets. Due to the patient privacy and intellectual property issues, the datasets and solutions may not be publicly available. However, researchers may hope that the datasets, entire source code and trained models could be provided by authors.14

Existing studies demonstrate that the classical U-Net (or V-Net) can achieve promising segmentation performance if hundreds of well-labeled training cases are available.12, 15 83.1% to 91.6% of segmentation performance in Dice score coefficient was reported in various U-Net-based approaches on different private datasets. Shan et al. developed a neural network based on V-Net and the bottle-neck structure to segment and quantify infection regions.15, 16 In their human-in-the-loop strategy, they achieved 85.1%, 91.0%, and 91.6% dice with 36, 114, and 249 labeled CT scans, respectively. Huang et al.12 employed U-Net17 for lung and infection using an annotated dataset of 774 cases, and demonstrated that the trained model could be used for quantifying the disease burden and monitoring disease progressions or treatment responses. In general, we have observed a trend that training deep learning models with more annotations will decrease the time needed for contouring the infection, and increase segmentation accuracy, which is consistent with the Shan et al.'s findings.15 However, annotations for 3D CT volume data are expensive to acquire because it not only relies on professional diagnosis knowledge of radiologists but also takes much time and labor, especially in current situation. Thus, a critical question would be:

How can we automatically annotate COVID-19 CT scans with limited training data?

Toward this question, three basic but important problems remain unsolved:
There is no publicly well-labeled COVID-19 CT 3D dataset; Data collection is the first and essential step to develop deep learning methods for COVID-19 segmentation. Most of the existing studies rely on large private dataset with hundreds of annotated CT scans.
There are no public benchmarks to evaluate different deep learning-based solutions, and different studies often use various evaluation metrics. Similarly, datasets for training, validating, and testing are split diversely, which also makes it hard for readers to compare those methods. For example, although MedSeg (http://medicalsegmentation.com/covid19/) dataset with 100 annotated slices were used in Refs. [18-21] and [22] to develop COVID-19 segmentation methods, it was split in different ways and the developed methods were evaluated by different metrics.
There are no publicly available trained baseline models for COVID-19. U-Net, a well-known segmentation architecture, is commonly used as a baseline network in many studies. However, due to different implementations, the reported performance varies in different studies,19, 22 even though the experiments are conducted on the same dataset.
In this paper, we focus on annotation-efficient deep learning solutions and aim to alleviate the above problems by providing a well-labeled COVID-19 CT dataset and a benchmark. In particular, we first provide a COVID-19 3D CT dataset with left lung, right lung, and infection annotations, and then establish three benchmark tasks to explore different deep learning strategies with limited training cases. Finally, we build comprehensive baselines for each task based on U-Net.

Our tasks target on three popular research fields in medical imaging community:
Few-shot learning: building high performance models from very few samples. Although most existing approaches focus on natural images, it has received growing attention in medical imaging because generating labels for medical images is much more difficult.23, 24
Domain generalization: learning from known domains and applying to an unknown target domain that has different data distribution.25 The ultimate goal of domain generalization is to train robust models that are generalizable to new unseen domains. Recently, model-agnostic learning has been an emerging research topic to achieve this goal.26
Knowledge transfer: reusing existing annotations to boost the training/fine-tuning on a new dataset or a related new task. In contrast to domain generalization, both the source domain/task and the target domain/task are known, and we focus on storing and reusing knowledge gained from the source domain/task. This task has achieved significant attention in recent studies, such as using transfer learning or generative adversarial network to transfer knowledge from publicly available annotated datasets to new datasets27, 28 in segmentation tasks.
Large-scale data remedies of lung and infection segmentation have been well studied.12, 13 In this paper, we focus on small-data learning tasks because it is a more practical problem and large-scale annotated datasets are expensive and time-consuming to collect. Moreover, the goal is to lay the foundation for important machine learning tasks when only limited cases are available. Designing novel methods is beyond the scope of this paper. Our contributions can be summarized as follows.
We present a new COVID-19 CT datasets and the left lung, the right lung, and the infections are well annotated by chest radiologists.
We setup three benchmark tasks to promote the studies on data-efficient deep learning for COVID-19 CT scans segmentation. Specifically, we focus on few-shot learning, domain generalization, and knowledge transfer that are also current research hotspots. To the best of our knowledge, this is the first data-efficient learning benchmark in medical image segmentation.
We provide 40+ trained state-of-the-art models and corresponding segmentation results are publicly available, which can serve as strong baselines. More importantly, these trained models can be used as out-of-the-box tools for COVID-19 CT lung and infection segmentation, which could reduce the annotation time for radiologists.
2 MATERIALS
Annotations of COVID-19 CT scans are scarce, but several lung CT annotations with other diseases are publicly available. Thus, one of the main goals of our benchmark is to explore whether it is possible for using these existing annotations to assist COVID-19 CT segmentation. This section introduces the public datasets used in our segmentation benchmarks. Figure 1 presents some examples from each dataset.

Details are in the caption following the image
Fig. 1
Open in figure viewer
PowerPoint
Examples of five lung computed tomography (CT) datasets. The first and second row denote original noncontrast CT images and corresponding ground truth of lung and lesions, respectively. The third row shows the three-dimensional rendering results of ground truth. MSD Lung Tumor and MosMed datasets (third column) do not provide lung masks. The red, green, and blue colors denote left lung, right lung, and lung lesions, respectively. The blue legend in the third rows stands for different lung lesion types. [Color figure can be viewed at wileyonlinelibrary.com]
2.A Existing lung CT segmentation datasets
2.A.1 StructSeg lung organ segmentation
Fifty lung cancer patient CT scans are accessible, and all the cases are from one medical center. This dataset served as a segmentation challenge† during MICCAI 2019. Six organs are annotated, including left lung, right lung, spinal cord, esophagus, heart, and trachea. In this paper, we only use the left lung and right lung annotations.

2.A.2 NSCLC left and right lung segmentation
This dataset consists of left and right thoracic volume segmentations delineated on 402 CT scans from The Cancer Imaging Archive NSCLC Radiomics.29, 30

2.B Existing lung lesion CT segmentation datasets
2.B.1 MSD lung tumor segmentation
This dataset is comprised of patients with nonsmall cell lung cancer (NSCLC) from Stanford University (Palo Alto, CA, USA) publicly available through TCIA. The dataset served as a segmentation challenge‡ during MICCAI 2018. The tumor is annotated by an expert thoracic radiologist, and 63 labeled CT scans are available.

2.B.2 StructSeg gross target volume segmentation of lung cancer
The same 50 lung cancer patient CT scans as the above StructSeg lung organ segmentation dataset are provided, and gross target volumes of tumors are annotated in each case.

2.B.3 NSCLC pleural effusion (PE) segmentation
The CT scans in this dataset are the same as those in NSCLC left and right lung segmentation dataset, while pleural effusion is delineated for 78 cases.29-31

2.B.4 MosMed dataset
This dataset contains 50 annotated COVID-19 CT scans that are provided by municipal hospitals in Moscow, Russia.32 To evaluate the generalization ability of deep learning models, we used this dataset as an independent testing set in following benchmark settings.

2.C Our COVID-19-CT-Seg dataset
We collected 20 public COVID-19 CT scans from the Coronacases Initiative and Radiopaedia, which can be freely downloaded§ with CC BY-NC-SA license. All the cases contain COVID-19 infections. The proportion of infections in the lungs ranges from 0.01% to 59%. The left lung, right lung, and infection| were firstly delineated by junior annotators with 1–5 yr experience, then refined by two radiologists with 5–10 yr experience, and finally all the annotations were verified and refined by a senior radiologist with more than 10 yr experience in chest radiology. The whole lung mask includes both normal and pathological regions. All the annotations were manually performed by ITK-SNAP in a slice-by-slice manner on axial images. On average, it takes about 400 ± 45 min to delineate one CT scan with 250 slices. There are totally 300+ infections with 1800+ slices. We have made all the annotations publicly available33 at https://zenodo.org/record/3757476 with CC BY-NC-SA license.

3 METHODS
As mentioned in Section 1, there is a need for innovative strategies that enable data-efficient methods for COVID-CT segmentation. Thus, we setup three tasks to evaluate potential annotation-efficient strategies. In particular, we focus on learning to segment left lung, right lung, and infection in COVID-19 CT scans using
pure but limited COVID-19 CT scans;
existing annotated lung CT scans from other non-COVID-19 lung diseases;
heterogeneous datasets include both COVID-19 and non-COVID-19 CT scans.
Furthermore, we also provide unified data (training, validation, and testing) splits, experimental settings, and evaluation metrics to standardize the deep learning-based segmentation protocols that can enable fair comparisons between different studies.

3.A Task 1: Learning with limited annotations
Task 1 (Table I) is designed to address the problem of few-shot learning, where few annotations are available for training. This task is based on the COVID-19-CT-Seg dataset only. It contains three subtasks aiming to segment lung, infection and both of them, respectively. For each subtask, fivefold cross-validation results (based on a predefined dataset split file) are reported. In each fold, four training cases are used for training and the rest 16 cases are used for validation. Moreover, MosMed dataset is used as an independent testing set.

Table I. Experimental settings of Task 1 (Learning with limited annotations) for lung and infection segmentation in COVID-19 CT scans. All the experiments are base on the COVID-19-CT-Seg dataset. (Number) denotes the number of cases in the dataset.
Seg. task	Training and validation	Testing
Lung	Fivefold cross validation	MosMed (50)
Infection	4 cases (20% for training)
Lung and infection	16 cases (80% for validation)
3.B Task 2: Learning to segment COVID-19 CT scans from non-COVID-19 CT scans
Task 2 (Table II) is designed to address the problem of domain generalization, where only out-of-domain data (non-COVID-19 datasets) are available for training. Specifically, in the first subtask, the StructSeg Lung dataset and the NSCLC Lung dataset are used for training. In the second subtask, the MSD Lung Tumor, the StructSeg Gross Target and the NSCLC Pleural Effusion datasets are used as training sets. For both subtasks, 80% of the data are randomly selected for training and the rest 20% are held-out as in-domain testing sets. All cases in two labeled COVID-19 CT datasets are kept for testing.

Table II. Experimental settings of Task 2 (Learning to segment COVID-19 CT scans from non-COVID-19 CT scans) for lung and infection segmentation in COVID-19 CT scans. (Number) denotes the number of cases in the dataset.
Seg. task	Training	In-domain testing	(Unseen) testing 1	(Unseen) testing 2
Lung	StructSeg lung (40)	StructSeg lung (10)	COVID-CT-Seg lung (20)	–
NSCLC lung (322)	NSCLC lung (80)
Infection	MSD lung tumor (51)	MSD lung tumor (12)	COVID-CT-Seg infection (20)	MosMed (50)
StructSeg gross target (40)	StructSeg gross target (10)	
NSCLC pleural effusion (62)	NSCLC pleural effusion (16)	
3.C Task 3: Learning with both COVID-19 and non-COVID-19 CT scans
Task 3 (Table III) is designed to address the problem of knowledge transfer with heterogeneous datasets, where both in-domain and out-of-domain data are included in the training set. Specifically, in both subtasks (lung segmentation and lung infection segmentation), 80% non-COVID-19 data and 20% COVID-19 data are used for training, while remained 20% and 80% data are used for validation and testing, respectively. Besides, MosMed dataset is used as an additional testing set.

Table III. Experimental settings of Task 3 (Learning with both COVID-19 and non-COVID-19 CT scans) for lung and infection segmentation in COVID-19 CT scans. (Number) denotes the number of cases in the dataset.
Seg. task	Training	Validation	Testing 1	Testing 2
Lung	StructSeg lung (40)	COVID-CT-Seg	StructSeg lung (10)	COVID-CT-Seg	–
NSCLC lung (322)	Lung (4)	NSCLC lung (80)	Lung (16)
Infection	MSD lung tumor (51)	COVID-CT-Seg	MSD lung tumor (12)	COVID-CT-Seg	MosMed (50)
StructSeg gross target (40)	Infection (4)	StructSeg gross target (10)	Infection (16)
NSCLC pleural effusion (62)		NSCLC pleural effusion (16)	
3.D Evaluation metrics
Motivated by the evaluation methods of the well-known medical image segmentation decathlon,¶ we also employ two complementary metrics to evaluate the segmentation performance. Dice similarity coefficient (DSC), a region-based measure, is used to evaluate the region overlap. Normalized surface Dice (NSD),34 a boundary-based measure is used to evaluate how close the segmentation and ground truth surfaces are to each other at a specified tolerance urn:x-wiley:00942405:media:mp14676:mp14676-math-0001. For both two metrics, higher scores admit better segmentation performance, and 100% means perfect segmentation. Let G, S denote the ground truth and the segmentation result, respectively. We formulate the definitions of the two measures as follows:

3.D.1 Region-based measure
urn:x-wiley:00942405:media:mp14676:mp14676-math-0002
3.D.2 Boundary-based measure
urn:x-wiley:00942405:media:mp14676:mp14676-math-0003
where urn:x-wiley:00942405:media:mp14676:mp14676-math-0004 denote the border region of ground truth and segmentation surface at tolerance urn:x-wiley:00942405:media:mp14676:mp14676-math-0005, which are defined as urn:x-wiley:00942405:media:mp14676:mp14676-math-0006 and urn:x-wiley:00942405:media:mp14676:mp14676-math-0007, respectively. We set tolerance urn:x-wiley:00942405:media:mp14676:mp14676-math-0008 as 1 and 3 mm for lung segmentation and infection segmentation, respectively. The tolerance is computed by measuring the inter-rater segmentation variation between two different radiologists, which is also in accordance with another independent study.34 Python implementations of the two metrics are publicly available.**

The main benefit of introducing Surface Dice is that it ignores small boundary deviations because small interobserver errors are also unavoidable and often not clinically relevant when segmenting the objects by radiologists.

3.E U-Net baselines: Oldies but goldies
U-Net (17, 35) has been proposed for 5 yr, and many variants have been proposed to improve it. However, recent study36 demonstrates that it is still hard to surpass a basic U-Net if the corresponding pipeline is designed adequately. In particular, nnU-Net (no-new-U-Net)36 was proposed to automatically adapt preprocessing strategies and network architectures (i.e., the number of pooling, convolutional kernel size, and stride size) to a given 3D medical dataset. Without manual tuning, nnU-Net can achieve better performance than most specialised deep learning pipelines in 19 public international segmentation competitions and set a new state-of-the-art in the majority of 49 tasks. The source code is publicly available at https://github.com/MIC-DKFZ/nnUNet.

U-Net is often used as a baseline model in existing COVID-19 CT segmentation studies. However, reported results vary a lot even in the same dataset, which make it hard to compare different studies. To standardize the U-Net performance, we build our baselines on nnU-Net that is the most powerful U-Net implementation to the best of our knowledge. To make it comparable between different tasks, we manually adjust the patch sizes and network architectures in Task 2 and Task 3 to be the same as Task 1. Figure 2 shows details of the U-Net architecture.

Details are in the caption following the image
Fig. 2
Open in figure viewer
PowerPoint
Details of the 3D U-Net architecture that is used in this work. The numbers (e.g., 32 × 56 × 160 × 92) near convolutional blocks denote feature map size in each resolution, and the numbers in white rectangles (e.g., 1, 2, 2) denote stride sizes of convolutional kernels. [Color figure can be viewed at wileyonlinelibrary.com]
During preprocessing, we apply Z-score (mean subtraction and division by standard deviation) to normalize the image intensities. During training, we use the standard training scheme of nnU-Net. For example, the sum between cross entropy and Dice loss is used as the loss function. The optimizer is stochastic gradient descent with initial learning rate (0.01) and a large nesterov momentum (0.99) and, “PolyLR” schedule37 is used to reduce the learning rate. We randomly sample image patches with size 192 × 192 × 64. All training procedures run for a fixed length of 1000 epochs, where each epoch is defined as 250 training iterations (batch size 2). During testing, we use the same patch size with sliding window to infer testing cases and the sliding stride is half the patch size.††

4 RESULTS AND DISCUSSION
This section presents the quantitative segmentation results in each task. For clarity, the first three subsections show the segmentation results on the validation set and the first testing set for each task. We summarize all the segmentation results on the second testing set (MosMed dataset) and compare the results of three tasks in the last subsection.

4.A Results of task 1: Learning with limited annotations
Table IV presents average DSC and NSD results of lung and infection of each subtask in Task 1. It can be found that
the average DSC and NSD values among different folds vary greatly. This is because the testing cases in each fold have different degrees of difficulty, which demonstrates that reporting fivefold cross-validation results is necessary to obtain an objective evaluation as onefold results may be biased.
promising results for left and right lung segmentation in COVID-19 CT scans can be achieved with as few as four training cases. Models trained for segmenting lung obtain significantly better results compared with those trained for segmenting lung and infection simultaneously.
there is still large room for improving infection segmentation with limited annotations.
Table IV. Quantitative Results of fivefold cross validation on COVID-19-CT-Seg dataset for Task 1: Learning with limited annotations. For each fold, average DSC and NSD values are reported. The last row shows the average results of 80 (=5 folds × 16 testing cases per fold) testing cases. The bold numbers are the best results.
Subtask	Lung	Infection	Lung and infection union segmentation
Left lung	Right lung	Left lung	Right lung	Infection
DSC (%)	NSD (%)	DSC (%)	NSD (%)	DSC (%)	NSD (%)	DSC (%)	NSD (%)	DSC (%)	NSD (%)	DSC (%)	NSD (%)
Fold-0	84.9 ± 8.2	68.7 ± 13.3	85.2 ± 13.0	70.6 ± 15.8	68.1 ± 20.5	70.9 ± 21.3	50.5 ± 30.4	36.9 ± 19.6	64.8 ± 18.9	47.1 ± 13.8	66.5 ± 23.4	68.7 ± 22.5
Fold-1	80.3 ± 14.5	61.8 ± 15.1	83.9 ± 9.6	68.3 ± 9.0	71.3 ± 20.5	71.8 ± 23.0	40.3 ± 18.7	27.5 ± 12.0	60.1 ± 11.1	41.7 ± 9.9	64.7 ± 21.8	60.6 ± 25.1
Fold-2	87.1 ± 12.1	74.3 ± 16.0	90.3 ± 8.2	78.5 ± 12.0	66.2 ± 21.7	71.7 ± 24.2	80.3 ± 18.8	66.8 ± 18.8	85.2 ± 12.4	68.6 ± 15.1	60.7 ± 27.6	62.5 ± 28.9
Fold-3	88.4 ± 7.0	75.2 ± 8.8	89.9 ± 6.3	78.5 ± 8.0	68.1 ± 23.1	70.8 ± 27.1	79.7 ± 13.6	65.4 ± 14.4	84.0 ± 9.8	67.7 ± 13.0	62.0 ± 27.9	65.3 ± 28.9
Fold-4	88.3 ± 7.6	75.8 ± 11.0	90.2 ± 7.0	78.3 ± 10.2	62.7 ± 26.9	64.9 ± 28.2	72.4 ± 21.1	58.6 ± 20.8	80.9 ± 13.4	63.4 ± 15.9	51.4 ± 30.2	51.9 ± 31.0
Avg	85.8 ± 10.5	71.2 ± 13.8	87.9 ± 9.3	74.8 ± 11.9	67.3 ± 22.3	70.0 ± 24.4	64.6 ± 26.4	51.1 ± 23.4	75.0 ± 16.8	57.7 ± 17.4	61.0 ± 26.2	61.8 ± 27.4
Figure 3 presents some visualized segmentation results in Task 1. It can be found that the separate training manner yields better results, especially for the left lung and right lung segmentation. The union training manner could confuse left and right lung, adversely affecting the infection segmentation. This is because multitask segmentation is much harder than single task, especially when each separate task is challenging.

Details are in the caption following the image
Fig. 3
Open in figure viewer
PowerPoint
Visualized examples of segmentation results in Task 1. Task1-Separate means the results of training separate networks for lung and infection segmentation. Task1-Union means training a single model for both lung and infection segmentation. The red, green, and blue colors denote the left lung, the right lung, and the infection, respectively. [Color figure can be viewed at wileyonlinelibrary.com]
4.B Results of task 2: Learning to segment COVID-19 CT scans from non-COVID-19 CT scans
This task is quite challenging as the model does not see any cases from target domain during training. In other words, the trained models are expected to generalize to the unseen domain (COVID-19 CT). Table V shows left lung and right lung segmentation results in terms of average and standard deviation values of DSC and NSD. It can be found that
3D U-Net achieves excellent performance in terms of DSC on the in-domain set. Average NSD values are lower than DSC values, implying that most of the errors come from the boundaries.
the performance on the testing set drops significantly on both subtasks. The performance of the model trained on NSCLC Lung dataset is worse than the model trained on StuctSeg lung. The potential reason could be the difference in the distribution lung appearance is smaller between StructSeg and COVID-19-CT.
Table V. Quantitative results (mean ± standard deviation) of Lung segmentation in Task 2. The bold numbers are the best results.
Subtask	In-domain testing set	(Unseen) testing set 1
Left lung	Right lung	Left lung	Right lung
DSC (%)	NSD (%)	DSC (%)	NSD (%)	DSC (%)	NSD (%)	DSC (%)	NSD (%)
StructSeg lung	96.4 ± 1.4	74.6 ± 9.1	97.3 ± 0.3	74.3 ± 7.2	92.2 ± 19.7	82.0 ± 15.7	95.5 ± 7.2	84.2 ± 11.6
NSCLC lung	95.3 ± 4.9	80.2 ± 8.3	95.4 ± 10.9	80.7 ± 10.7	57.5 ± 21.5	46.9 ± 16.9	72.2 ± 15.3	51.7 ± 16.8
Table VI shows quantitative infection segmentation results in terms of average and standard deviation values of DSC and NSD. It can be found that
on the in-domain testing set, the performance of lesion segmentation is not as good as the performance of lung segmentation (Table V), which means that tumor segmentation remains a challenging problem. This observation is in line with recent results in MICCAI tumor segmentation challenge, that is, liver tumor segmentation38 and kidney tumor segmentation.39
the models almost fail to predict COVID-19 infections on testing set, which highlights that the lesion appearances differ significantly among lung cancer, pleural effusion, and COVID-19 infections in CT scans.
Table VI. Quantitative results (mean ± standard deviation) of Infection segmentation in Task2. The bold numbers are the best results.
Subtask	In-domain testing set	(Unseen) testing set 1
DSC (%)	NSD (%	DSC (%)	NSD (%)
MSD lung tumor	67.2 ± 27.1	77.1 ± 31.4	25.2 ± 27.4	26.0 ± 28.5
StructSeg tumor	71.3 ± 29.6	70.3 ± 29.5	6.0 ± 12.7	5.5 ± 10.7
NSCLC-PE	64.4 ± 45.5	73.7 ± 12.9	0.4 ± 0.8	3.7 ± 4.8
4.C Results of task 3: Learning with both COVID-19 and non-COVID-19 CT scans
In Task 3, heterogeneous cases from both COVID-19 and non-COVID-19 datasets are leveraged to train for segmenting lung and infections on COVID-19 CT scans. Due to the gap between multiple domains, this data fusion is expected to explore how fusing different annotations influences the model's performance on each individual dataset separately.

Table VII presents quantitative fivefold cross-validation results of left lung and right lung segmentation in terms of average DSC and NSD. It can be found that
on the validation set, the average DSC and NSD values are basically consistent with the results in Task 2, achieving high performance with up to 96.4% in DSC for left lung segmentation and 97.2% in DSC for right lung segmentation.
on the testing set, however, the average DSC and NSD values slightly drop, indicating that though segmenting the same organ lung on CT scans, there still exists some domain gaps between non-COVID-19 and COVID-19 CT datasets.
Table VII. Quantitative Results of fivefold cross validation of left lung and right lung segmentation in Task 3. The bold numbers are the best results.
Subtask	Validation set	Testing set 1
Left lung	Right lung	Left lung	Right lung
DSC (%)	NSD (%)	DSC (%)	NSD (%)	DSC (%)	NSD (%)	DSC (%)	NSD (%)
StructSeg	Fold-0	96.3 ± 0.1	79.9 ± 8.5	97.2 ± 0.4	73.9 ± 7.0	97.4 ± 1.9	97.6 ± 2.0	90.3 ± 5.9	90.8 ± 6.1
Fold-1	96.3 ± 1.2	73.7 ± 8.4	97.1 ± 0.4	73.4 ± 7.0	97.7 ± 1.3	91.0 ± 5.3	98.0 ± 1.1	91.8 ± 4.9
Fold-2	96.4 ± 1.3	74.3 ± 8.5	97.2 ± 0.3	74.0 ± 6.9	96.8 ± 3.1	89.4 ± 8.8	97.6 ± 2.8	90.9 ± 7.8
Fold-3	96.3 ± 1.2	73.9 ± 8.5	97.2 ± 0.3	73.9 ± 7.0	96.9 ± 2.5	90.7 ± 5.6	97.3 ± 2.5	91.3 ± 6.3
Fold-4	96.3 ± 1.3	73.8 ± 8.9	97.2 ± 0.4	73.8 ± 7.3	97.8 ± 1.3	91.6 ± 5.3	98.0 ± 1.3	92.0 ± 5.7
Avg	96.3 ± 1.2	73.9 ± 8.2	97.2 ± 0.3	73.8 ± 6.7	97.3 ± 2.1	90.6 ± 6.2	97.7 ± 2.1	91.4 ± 6.1
NSCL	Fold-0	95.7 ± 4.6	81.2 ± 7.5	95.5 ± 10.9	81.0 ± 10.9	92.7 ± 6.3	75.4 ± 14.5	93.0 ± 7.0	85.3 ± 16.0
Fold-1	95.4 ± 5.0	80.5 ± 8.7	95.2 ± 11.1	80.5 ± 11.4	92.2 ± 7.2	73.6 ± 17.5	94.3 ± 3.8	76.7 ± 14.3
Fold-2	95.4 ± 4.9	79.7 ± 8.2	95.2 ± 10.9	80.0 ± 11.3	94.1 ± 4.1	77.4 ± 12.0	93.8 ± 5.8	75.6 ± 16.2
Fold-3	95.4 ± 4.9	79.8 ± 7.7	94.8 ± 11.2	79.5 ± 11.6	93.6 ± 5.1	77.9 ± 11.6	93.6 ± 5.9	78.2 ± 13.3
Fold-4	95.7 ± 4.6	81.1 ± 7.9	95.5 ± 10.9	80.9 ± 11.0	94.8 ± 3.8	80.5 ± 10.5	95.1 ± 3.3	80.5 ± 11.1
Avg	95.5 ± 4.8	80.5 ± 8.0	95.2 ± 10.9	80.4 ± 11.2	93.5 ± 5.4	76.9 ± 13.3	94.0 ± 5.3	77.2 ± 14.1
Table VIII present quantitative fivefold cross-validation results of the infection segmentation, it can be found that
even large amounts of lung lesion annotations from non-COVID-19 dataset are used, the variance among the results of fivefold cross-validation is obvious. Thus, reporting fivefold cross-validation results is necessary in this task for reliable and robust evaluation.
compared with the results in Task 2 (Table VI), including four COVID-19 cases bring remarkable improvements with up to 7.5% in DSC for StructSeg tumor segmentation, and 1.1% in DSC for NSCLC pleural effusion segmentation, while the performance drops up to 3.3% in DSC for MSD lung tumor segmentation. These results imply that including few out-of-domain cases in a training set can lead to significant change to the model's performance.
on the testing set, the relative performance drops about 4%–14% in DSC and 11%–16% in NSD, indicating that simply fusing both COVID-19 and non-COVID-19 cases is still inefficient to segment infections on COVID-19 CT scans. Therefore, there remains much scope for improvement to bridge the gap among many non-COVID-19 lung lesion cases and limited COVID-19 cases by advanced knowledge transfer techniques.
Table VIII. Quantitative Results of fivefold cross validation of infection segmentation in Task 3. The bold numbers are the best results.
Subtask	Validation set	Testing set 1
DSC (%)	NSD (%)	DSC (%)	NSD (%)
MSD lung tumor	Fold-0	67.2 ± 26.7	78.1 ± 30.8	68.0 ± 22.5	66.6 ± 23.7
Fold-1	66.3 ± 26.1	76.9 ± 29.6	67.0 ± 22.0	65.1 ± 25.9
Fold-2	67.1 ± 25.4	77.4 ± 27.8	63.0 ± 27.9	64.4 ± 28.7
Fold-3	63.9 ± 26.6	73.8 ± 31.1	61.7 ± 24.5	59.8 ± 28.5
Fold-4	68.0 ± 25.9	78.8 ± 29.9	51.9 ± 30.6	50.6 ± 30.9
Avg	66.5 ± 25.3	77.0 ± 28.9	62.3 ± 25.7	61.3 ± 27.6
StructSeg tumor	Fold-0	78.2 ± 14.1	75.4 ± 17.5	69.3 ± 20.5	68.0 ± 21.8
Fold-1	78.6 ± 14.0	76.1 ± 18.1	68.3 ± 22.4	64.8 ± 26.3
Fold-2	77.0 ± 13.7	73.0 ± 17.0	63.6 ± 25.4	66.1 ± 25.5
Fold-3	78.8 ± 13.6	76.0 ± 17.5	67.0 ± 24.1	66.4 ± 25.2
Fold-4	77.5 ± 13.7	74.6 ± 18.4	52.6 ± 28.7	51.2 ± 28.5
Avg	78.0 ± 13.3	75.0 ± 17.0	64.2 ± 24.5	63.3 ± 25.7
NSCLC	Fold-0	65.5 ± 15.4	74.3 ± 13.2	69.2 ± 20.7	66.5 ± 22.4
Fold-1	64.7 ± 15.4	73.8 ± 14.3	59.7 ± 22.7	55.8 ± 25.5
Fold-2	65.5 ± 15.2	74.8 ± 13.7	61.6 ± 28.2	61.7 ± 29.1
Fold-3	64.7 ± 16.0	74.0 ± 13.6	62.7 ± 25.7	62.0 ± 27.7
Fold-4	65.2 ± 15.6	74.7 ± 13.0	47.7 ± 27.2	46.5 ± 27.0
Avg	65.1 ± 15.2	74.3 ± 13.2	60.2 ± 25.4	58.5 ± 26.7
4.D Comparison among different tasks
Tasks 1–3 correspond to different strategies for lung and infection segmentation in COVID-19 CT scans with limited in-domain training cases and out-of-domain datasets. The testing cases are the same in Tasks 1–3, so it is feasible and reasonable to conduct comparison among different tasks. Table IX presents quantitative results on all testing sets of the three tasks in terms of average DSC and NSD. Task 1-Separate and -Union denote training the network to segment lung and infection separately and simultaneously, respectively. Figure 4 shows the violin plots of left lung, right lung, and infection segmentation results on COVID-19-CT-Seg dataset in terms of DSC and NSD for all tasks. The violin plot shows not only the summary statistics such as median and interquartile ranges but also the entire distribution of the quantitative results.

Table IX. Quantitative comparison of COVID-19 CT lung and infection segmentation results among different tasks on testing set in terms of average DSC and NSD values of all testing cases.The bold numbers are the best results.
Subtask	Left lung	Right lung	Infection (COVID-19-CT-Seg)	Infection (MosMed)
DSC (%)	NSD (%)	DSC (%)	NSD (%)	DSC (%)	NSD (%)	DSC (%)	NSD (%)
Task1-Separate	85.8 ± 10.5	71.2 ± 13.8	87.9 ± 9.3	74.8 ± 11.9	67.3 ± 22.3	70.0 ± 24.4	58.8 ± 20.6	66.4 ± 20.3
Task1-Union	64.6 ± 26.4	51.1 ± 23.4	75.0 ± 16.8	57.7 ± 17.4	61.0 ± 26.2	61.8 ± 27.4	48.2 ± 22.1	41.4 ± 19.1
Task2-MSD	–	–	25.2 ± 27.4	26.0 ± 28.5	16.2 ± 23.2	17.5 ± 23.4
Task2-StructSeg	92.2 ± 19.7	82.0 ± 15.7	95.5 ± 7.2	84.2 ± 11.6	6.0 ± 12.7	5.5 ± 10.7	2.6 ± 9.5	3.3 ± 9.9
Task2-NSCLC	57.5 ± 21.5	46.9 ± 17.0	72.2 ± 15.3	51.7 ± 16.8	0.4 ± 0.9	3.7 ± 4.8	0.0 ± 0.0	0.5 ± 1.4
Task3-MSD	–	–	62.3 ± 25.7	61.3 ± 27.6	39.2 ± 30.6	41.3 ± 30.5
Task3-StructSeg	97.3 ± 2.1	90.6 ± 6.2	97.7 ± 2.1	91.4 ± 6.1	64.2 ± 24.5	63.3 ± 25.7	44.3 ± 25.3	49.1 ± 25.8
Task3-NSCLC	93.5 ± 5.4	76.9 ± 13.3	94.0 ± 5.3	77.2 ± 14.1	60.2 ± 25.4	58.5 ± 26.7	30.1 ± 26.7	33.4 ± 27.1
Details are in the caption following the image
Fig. 4
Open in figure viewer
PowerPoint
The violin plots present the performances (DSC or NSD) of different methods for left lung, right lung, and infection segmentation on COVID-19-CT-Seg testing set. [Color figure can be viewed at wileyonlinelibrary.com]
For lung segmentation,
Task 3 achieves the best performance reaching up to 97.3% in DSC and 90.6% in NSD for left lung segmentation, and 97.7% in DSC and 91.4% in NSD for right lung segmentation.
comparison of the results between Task 1 and Task 2, StructSeg achieves significantly better performance than NSCLC on the unseen testing set, indicating that the domain gap of lung between StructSeg and COVID-19 CT is smaller. The results on NSCLC dataset with 300+ training cases are worse than the results on COVID-19 dataset with four training cases, indicating that the number of training cases is not the most important while including the in-domain data during learning process is much more important.
comparison of the results between Task 1 and Task 3, adding both out-of-domain datasets (StructSeg and NSCLC) can boost performance of left and right lung segmentation. The results imply that existing non-COVID CT annotations can be used to assist lung segmentation in COVID-19 CT scans. This finding is very encouraging for fast developing a COVID-19 lung segmentation system with limited data especially COVID-19 CT annotations are scarce at present.
comparison of the results between Task 2 and Task 3, adding COVID-19 cases into training set can obtain performance gains on both subtasks (StructSeg and NSCLC), especially for NSCLC that achieves a significant increase of up to 34% in DSC for left lung segmentation. This result highlights that including few COVID-19 cases in training set is critical for lung segmentation.
For infection segmentation,
Task 1 achieves the best performance in both two testing set reaching up to 67.3% in DSC and 70.0% in NSD.
comparison of the results between Task 1 and Task 2, using four COVID-19 CT training cases obtains significant better results than using other lung lesion cases (i.e., lung cancer and pleural effusion), which highlights the importance of in-domain data when developing the COVID-19 infection segmentation system.
comparison of the results between Task 1 and Task 3, adding many (40–62) non-COVID-19 cases during training degrades instead of increasing the performance, implying that out-of-domain lung lesion data can bias the model's representation ability for COVID-19 infection segmentation.
comparison of the results between Task 2 and Task 3: using only out-of-domain cases can not predict COVID-19 infections while adding a few COVID-19 cases can significantly boost the performance, which highlights that including a few in-domain cases during training is very critical for developing infection segmentation models.
In addition, it can be found that the infection segmentation performance is lower than lung segmentation in all the experiments. There are two main reasons. First, the task setting is few-shot learning, where only limited labeled cases are allowed to train networks. Second, compared with the lung, the infection areas are relatively small and most of the infections have weak boundaries. Thus, infection segmentation is much more challenging than lung segmentation. Table X shows the recall and precision of the segmentation results in different tasks. It can be found that the infection segmentation results have relatively high precision scores and low recall scores, indicating that the model fails more in detecting all infections. Lung segmentation results achieve better recall and precision scores because the lung tissues have more clear boundaries and larger sizes. Figure 5 presents some visualized segmentation results in different tasks. We can find that the lung segmentation mostly failed due to the confusion between the left lung and the right lung because they share very similar appearances. The large and obvious infections have better segmentation results. However, the infections in small sizes or with weak boundaries are the most often failed cases.

Table X. Quantitative comparison of COVID-19 CT lung and infection segmentation results among different tasks on testing set in terms of average sensitivity and specificity values of all testing cases.
Subtask	Left lung	Right lung	Infection (COVID-19-CT-Seg)	Infection (MosMed)
Sensitivity (%)	Specificity (%)	Sensitivity (%)	Specificity (%)	Sensitivity (%)	Specificity (%)	Sensitivity (%)	Specificity (%)
Task1-Separate	86.2 ± 11.6	99.2 ± 1.5	89.7 ± 12.3	99.1 ± 7.3	62.0 ± 23.7	99.9 ± 15.9	57.5 ± 23.8	99.9 ± 0.0
Task1-Union	67.0 ± 28.5	98.8 ± 1.2	81.4 ± 19.5	98.2 ± 1.6	62.8 ± 27.1	99.7 ± 3.0	60.1 ± 24.3	99.9 ± 0.2
Task2-MSD	–	–	18.6 ± 23.1	100 ± 0.1	13.1 ± 22.9	100 ± 0.0
Task2-StructSeg	91.7 ± 20.8	99.9 ± 0.1	95.3 ± 10.2	99.8 ± 0.2	1.2 ± 2.4	100 ± 0.0	1.8 ± 6.9	100 ± 0.0
Task2-NSCLC	47.6 ± 23.4	99.4 ± 0.6	81.6 ± 21.7	97.4 ± 1.9	37.6 ± 26.6	100 ± 0.0	0.0 ± 0.0	100 ± 0.0
Task3-MSD	–	–	63.0 ± 27.4	99.8 ± 0.3	36.4 ± 32.7	100 ± 0.0
Task3-StructSeg	97.5 ± 2.7	99.9 ± 0.2	98.0 ± 2.0	99.8 ± 0.2	64.8 ± 25.3	99.8 ± 0.3	42.2 ± 29.5	100 ± 0.1
Task3-NSCLC	93.4 ± 7.1	99.7 ± 0.3	96.1 ± 3.4	99.5 ± 0.6	62.4 ± 26.7	99.7 ± 0.4	24.9 ± 25.9	100 ± 0.0
Subtask	Left lung	Right lung	Infection (COVID-19-CT-Seg)	Infection (MosMed)
Recall (%)	Precision (%)	Recall (%)	Precision (%)	Recall (%)	Precision (%)	Recall (%)	Precision (%)
Task1-Separate	86.9 ± 11.2	79.6 ± 19.6	90.2 ± 13.4	77.6 ± 14.7	62.0 ± 23.7	84.0 ± 19.6	57.5 ± 23.8	67.9 ± 21.7
Task1-Union	67.0 ± 28.5	71.6 ± 23.9	81.4 ± 19.5	72.8 ± 17.9	62.8 ± 27.1	74.1 ± 26.2	60.1 ± 24.3	57.7 ± 28.2
Task2-MSD	–	–	18.6 ± 23.1	78.7 ± 34.3	13.1 ± 22.9	47.9 ± 38.6
Task2-StructSeg	91.7 ± 20.8	97.0 ± 2.1	95.3 ± 10.2	96.3 ± 3.2	1.2 ± 2.4	22.6 ± 32.8	1.8 ± 6.9	9.3 ± 22.3
Task2-NSCLC	47.6 ± 23.4	80.2 ± 15.0	81.6 ± 21.7	67.3 ± 11.7	20.7 ± 49.8	6.8 ± 11.3	9.0 ± 3.5	17.6 ± 71.8
Task3-MSD	–	–	63.0 ± 27.4	75.0 ± 26.1	36.4 ± 32.7	61.4 ± 30.4
Task3-StructSeg	97.5 ± 2.7	97.2 ± 2.8	98.0 ± 2.0	97.4 ± 2.7	64.8 ± 25.3	76.8 ± 24.8	42.2 ± 29.5	60.7 ± 26.0
Task3-NSCLC	93.7 ± 6.8	93.6 ± 5.9	95.9 ± 3.8	92.3 ± 7.7	62.4 ± 27.3	73.1 ± 26.9	24.9 ± 25.9	61.4 ± 33.0
Details are in the caption following the image
Fig. 5
Open in figure viewer
PowerPoint
Visualized examples of segmentation results in different tasks. The red, green, and blue colors denote the left lung, the right lung, and the infection, respectively. [Color figure can be viewed at wileyonlinelibrary.com]
In summary, using the non-COVID-19 chest CT dataset can directly improve the lung segmentation results significantly, but it has few positive impacts on the infection segmentation because of the large domain gap. We found that the infections with good contrast and clear boundaries can be well segmented even with only four training cases. However, the trained models often miss the small infections and weak-boundary infections, indicating that it is hard for the models to capture these features during learning process. Our results also highlight the need of efficient learning methods with limited annotated data. Although including more training cases could be a simple and direct way to boost the infection segmentation performance, one should keep in mind that, in clinical practice, it is impractical to manually annotate many 3D COVID-19 CT scans for each medical center especially when the radiologists are busy with fighting the pandemic. This is our main motivation to setup the data-efficient learning benchmark.

There are some potential solutions for the performance improvements. For example, in few-shot learning task (Task 1), one can use more advanced data augmentation methods40 to increase the training set. In addition, designing task-specific data augmentation methods is also a promising solution, such as augment more cases with small and weak boundary infections. In domain generalization task (Task 2), the models trained with only non-COVID-19 dataset fail to segment infections. One can introduce more advanced model-agnostic learning methods to handle the domain gap, such as meta-learning.41, 42 In knowledge transfer task (Task 3), simply fusing non-COVID-19 and COVID-19 dataset with the SOTA network could bias the model to learn more non-COVID-19 features. One can use more robust and powerful domain adaptation methods to handle heterogeneous datasets, such as self-supervised learning,43 cross-domain adaptation.44, 45

4.E Limitation
One possible limitation is that the number of cases in our dataset is relatively small. However, this paper focuses on how to learn from limited training cases. Thus, we believe the number of training cases is acceptable for our benchmark tasks. More importantly, this benchmark is also applicable to general small-sample learning problems. In addition, the number of cases (16 or 20 cases) for the first testing set (COVID-19-CT-Seg) and 50 cases for the second testing set (MosMed) is comparable with recent MICCAI 2020 segmentation challenges. For example, StructSeg (Automatic Structure Segmentation for Radiotherapy Planning Challenge 2020) has 10 testing cases46 and ASOCA (Automated Segmentation Of Coronary Arteries) has 20 testing cases.47 Another limitation is that the innovative methodology contribution is limited. However, this is not the primary goal in this paper. Rather, our main goal is to lay the foundation for future work in learning with limited annotated data, and we believe the dataset and the tasks mentioned in the benchmark could attract attentions in the field.

5 CONCLUSION
With the outbreak of COVID-19 around the world, it has become an emergency need to develop deep learning-based COVID-19 image analysis tools with limited data. To promote the research toward this goal, in this paper, we created a COVID-19 CT dataset, established three segmentation benchmark tasks, and provided 40+ baselines models based on state-of-the-art segmentation architectures. All the related results are publicly available at https://github.com/JunMa11/COVID-19-CT-Seg-Benchmark. The unified task settings can make the comparison between studies more feasible. The public baselines can save model training time for researchers so that they can focus on developing their own methods. We hope this work could accelerate the COVID-19 studies on learning with limited data for years to come.

ACKNOWLEDGMENT
This project is supported by the National Natural Science Foundation of China (Nos. 11531005, 11971229). The numerical calculations in this paper have been done on the computing facilities in the High Performance Computing Center of Nanjing University. We thank all the organizers of MICCAI 2018 Medical Segmentation Decathlon, MICCAI 2019 Automatic Structure Segmentation for Radiotherapy Planning Challenge, the Coronacases Initiative and Radiopaedia, and Moscow municipal hospitals for their publicly available lung CT dataset. We also thank Joseph Paul Cohen for providing the convenient download link of 20 COVID-19 CT scans, and all the contributors of NSCLC and COVID-19-Seg-CT dataset for providing the annotations of lung, pleural effusion, and COVID-19 infection. We are grateful to the contributors of the two great COVID-19 related resources: COVID-19 imaging AI paper list‡‡ and MedSeg§§ for their timely update about COVID-19 publications and datasets. Last but not least, we thank the anonymous reviewers, Chen Chen, Xin Yang, and Yao Zhang for their important and valuable feedback on this work.

Summary
The implementation of clinical-decision support algorithms for medical imaging faces challenges with reliability and interpretability. Here, we establish a diagnostic tool based on a deep-learning framework for the screening of patients with common treatable blinding retinal diseases. Our framework utilizes transfer learning, which trains a neural network with a fraction of the data of conventional approaches. Applying this approach to a dataset of optical coherence tomography images, we demonstrate performance comparable to that of human experts in classifying age-related macular degeneration and diabetic macular edema. We also provide a more transparent and interpretable diagnosis by highlighting the regions recognized by the neural network. We further demonstrate the general applicability of our AI system for diagnosis of pediatric pneumonia using chest X-ray images. This tool may ultimately aid in expediting the diagnosis and referral of these treatable conditions, thereby facilitating earlier treatment, resulting in improved clinical outcomes.

Video Abstract
Media player

PlayRestartRewindForward
0 secondsVolume
SlowerFasterPreferencesEnter full screen
0:00 / 3:35Speed: 1xStopped
Download : Download video (30MB)

Graphical Abstract

Download : Download high-res image (179KB)Download : Download full-size image

Previous article in issueNext article in issue
Keywords
artificial intelligencetransfer learningdeep learningage-related macular degenerationchoroidal neovascularizationdiabetic retinopathydiabetic macular edemascreeningoptical coherence tomographypneumonia
Introduction
Artificial intelligence (AI) has the potential to revolutionize disease diagnosis and management by performing classification difficult for human experts and by rapidly reviewing immense amounts of images. Despite its potential, clinical interpretability and feasible preparation of AI remains challenging.

The traditional algorithmic approach to image analysis for classification previously relied on (1) handcrafted object segmentation, followed by (2) identification of each segmented object using statistical classifiers or shallow neural computational machine-learning classifiers designed specifically for each class of objects, and finally (3) classification of the image (Goldbaum et al., 1996). Creating and refining multiple classifiers required many skilled people and much time and was computationally expensive (Chaudhuri et al., 1989, Hoover and Goldbaum, 2003, Hoover et al., 2000).

The development of convolutional neural network layers has allowed for significant gains in the ability to classify images and detect objects in a picture (Krizhevsky et al., 2017, Zeiler and Fergus, 2014). These are multiple processing layers to which image analysis filters, or convolutions, are applied. The abstracted representation of images within each layer is constructed by systematically convolving multiple filters across the image, producing a feature map that is used as input to the following layer. This architecture makes it possible to process images in the form of pixels as input and to give the desired classification as output. The image-to-classification approach in one classifier replaces the multiple steps of previous image analysis methods.

One method of addressing a lack of data in a given domain is to leverage data from a similar domain, a technique known as transfer learning. Transfer learning has proven to be a highly effective technique, particularly when faced with domains with limited data (Donahue et al., 2013, Razavian et al., 2014, Yosinski et al., 2014). Rather than training a completely blank network, by using a feed-forward approach to fix the weights in the lower levels already optimized to recognize the structures found in images in general and retraining the weights of the upper levels with back propagation, the model can recognize the distinguishing features of a specific category of images, such as images of the eye, much faster and with significantly fewer training examples and less computational power (Figure 1).


Download : Download high-res image (692KB)Download : Download full-size image
Figure 1. Schematic of a Convolutional Neural Network

Schematic depicting how a convolutional neural network trained on the ImageNet dataset of 1,000 categories can be adapted to significantly increase the accuracy and shorten the training duration of a network trained on a novel dataset of OCT images. The locally connected (convolutional) layers are frozen and transferred into a new network, while the final, fully connected layers are recreated and retrained from random initialization on top of the transferred layers.

In this study, we sought to develop an effective transfer learning algorithm to process medical images to provide an accurate and timely diagnosis of key pathology in each image. The primary illustration of this technique involved optical coherence tomography (OCT) images of the retina, but the algorithm was also tested in a cohort of pediatric chest radiographs to validate the generalizability of this technique across multiple imaging modalities.

Results
The primary application of our transfer learning algorithm was in the diagnosis of retinal OCT images. Spectral-domain OCT uses light to capture high-resolution in vivo optical cross sections of the retina that can be assembled into three-dimensional-volume images of living retinal tissue. It has become one of the most commonly performed medical imaging procedures, with approximately 30 million OCT scans performed each year worldwide (Swanson and Fujimoto, 2017). OCT imaging is now a standard of care for guiding the diagnosis and treatment of some of the leading causes of blindness worldwide: age-related macular degeneration (AMD) and diabetic macular edema. Almost 10 million individuals suffer from AMD in the United States, and each year, more than 200,000 people develop choroidal neovascularization, a severe blinding form of advanced AMD (Ferrara, 2010, Friedman et al., 2004, Wong et al., 2014). In addition, nearly 750,000 individuals aged 40 or older suffer from diabetic macular edema (Varma et al., 2014), a vision-threatening form of diabetic retinopathy that involves the accumulation of fluid in the central retina. The prevalence of these diseases will likely increase even further over time due to the aging population and the global diabetes epidemic. Fortunately, the advent and widespread utilization of anti-vascular endothelial growth factor (anti-VEGF) medications has revolutionized the treatment of exudative retinal diseases (Kaiser et al., 2007, Ferrara, 2010), allowing patients to retain useful vision and quality of life. OCT is critical to guiding the administration of anti-VEGF therapy by providing a clear cross-sectional representation of the retinal pathology in these conditions (Figure 2A), allowing visualization of individual retinal layers, which is impossible with clinical examination by the human eye or by color fundus photography.


Download : Download high-res image (707KB)Download : Download full-size image
Figure 2. Representative Optical Coherence Tomography Images and the Workflow Diagram

(A) (Far left) choroidal neovascularization (CNV) with neovascular membrane (white arrowheads) and associated subretinal fluid (arrows). (Middle left) Diabetic macular edema (DME) with retinal-thickening-associated intraretinal fluid (arrows). (Middle right) Multiple drusen (arrowheads) present in early AMD. (Far right) Normal retina with preserved foveal contour and absence of any retinal fluid/edema.

(B) Workflow diagram showing overall experimental design describing the flow of optical coherence tomography (OCT) images through the labeling and grading process followed by creation of the transfer learning model, which then underwent training and subsequent testing. The training dataset only included images that passed sufficient quality and diagnostic standards from the initial collected dataset.

See also Table S1.

Patient and Image Characteristics
We initially obtained 207,130 OCT images. 108,312 images (37,206 with choroidal neovascularization, 11,349 with diabetic macular edema, 8,617 with drusen, and 51,140 normal) from 4,686 patients passed initial image quality review and were used to train the AI system. The model was tested with 1,000 images (250 from each category) from 633 patients. Patient characteristics for each diagnosis category are listed in Table S1. After 100 epochs (iterations through the entire dataset), the training was stopped due to the absence of further improvement in both accuracy (Figure 3A) and cross-entropy loss (Figure 3B).


Download : Download high-res image (830KB)Download : Download full-size image
Figure 3. Plot Showing Performance in the Training and Validation Datasets Using TensorBoard

Accuracy is plotted against the training step (A), and cross-entropy loss is plotted against the training step (B) during the length of the training of the multi-class classifier over the course of 10,000 steps. Plots were normalized with a smoothing factor of 0.6 to clearly visualize trends. The validation accuracy and loss show better performance, since images with more noise and lower quality were also included in the training set to reduce overfitting and help generalization of the classifier. Training dataset: orange. Validation dataset: blue.

See also Figure S1.

Performance of the Model
We evaluated our AI system in diagnosing the most common blinding retinal diseases. This AI system categorized images with choroidal neovascularization and images with diabetic macular edema as “urgent referrals.” These conditions would demand relatively urgent referral to an ophthalmologist for definitive anti-VEGF treatment; if treatment is delayed, there is increased risk of bleeding, scarring, or other downstream complications that cause irreversible vision impairment. The system categorized images with drusen, which are lipid deposits present in the dry form of macular degeneration, as “routine referrals.” Anti-VEGF medications are not indicated for dry macular degeneration; therefore, referral to an eye specialist for drusen is less urgent. Normal images were labeled for “observation.” In a multi-class comparison between choroidal neovascularization, diabetic macular edema, drusen, and normal, we achieved an accuracy of 96.6% (Figure 4), with a sensitivity of 97.8%, a specificity of 97.4%, and a weighted error of 6.6%. Receiver operating characteristic (ROC) curves were generated to evaluate the model’s ability to distinguish urgent referrals (defined as choroidal neovascularization or diabetic macular edema) from drusen and normal exams. The area under the ROC curve was 99.9% (Figure 4).


Download : Download high-res image (410KB)Download : Download full-size image
Figure 4. Multi-class Comparison between Choroidal Neovascularization, Diabetic Macular Edema, Drusen, and Normal

(A) Receiver operating characteristic (ROC) curve for “urgent referrals” (CNV and DME detection) with human expert performance for comparison. The area under the ROC curve was 99.9%. The zoomed area shows that the most accurate model demonstrates a performance that rivals that of six human experts.

(B) Confusion table of best model’s classification of the validation image set. The model successfully scored all urgent referrals as higher than observation.

(C) Weighted error results based on penalties in Figure S4 depicting neural networks in gold and human experts in blue.

See also Figures S2, S3, and S4 and Table S2.

We also trained a “limited model” classifying between the same four categories but only using 1,000 images randomly selected from each class during training to compare transfer learning performance using limited data compared to results using a large dataset. Using the same testing images, the model achieved an accuracy of 93.4%, with a sensitivity of 96.6%, a specificity of 94.0%, and a weighted error of 12.7%. The ROC curves distinguishing urgent referrals (i.e., distinguishing images with choroidal neovascularization or diabetic macular edema from normal images had an area under the curve of 98.8%.

Binary classifiers were also implemented to compare choroidal neovascularization/diabetic macular edema/drusen from normal using the same datasets in order to determine a breakdown of the model’s performance (Figure S1). The classifier distinguishing choroidal neovascularization images from normal images achieved an accuracy of 100.0%, with a sensitivity of 100.0% and specificity of 100.0%. The area under the ROC curve was 100.0% (Figure S2A). The classifier distinguishing diabetic macular edema images from normal images achieved an accuracy of 98.2%, with a sensitivity of 96.8% and specificity of 99.6%. The area under the ROC curve was 99.87% (Figure S2B). The classifier distinguishing drusen images from normal images achieved an accuracy of 99.0%, with a sensitivity of 98.0% and specificity of 99.2%. The area under the ROC curve was 99.96% (Figure S2C).


Download : Download high-res image (842KB)Download : Download full-size image
Figure S1. Plots Showing Binary Performance in the Training and Validation Datasets Using TensorBoard, Related to Figure 3

Comparisons were made for choroidal neovascularization (CNV) versus normal (A), diabetic macular edema (DME) versus normal (B), and drusen versus normal (C). Plots were normalized with a smoothing factor of 0.6 in order to clearly visualize trends. The validation accuracy and loss shows better performance since images with more noise and lower quality were also included in the training set to reduce overfitting and help generalization of the classifier. Training dataset: orange. Validation dataset: blue.


Download : Download high-res image (225KB)Download : Download full-size image
Figure S2. Receiver Operating Characteristic Curves for Binary Classifiers, Related to Figure 4

The corresponding area under the ROC curve (AUROC) for the graphs are 100% for choroidal neovascularization (CNV) versus normal (A), 99.87% for diabetic macular edema (DME) versus normal (B), and 99.96% for drusen versus normal (C). The straight vertical and horizontal lines in (A) and the nearly straight lines in (B) and (C) demonstrate that the binary convolutional neural network models have a near perfect classification performance.

Comparison of the Model with Human Experts
An independent test set of 1,000 images from 633 patients was used to compare the AI network’s referral decisions with the decisions made by human experts. Six experts with significant clinical experience in an academic ophthalmology center were instructed to make a referral decision on each test patient using only the patient’s OCT images. Performance on the clinically most important decision of distinguishing patients needing urgent referral (those with choroidal neovascularization or diabetic macular edema) compared to normal patients is displayed as a ROC curve, and this performance was comparable between the AI system and the human experts (Figure 4A).

Having established a standard expert performance evaluation system, we next compared the potential impact of patient referral decisions between our network and human experts. The sensitivities and specificities of the experts were plotted on the ROC curve of the trained model, and the differences in diagnostic performance, measured by likelihood ratios, between the model and the human experts were determined to be statistically similar within a 95% confidence interval (Figure S3). However, the pure error rate does not accurately reflect the impact that a wrong referral decision might have on the outcome of an individual patient. To illustrate, a false-positive result occurs when a patient is normal or has drusen but is inaccurately labeled as an urgent referral, and this can cause undue distress or unnecessary investigation for the patient and place extra burdens on the healthcare system. However, a false-negative result is far more serious, because in this instance, a patient with choroidal neovascularization or diabetic macular edema is not appropriately referred, which could result in irreversible visual loss. To account for these issues, weighted error scoring was incorporated during model evaluation and expert testing (Figure S4A). By assigning these penalty points to each decision made by the model and the experts, we computed the average error of each.


Download : Download high-res image (249KB)Download : Download full-size image
Figure S3. Plots Depicting the Positive and Negative Likelihood Ratios with Their Corresponding 95% Confidence Intervals Marked, Related to Figure 4

(A) The positive likelihood ratio is defined as the true positive rate over the false positive rate, so that an increasing likelihood ratio greater than 1 indicates increasing probability that the predicted result is associated with the disease.

(B) The negative likelihood ratio is defined as the false negative rate over the true negative rate, so that a decreasing likelihood ratio less than 1 indicates increasing probability that the predicted result is associated with the absence of disease.

The confidence intervals show that the best trained model demonstrated statistically similar screening performance in when compared to human experts.


Download : Download high-res image (605KB)Download : Download full-size image
Figure S4. Proposed Penalties for Incorrect Labeling during Weighted Error Calculations and Confusion Matrix of Experts Grading OCT Images, Related to Figure 4

(A) The penalties include an error score of 4 for “urgent referrals” scored as normal and an error score of 2 for “urgent referrals” scored as drusen. All other incorrect answers carry an error score of 1.

(B) The results for each of the human experts is depicted here, comparing the true labels and the predicted labels for each individual grader.

The best convolutional neural network model yielded a score of 6.6% under this weighted error system. The weighted error of the experts ranged from 0.4% to 10.5%, with a mean weighted error of 4.8% (Table S2). The exact breakdown of each expert’s performance regarding the correlation of their predicted labels with the true labels is depicted as confusion matrices in Figure S4B. As seen in Figure 4, the best model outperformed some human experts based on this weighted scale and on the ROC curve.

Occlusion Testing
We performed an occlusion test on 491 images to identify the areas contributing most to the neural network’s assignment of the predicted diagnosis. This testing successfully identified the region of interest in 94.7% of images that contributed the highest importance to the deep-learning algorithm (Figure 5A; see also Figure S5 for additional examples). Drusen were located correctly through occlusion testing in 100% of all the images, while choroidal neovascularization yielded an accuracy of 94.0% and diabetic macular edema yielded an accuracy of 91.0% (Table S3). Furthermore, these regions identified by occlusion testing were also verified by human experts to be the most clinically significant areas of pathology.


Download : Download high-res image (2MB)Download : Download full-size image
Figure 5. Occlusion Maps and Longitudinal Follow-up OCT Images Comparing Retinal Structural Changes before and after Anti-VEGF Therapy

(A) Occlusion maps highlighting areas of pathology in diabetic macular edema (left), choroidal neovascularization (middle), and drusen (right). An occlusion map was generated by convolving an occluding kernel across the input image. The occlusion map is created after prediction by assigning the softmax probability of the correct label to each occluded area. The occlusion map can then be superimposed on the input image to highlight the areas the model considered important in making its diagnosis.

(B and C) Horizontal cross-section OCT images through the fovea of patients with wet AMD (B) or diabetic retinopathy with macular edema (C) before and after three monthly intravitreal injections of bevacizumab. Both intraretinal and subretinal fluid (white arrows) lessened after treatment. Scar tissue of choroidal neovascularization remained (arrow heads). All visual accurity (VA) was improved: 20/320 to 20/250, 5 months (patient 1); 20/40 to 20/32, 9 months (patient 2); 20/400 to 20/250, 3 months (patient 3); 20/80 to 20/50, 7 months (patient 4); 20/40 to 20/25; 7 months (patient 5); and 20/32 to 20/25, 7 months (patient 6).

See also Figure S5 and Table S3.


Download : Download high-res image (740KB)Download : Download full-size image
Figure S5. Occlusion Maps of Diabetic Macular Edema, Choroidal Neovascularization, and Drusen, Related to Figure 5

(Top) Diabetic macular edema (DME), (middle) choroidal neovascularization (CNV), and (bottom), drusen. Additional examples of occlusion test images, illustrating how an occluding kernel was convolved across the input image to identify areas contributing to the algorithm’s determination of the diagnosis.

Application of the AI System for Pneumonia Detection Using Chest X-Ray Images
To investigate the generalizability of our AI system in the diagnosis of common diseases, we applied the same transfer learning framework to the diagnosis of pediatric pneumonia. According to the World Health Organization (WHO), pneumonia kills about 2 million children under 5 years old every year and is consistently estimated as the single leading cause of childhood mortality (Rudan et al., 2008), killing more children than HIV/AIDS, malaria, and measles combined (Adegbola, 2012). The WHO reports that nearly all cases (95%) of new-onset childhood clinical pneumonia occur in developing countries, particularly in Southeast Asia and Africa. Bacterial and viral pathogens are the two leading causes of pneumonia (Mcluckie, 2009) but require very different forms of management. Bacterial pneumonia requires urgent referral for immediate antibiotic treatment, while viral pneumonia is treated with supportive care. Therefore, accurate and timely diagnosis is imperative. One key element of diagnosis is radiographic data, since chest X-rays are routinely obtained as standard of care and can help differentiate between different types of pneumonia (Figure S6). However, rapid radiologic interpretation of images is not always available, particularly in the low-resource settings where childhood pneumonia has the highest incidence and highest rates of mortality. To this end, we also investigated the effectiveness of our transfer learning framework in classifying pediatric chest X-rays to detect pneumonia and furthermore to distinguish viral and bacterial pneumonia to facilitate rapid referrals for children needing urgent intervention.


Download : Download high-res image (225KB)Download : Download full-size image
Figure S6. Illustrative Examples of Chest X-Rays in Patients with Pneumonia, Related to Figure 6

The normal chest X-ray (left panel) depicts clear lungs without any areas of abnormal opacification in the image. Bacterial pneumonia (middle) typically exhibits a focal lobar consolidation, in this case in the right upper lobe (white arrows), whereas viral pneumonia (right) manifests with a more diffuse “interstitial” pattern in both lungs.

We collected and labeled a total of 5,232 chest X-ray images from children, including 3,883 characterized as depicting pneumonia (2,538 bacterial and 1,345 viral) and 1,349 normal, from a total of 5,856 patients to train the AI system. The model was then tested with 234 normal images and 390 pneumonia images (242 bacterial and 148 viral) from 624 patients. After 100 epochs (iterations through the entire dataset) of the model, the training was stopped due to the absence of further improvement in both loss and accuracy (Figures 6A and 6B).


Download : Download high-res image (811KB)Download : Download full-size image
Figure 6. Plots Depicting Performance of Pneumonia Diagnosis using Chest X-Ray Images in the Training and Validation Datasets Using TensorBoard

(A–F) Comparisons were made for pneumonia versus normal (A) with cross-entropy loss plotted against the training step (B), as well as comparisons between bacterial pneumonia and viral pneumonia (C) and the associated cross-entropy loss (D). Plots were normalized with a smoothing factor of 0.6 in order to clearly visualize trends. The area under the ROC curve for detecting pneumonia versus normal was 96.8% (E). The area under the ROC curve for detecting bacterial versus viral pneumonia was 94.0% (F). Training dataset: orange. Validation dataset: blue.

See also Figure S6.

In the comparison of chest X-rays presenting as pneumonia versus normal, we achieved an accuracy of 92.8%, with a sensitivity of 93.2% and a specificity of 90.1%. The area under the ROC curve for detection of pneumonia from normal was 96.8% (Figure 6E). Binary comparison of bacterial and viral pneumonia resulted in a test accuracy of 90.7%, with a sensitivity of 88.6% and a specificity of 90.9% (Figures 6C and 6D). The area under the ROC curve for distinguishing bacterial and viral pneumonia was 94.0% (Figure 6F).

Discussion
In this study, we describe a general AI platform for the diagnosis and referral of two common causes of severe vision loss: diabetic macular edema and choroidal neovascularization seen in neovascular AMD. By employing a transfer learning algorithm, our model demonstrated competitive performance of OCT image analysis without the need for a highly specialized deep-learning machine and without a database of millions of example images (STAR Methods). Moreover, the model’s performance in diagnosing retinal OCT images was comparable to that of human experts with significant clinical experience with retinal diseases. When the model was trained with a much smaller number of images (about 1,000 from each class), it retained high performance in accuracy, sensitivity, specificity, and area under the ROC curve for achieving the correct diagnosis and referral, thereby illustrating the power of the transfer learning system to make highly effective classifications, even with a very limited training dataset.

Although our AI platform was trained and validated using the Heidelberg Spectralis imaging system, the Digital Imaging and Communications in Medicine (DICOM) standards make the OCT images from different manufacturers (e.g., Zeiss and Optovue) reasonably consistent. The goal of this preliminary approach was to develop a system and demonstrate the soundness of the methods. Future studies could entail the use of images from different manufacturers in both the training and testing datasets so that the system will be universally useful. Moreover, the efficacy of the transfer learning technique for image analysis very likely extends beyond the realm of OCT images and ophthalmology—in principle, the techniques we have described here could potentially be employed in a wide range of medical images across multiple disciplines, and in fact, we provide a direct illustration of its wide applicability by demonstrating its efficacy in analysis of chest X-ray images.

Occlusion testing was performed to identify the areas of greatest importance used by the model in assigning a diagnosis. The greatest benefit of an occlusion test is that it reveals insights into the decisions of neural networks, which are infamously known as “black boxes” with no transparency. Since this test was performed after training was completed, it demystified the algorithm without affecting its results. The occlusion test also confirmed that the network made its decisions using accurate distinguishing features, which can be shared with a healthcare professional. All areas containing drusen were recognized correctly on all images used for testing, while the diabetic macular edema and choroidal neovascularization occlusion tests occasionally did not present a clear point of interest. This is likely due to the lesions and fluid pockets of choroidal neovascularization and diabetic macular edema sometimes presenting much larger than the occlusion window, while drusen tend to be smaller in size.

Although transfer learning allows the training of a highly accurate model with a relatively small training dataset, its performance would be inferior to that of a model trained from a random initialization on an extremely large dataset of OCT images, since even the internal weights can be directly optimized for OCT feature detection. However, in practice, a new convolutional neural network trained from random initialization, even with an unlimited supply of training data, would require weeks to achieve a good accuracy, whereas the multi-class holdout model implemented using transfer learning finished training and testing on different data in under 2 hr. Each binary classification and the limited model converged to a high accuracy in under 30 min. Since medical images are difficult to collect in the large amounts necessary to train a blank convolutional neural network, transfer learning using a pre-trained model trained on millions of various medical images would likely yield a more accurate model in much less time when retraining layers for other medical classifications.

The performance of our model depends highly on the weights of the pre-trained model. Therefore, the performance of this model would likely be enhanced when tested on a larger ImageNet dataset with more advanced deep-learning techniques and architecture. Further, the rapid progression and development of the field of convolutional neural networks applied outside of medical imaging would also improve the performance of our approach.

Finally, as mentioned earlier, we use OCT imaging as a demonstration of a generalized approach in medical image interpretation and subsequent decision making. Our framework effectively identified potential pathology on a tissue map to make a referral decision with performance comparable to (and sometimes even better than) human experts, enabling timely diagnosis of the two most common causes of irreversible severe vision loss. OCT is particularly useful in the management of retinal diseases because it has become critical to guiding anti-VEGF treatment for the intraretinal and/or subretinal fluid seen in many retinal conditions. This fluid often cannot be clearly visualized by the examiner’s eyes or by color fundus photography. In addition, the OCT appearance often correlates well with visual acuity. The presence of fluid is typically associated with worse visual acuity, which improves once the fluid is resolved with anti-VEGF treatment (Figure 5B). As a testament to the value of this imaging modality, treatment decisions for exudative retinal diseases are now guided by OCT rather than by clinical examination or fundus photography, making this demonstration of AI-guided classification of images more clinically relevant than prior studies that have analyzed retinal fundus photographs, such as that from Gulshan et al. (2016). Given that OCT imaging has played such a crucial role in guiding treatment, extending the application of AI beyond diagnosis or classification of images and into the realm of making treatment recommendations is a promising area of future investigation.

Furthermore, our network represents a generalized platform that can potentially be applied to a wide range of medical imaging techniques (e.g., chest X-ray, MRI, computed tomography) to make a clinical diagnostic decision. We demonstrated this point by training our network on a dataset of chest X-ray images of pediatric pneumonia. Chest X-rays present a difficult classification task due to the relatively large amount of variable objects, specifically the imaged areas outside the lungs that are irrelevant to the diagnosis of pneumonia. The resulting high-accuracy model suggests that this AI system has the potential to effectively learn from increasingly complicated images with a high degree of generalization using a relatively small repository of data. By demonstrating efficacy with multiple imaging modalities and with a wide range of pathology, this transfer learning framework presents a compelling system for further exploration and analysis in biomedical imaging and more generalized application to an automated community-based AI system for the diagnosis and triage of common human diseases. By providing our data and codes in a publicly available database, we also hope that other biomedical researchers may use our work as a resource to improve the performance of future models and help drive the field forward. This could facilitate screening programs and create more efficient referral systems in all of medicine, particularly in remote or low-resource areas, leading to a broad clinical and public health impact.

STAR★Methods
Key Resources Table
REAGENT or RESOURCE	SOURCE	IDENTIFIER
Deposited Data
OCT and Chest X-Ray images and codes	https://data.mendeley.com/datasets/rscbjbr9sj/2	N/A
Software and Algorithms
TensorFlow	https://www.tensorflow.org/	N/A
ImageNet	www.image-net.org	N/A

Contact for Reagent and Resource Sharing
Further information and requests for resources and classifiers should be directed to and will be fulfilled by the Lead Contact, Kang Zhang (kang.zhang@gmail.com). There are no restrictions for use of the materials disclosed.

Experimental Model and Subject Details
Images from Human Subjects
Optical coherence tomography (OCT) images (Spectralis OCT, Heidelberg Engineering, Germany) were selected from retrospective cohorts of adult patients from the Shiley Eye Institute of the University of California San Diego, the California Retinal Research Foundation, Medical Center Ophthalmology Associates, the Shanghai First People’s Hospital, and Beijing Tongren Eye Center between July 1, 2013 and March 1, 2017. All OCT imaging was performed as part of patients’ routine clinical care. There were no exclusion criteria based on age, gender, or race. We searched local electronic medical record databases for diagnoses of choroidal neovascularization, diabetic macular edema, drusen and normal to initially assign images. A horizontal foveal cut of OCT scans was downloaded with a standard image format according to manufacure’s softwares and instructions. Chest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children’s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients’ routine clinical care. Institutional Review Board (IRB)/Ethics Committee approvals were obtained. The work was conducted in a manner compliant with the United States Health Insurance Portability and Accountability Act (HIPAA) and was adherent to the tenets of the Declaration of Helsinki.

Method Details
OCT examinations were interpreted to confirm a diagnosis, and referral decisions were made thereafter (“urgent referral” for diagnoses of choroidal neovascularization or diabetic macular edema, “routine referral” for drusen, and “observation only” for normal). The dataset represents the most common medical retina patients presenting and receiving treatment at all participating clinics.

Chest X-ray examinations were interpreted to confirm a diagnosis, and referral decisions were made thereafter (“urgent referral” for diagnoses of bacterial pneumonia, “supportive care” for viral pneumonia, and “observation only” for normal).

Image Labeling
Before training, each image went through a tiered grading system consisting of multiple layers of trained graders of increasing expertise for verification and correction of image labels. Each image imported into the database started with a label matching the most recent diagnosis of the patient. The first tier of graders consisted of undergraduate and medical students who had taken and passed an OCT interpretation course review. This first tier of graders conducted initial quality control and excluded OCT images containing severe artifacts or significant image resolution reductions. The second tier of graders consisted of four ophthalmologists who independently graded each image that had passed the first tier. The presence or absence of choroidal neovascularization (active or in the form of subretinal fibrosis), macular edema, drusen, and other pathologies visible on the OCT scan were recorded. Finally, a third tier of two senior independent retinal specialists, each with over 20 years of clinical retina experience, verified the true labels for each image. The dataset selection and stratification process is displayed in a CONSORT-style diagram in Figure 2B. To account for human error in grading, a validation subset of 993 scans was graded separately by two ophthalmologist graders, with disagreement in clinical labels arbitrated by a senior retinal specialist.

For the analysis of chest X-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.

Transfer Learning Methods
Using the Tensorflow we adapted an Inception V3 architecture pretrained on the ImageNet dataset (Szegedy et al., 2016). Retraining consisted of initializing the convolutional layers with loaded pretrained weights and retraining the final, softmax layer to recognize our classes from scratch. In this study, the convolutional layers were frozen and used as fixed feature extractors. The convolutional “bottlenecks” are the values of each training and testing images after they have passed through the frozen layers of our model and since the convolutional weights are not updated, these values are initially calculated and stored in order to reduce redundant processes and speed up training. The newly initialized network, then, takes the image bottlenecks as input and retrains to classify our specific categories. Attempts at “fine-tuning” the convolutional layers by unfreezing and updating the pretrained weights on our medical images using backpropagation tended to decrease model performance due to overfitting (Figure 1).

The Inception model was trained on an Ubuntu 16.04 computer with 2 Intel Xeon CPUs, using a NVIDIA GTX 1080 8Gb GPU for training and testing, with 256Gb available in RAM memory. Training of layers was performed by stochastic gradient descent in batches of 1,000 images per step using an Adam Optimizer with a learning rate of 0.001. Training on all categories was run for 10,000 steps, or 100 epochs, since training of the final layers will have converged by then for all classes. Holdout method testing was performed after every step using a test partition containing images from patients independent of the patients represented in the training partition by passing each image through the network without performing gradient descent and backpropagation, and the best performing model was kept for analysis.

Expert Comparisons
In order to evaluate our model in the context of clinical experts, a validation set of 1000 images (633 patients), independent of the patients in the training set, was used to compare our network referral decisions with the decisions made by human experts. Weighted error scoring was used to reflect the fact that a false negative result (failing to refer) is more detrimental than a false positive result (making a referral when it was not warranted). Using these weighted penalty points, error rates were computed for the model and for each of the human experts.

Occlusion Test
Similarly to the methods described by Lee et al. and Zeiler and Fergus, an occlusion test was performed to identify the areas contributing the most to the neural network’s assignment of the predicted diagnosis(Lee et al., 2016, Zeiler and Fergus, 2014). A blank 20x20 pixel box was systematically moved across every possible position in the image and the probabilities of the disease were recorded. The highest drop in the probability represents the region of interest that contributed the highest importance to the deep learning algorithm (Figure 5A, see also Figure S5 for additional examples).

Quantification and Statistical Analysis
The 207,130 images collected were reduced to the 108,312 OCT images (from 4686 patients) and used for training the AI platform. Another subset of 633 patients not in the training set was collected based on a sample size requirement of 583 patients to detect sensitivity and specificity at 0.05 marginal error and 95% confidence. The test images (n = 1000) were used to evaluate model and human expert performance. Receiver operating characteristics (ROC) curves plot the true positive rate (sensitivity) versus the false positive rate (1 – specificity). ROC curves were generated using classification probabilities of urgent referral versus otherwise and the true labels of each test image and the ROC function of the Python scikit-learn library. The area under the ROC curve is a measure of performance and the true positive rate (TPR or sensitivity) at some chosen true negative rate (TNR or specificity) on the ROC curve is the probability that the classifier will rank a randomly chosen “urgent referral” instance higher than a randomly chosen normal or drusen instance. Accuracy was measured by dividing the number of correctly labeled images by the total number of test images. Sensitivity and specificity were determined by dividing the total number of correctly labeled urgent referrals and the total number of correctly labeled non-urgent referrals, respectively, by the total number of test images.

Data and Software Availability
All deep learning methods were implemented using either TensorFlow (https://www.tensorflow.org). ImageNet, a public database of images, can be found at https://www.image-net.org. Dataset on high resolution JPEG OCT and chest X-ray images are deposited into the public Mendeley database (https://doi.org/10.17632/rscbjbr9sj.3).

Acknowledgments
This study was funded by the National Key Research and Development Program of China (2017YFC1104600), National Natural Science Foundation of China (81771629 and 81700882), Guangzhou Women and Children’s Medical Center, Guangzhou Regenerative Medicine and Health Guangdong Laboratory, the Richard Annesser Fund, the Michael Martin Fund, and the Dick and Carol Hertzberg Fund.

Author Contributions
D.S.K., M.A.L., W.C., Justin Dong, C.C.S.V., G.Y., H.L., A.M., X. Wu, F.Y., J.Z., S.L.B., M.K.P., J.P., A.S., M.Y.L.T., C.L., S.H., Jason Dong, R.Z., L.Z., R.H., W.S., X.F., Y.D., V.A.N.H., I.Z., C.W., X. Wang, E.D.Z., C.L.Z., O.L., J.X., A.T., X.S., M.A.S., and H.X. collected and analyzed the data. K.Z. conceived the project. K.Z., D.S.K., M.G., and S.L.B. wrote the manuscript. All authors discussed the results and reviewed the manuscript.

Declaration of Interests
The authors declare no competing interests.

Resource
Identifying Medical Diagnoses and Treatable
Diseases by Image-Based Deep Learning
Graphical Abstract
Highlights
d An artificial intelligence system using transfer learning
techniques was developed
d It effectively classified images for macular degeneration and
diabetic retinopathy
d It also accurately distinguished bacterial and viral
pneumonia on chest X-rays
d This has potential for generalized high-impact application in
biomedical imaging
Authors
Daniel S. Kermany, Michael Goldbaum,
Wenjia Cai, ..., M. Anthony Lewis,
Huimin Xia, Kang Zhang
Correspondence
kang.zhang@gmail.com
In Brief
Image-based deep learning classifies
macular degeneration and diabetic
retinopathy using retinal optical
coherence tomography images and has
potential for generalized applications in
biomedical image interpretation and
medical decision making.
Kermany et al., 2018, Cell 172, 1122–1131
February 22, 2018 ª 2018 Elsevier Inc.
https://doi.org/10.1016/j.cell.2018.02.010
Resource
Identifying Medical Diagnoses and Treatable
Diseases by Image-Based Deep Learning
Daniel S. Kermany,1,2,14 Michael Goldbaum,2,14 Wenjia Cai,2,14 Carolina C.S. Valentim,2,14 Huiying Liang,1,14
Sally L. Baxter,2,14 Alex McKeown,3 Ge Yang,2 Xiaokang Wu,4 Fangbing Yan,4 Justin Dong,1 Made K. Prasadha,2
Jacqueline Pei,1,2 Magdalene Y.L. Ting,2 Jie Zhu,1,5 Christina Li,2 Sierra Hewett,1,2 Jason Dong,1 Ian Ziyar,2
Alexander Shi,2 Runze Zhang,2 Lianghong Zheng,6 Rui Hou,5 William Shi,2 Xin Fu,1,2 Yaou Duan,2 Viet A.N. Huu,1,2
Cindy Wen,2 Edward D. Zhang,1,2 Charlotte L. Zhang,1,2 Oulan Li,1,2 Xiaobo Wang,7 Michael A. Singer,8 Xiaodong Sun,9
Jie Xu,10 Ali Tafreshi,3 M. Anthony Lewis,11 Huimin Xia,1 and Kang Zhang1,2,4,12,13,15,* 1Guangzhou Women and Children’s Medical Center, Guangzhou Medical University, 510005 Guangzhou, China
2Shiley Eye Institute, Institute for Engineering in Medicine, Institute for Genomic Medicine, University of California, San Diego, La Jolla,
CA 92093, USA
3Heidelberg Engineering, Heidelberg, Germany
4Molecular Medicine Research Center, State Key Laboratory of Biotherapy, The National Clinical Research Center of Senile Disease,
West China Hospital, Sichuan University, Chengdu, China
5Guangzhou KangRui Biological Pharmaceutical Technology Company, 510005 Guangzhou, China
6YouHealth AI, 510005 Guangzhou, China
7Beihai Hospital, Dalian, 116021, China
8Department of Ophthalmology, University of Texas Health Science Center, San Antonio, TX 78229, USA
9Shanghai Key Laboratory of Ocular Fundus Diseases, Shanghai General Hospital, Shanghai JiaoTong University, 200080 Shanghai, China
10Beijing Instute of Ophthalmology, Beijing Tongren Eye Center, Beijing Tongren Hospital, Capital Medical University, Beijing, China
11Qualcomm, San Diego, CA 92121, USA
12Guangzhou Regenerative Medicine and Health Guangdong Laboratory, 510005 Guangzhou, China
13Veterans Administration Healthcare System, San Diego, CA 92037, USA
14These authors contributed equally
15Lead Contact
*Correspondence: kang.zhang@gmail.com
https://doi.org/10.1016/j.cell.2018.02.010
SUMMARY
The implementation of clinical-decision support
algorithms for medical imaging faces challenges
with reliability and interpretability. Here, we establish a diagnostic tool based on a deep-learning
framework for the screening of patients with
common treatable blinding retinal diseases. Our
framework utilizes transfer learning, which trains
a neural network with a fraction of the data of
conventional approaches. Applying this approach
to a dataset of optical coherence tomography
images, we demonstrate performance comparable
to that of human experts in classifying agerelated macular degeneration and diabetic macular edema. We also provide a more transparent
and interpretable diagnosis by highlighting the
regions recognized by the neural network. We
further demonstrate the general applicability of
our AI system for diagnosis of pediatric pneumonia using chest X-ray images. This tool may
ultimately aid in expediting the diagnosis and
referral of these treatable conditions, thereby
facilitating earlier treatment, resulting in improved
clinical outcomes.
INTRODUCTION
Artificial intelligence (AI) has the potential to revolutionize disease diagnosis and management by performing classification
difficult for human experts and by rapidly reviewing immense
amounts of images. Despite its potential, clinical interpretability
and feasible preparation of AI remains challenging.
The traditional algorithmic approach to image analysis for
classification previously relied on (1) handcrafted object segmentation, followed by (2) identification of each segmented
object using statistical classifiers or shallow neural computational machine-learning classifiers designed specifically for
each class of objects, and finally (3) classification of the image
(Goldbaum et al., 1996). Creating and refining multiple classifiers
required many skilled people and much time and was computationally expensive (Chaudhuri et al., 1989; Hoover and Goldbaum, 2003; Hoover et al., 2000).
The development of convolutional neural network layers has
allowed for significant gains in the ability to classify images
and detect objects in a picture (Krizhevsky et al., 2017; Zeiler
and Fergus, 2014). These are multiple processing layers to
which image analysis filters, or convolutions, are applied. The
abstracted representation of images within each layer is constructed by systematically convolving multiple filters across the
image, producing a feature map that is used as input to the
following layer. This architecture makes it possible to process
images in the form of pixels as input and to give the desired
1122 Cell 172, 1122–1131, February 22, 2018 ª 2018 Elsevier Inc.
classification as output. The image-to-classification approach
in one classifier replaces the multiple steps of previous image
analysis methods.
One method of addressing a lack of data in a given domain is
to leverage data from a similar domain, a technique known as
transfer learning. Transfer learning has proven to be a highly
effective technique, particularly when faced with domains with
limited data (Donahue et al., 2013; Razavian et al., 2014; Yosinski
et al., 2014). Rather than training a completely blank network, by
using a feed-forward approach to fix the weights in the lower
levels already optimized to recognize the structures found in
images in general and retraining the weights of the upper levels
with back propagation, the model can recognize the distinguishing features of a specific category of images, such as images
of the eye, much faster and with significantly fewer training examples and less computational power (Figure 1).
In this study, we sought to develop an effective transfer
learning algorithm to process medical images to provide an accurate and timely diagnosis of key pathology in each image. The
primary illustration of this technique involved optical coherence
Figure 1. Schematic of a Convolutional Neural Network
Schematic depicting how a convolutional neural
network trained on the ImageNet dataset of 1,000
categories can be adapted to significantly increase
the accuracy and shorten the training duration of a
network trained on a novel dataset of OCT images.
The locally connected (convolutional) layers are
frozen and transferred into a new network, while
the final, fully connected layers are recreated and
retrained from random initialization on top of the
transferred layers.
tomography (OCT) images of the retina,
but the algorithm was also tested in a
cohort of pediatric chest radiographs to
validate the generalizability of this technique across multiple imaging modalities.
RESULTS
The primary application of our transfer
learning algorithm was in the diagnosis
of retinal OCT images. Spectral-domain
OCT uses light to capture high-resolution
in vivo optical cross sections of the retina
that can be assembled into three-dimensional-volume images of living retinal
tissue. It has become one of the most
commonly performed medical imaging
procedures, with approximately 30 million
OCT scans performed each year worldwide (Swanson and Fujimoto, 2017).
OCT imaging is now a standard of care
for guiding the diagnosis and treatment
of some of the leading causes of
blindness worldwide: age-related macular degeneration (AMD) and diabetic macular edema. Almost
10 million individuals suffer from AMD in the United States, and
each year, more than 200,000 people develop choroidal neovascularization, a severe blinding form of advanced AMD (Ferrara,
2010; Friedman et al., 2004; Wong et al., 2014). In addition,
nearly 750,000 individuals aged 40 or older suffer from diabetic
macular edema (Varma et al., 2014), a vision-threatening form
of diabetic retinopathy that involves the accumulation of fluid
in the central retina. The prevalence of these diseases will likely
increase even further over time due to the aging population and
the global diabetes epidemic. Fortunately, the advent and widespread utilization of anti-vascular endothelial growth factor (antiVEGF) medications has revolutionized the treatment of exudative
retinal diseases (Kaiser et al., 2007; Ferrara, 2010), allowing
patients to retain useful vision and quality of life. OCT is critical
to guiding the administration of anti-VEGF therapy by providing
a clear cross-sectional representation of the retinal pathology
in these conditions (Figure 2A), allowing visualization of individual retinal layers, which is impossible with clinical examination
by the human eye or by color fundus photography.
Cell 172, 1122–1131, February 22, 2018 1123
Patient and Image Characteristics
We initially obtained 207,130 OCT images. 108,312 images
(37,206 with choroidal neovascularization, 11,349 with diabetic
macular edema, 8,617 with drusen, and 51,140 normal) from
4,686 patients passed initial image quality review and were
used to train the AI system. The model was tested with 1,000
images (250 from each category) from 633 patients. Patient
characteristics for each diagnosis category are listed in
Table S1. After 100 epochs (iterations through the entire dataset), the training was stopped due to the absence of further
improvement in both accuracy (Figure 3A) and cross-entropy
loss (Figure 3B).
Performance of the Model
We evaluated our AI system in diagnosing the most common
blinding retinal diseases. This AI system categorized images
with choroidal neovascularization and images with diabetic
macular edema as ‘‘urgent referrals.’’ These conditions would
demand relatively urgent referral to an ophthalmologist for
definitive anti-VEGF treatment; if treatment is delayed, there is
A
B
Figure 2. Representative Optical Coherence Tomography Images and the Workflow Diagram
(A) (Far left) choroidal neovascularization (CNV) with neovascular membrane (white arrowheads) and associated subretinal fluid (arrows). (Middle left) Diabetic
macular edema (DME) with retinal-thickening-associated intraretinal fluid (arrows). (Middle right) Multiple drusen (arrowheads) present in early AMD. (Far right)
Normal retina with preserved foveal contour and absence of any retinal fluid/edema.
(B) Workflow diagram showing overall experimental design describing the flow of optical coherence tomography (OCT) images through the labeling and grading
process followed by creation of the transfer learning model, which then underwent training and subsequent testing. The training dataset only included images that
passed sufficient quality and diagnostic standards from the initial collected dataset.
See also Table S1.
1124 Cell 172, 1122–1131, February 22, 2018
increased risk of bleeding, scarring, or other downstream complications that cause irreversible vision impairment. The system
categorized images with drusen, which are lipid deposits present
in the dry form of macular degeneration, as ‘‘routine referrals.’’
Anti-VEGF medications are not indicated for dry macular degeneration; therefore, referral to an eye specialist for drusen is
less urgent. Normal images were labeled for ‘‘observation.’’ In
a multi-class comparison between choroidal neovascularization,
diabetic macular edema, drusen, and normal, we achieved an
accuracy of 96.6% (Figure 4), with a sensitivity of 97.8%, a specificity of 97.4%, and a weighted error of 6.6%. Receiver operating characteristic (ROC) curves were generated to evaluate
the model’s ability to distinguish urgent referrals (defined as
choroidal neovascularization or diabetic macular edema) from
drusen and normal exams. The area under the ROC curve was
99.9% (Figure 4).
We also trained a ‘‘limited model’’ classifying between the
same four categories but only using 1,000 images randomly
selected from each class during training to compare transfer
learning performance using limited data compared to results
using a large dataset. Using the same testing images, the model
achieved an accuracy of 93.4%, with a sensitivity of 96.6%, a
specificity of 94.0%, and a weighted error of 12.7%. The ROC
curves distinguishing urgent referrals (i.e., distinguishing images
with choroidal neovascularization or diabetic macular edema
from normal images had an area under the curve of 98.8%.
Binary classifiers were also implemented to compare choroidal
neovascularization/diabetic macular edema/drusen from normal
Figure 3. Plot Showing Performance in the Training and Validation Datasets Using TensorBoard
Accuracy is plotted against the training step (A), and cross-entropy loss is plotted against the training step (B) during the length of the training of the multi-class
classifier over the course of 10,000 steps. Plots were normalized with a smoothing factor of 0.6 to clearly visualize trends. The validation accuracy and loss show
better performance, since images with more noise and lower quality were also included in the training set to reduce overfitting and help generalization of the
classifier. Training dataset: orange. Validation dataset: blue.
See also Figure S1.
Cell 172, 1122–1131, February 22, 2018 1125
using the same datasets in order to determine a breakdown
of the model’s performance (Figure S1). The classifier distinguishing choroidal neovascularization images from normal images achieved an accuracy of 100.0%, with a sensitivity of
100.0% and specificity of 100.0%. The area under the ROC curve
was 100.0% (Figure S2A). The classifier distinguishing diabetic
macular edema images from normal images achieved an accuracy of 98.2%, with a sensitivity of 96.8% and specificity of
99.6%. The area under the ROC curve was 99.87% (Figure S2B).
The classifier distinguishing drusen images from normal images
achieved an accuracy of 99.0%, with a sensitivity of 98.0%
and specificity of 99.2%. The area under the ROC curve was
99.96% (Figure S2C).
Comparison of the Model with Human Experts
An independent test set of 1,000 images from 633 patients was
used to compare the AI network’s referral decisions with the
decisions made by human experts. Six experts with significant
clinical experience in an academic ophthalmology center were
instructed to make a referral decision on each test patient using
only the patient’s OCT images. Performance on the clinically
most important decision of distinguishing patients needing
urgent referral (those with choroidal neovascularization or diabetic macular edema) compared to normal patients is displayed
as a ROC curve, and this performance was comparable between
the AI system and the human experts (Figure 4A).
Having established a standard expert performance evaluation
system, we next compared the potential impact of patient
referral decisions between our network and human experts.
The sensitivities and specificities of the experts were plotted
on the ROC curve of the trained model, and the differences in
diagnostic performance, measured by likelihood ratios, between
the model and the human experts were determined to be
statistically similar within a 95% confidence interval (Figure S3).
Figure 4. Multi-class Comparison between Choroidal Neovascularization, Diabetic Macular Edema, Drusen, and Normal
(A) Receiver operating characteristic (ROC) curve for ‘‘urgent referrals’’ (CNV and DME detection) with human expert performance for comparison. The area under
the ROC curve was 99.9%. The zoomed area shows that the most accurate model demonstrates a performance that rivals that of six human experts.
(B) Confusion table of best model’s classification of the validation image set. The model successfully scored all urgent referrals as higher than observation.
(C) Weighted error results based on penalties in Figure S4 depicting neural networks in gold and human experts in blue.
See also Figures S2, S3, and S4 and Table S2.
1126 Cell 172, 1122–1131, February 22, 2018
However, the pure error rate does not accurately reflect the
impact that a wrong referral decision might have on the outcome
of an individual patient. To illustrate, a false-positive result occurs when a patient is normal or has drusen but is inaccurately
labeled as an urgent referral, and this can cause undue distress
or unnecessary investigation for the patient and place extra burdens on the healthcare system. However, a false-negative result
is far more serious, because in this instance, a patient with
choroidal neovascularization or diabetic macular edema is not
appropriately referred, which could result in irreversible visual
loss. To account for these issues, weighted error scoring was
incorporated during model evaluation and expert testing (Figure S4A). By assigning these penalty points to each decision
made by the model and the experts, we computed the average
error of each.
The best convolutional neural network model yielded a score
of 6.6% under this weighted error system. The weighted error
of the experts ranged from 0.4% to 10.5%, with a mean
weighted error of 4.8% (Table S2). The exact breakdown of
each expert’s performance regarding the correlation of their
predicted labels with the true labels is depicted as confusion
matrices in Figure S4B. As seen in Figure 4, the best model outperformed some human experts based on this weighted scale
and on the ROC curve.
Occlusion Testing
We performed an occlusion test on 491 images to identify the
areas contributing most to the neural network’s assignment of
the predicted diagnosis. This testing successfully identified the
region of interest in 94.7% of images that contributed the highest
importance to the deep-learning algorithm (Figure 5A; see also
Figure S5 for additional examples). Drusen were located
correctly through occlusion testing in 100% of all the images,
while choroidal neovascularization yielded an accuracy of
94.0% and diabetic macular edema yielded an accuracy of
91.0% (Table S3). Furthermore, these regions identified by
occlusion testing were also verified by human experts to be
the most clinically significant areas of pathology.
Application of the AI System for Pneumonia Detection
Using Chest X-Ray Images
To investigate the generalizability of our AI system in the diagnosis of common diseases, we applied the same transfer
learning framework to the diagnosis of pediatric pneumonia.
According to the World Health Organization (WHO), pneumonia
kills about 2 million children under 5 years old every year and
is consistently estimated as the single leading cause of childhood mortality (Rudan et al., 2008), killing more children than
HIV/AIDS, malaria, and measles combined (Adegbola, 2012).
The WHO reports that nearly all cases (95%) of new-onset childhood clinical pneumonia occur in developing countries, particularly in Southeast Asia and Africa. Bacterial and viral pathogens
are the two leading causes of pneumonia (Mcluckie, 2009)
but require very different forms of management. Bacterial pneumonia requires urgent referral for immediate antibiotic treatment,
while viral pneumonia is treated with supportive care. Therefore,
accurate and timely diagnosis is imperative. One key element of
diagnosis is radiographic data, since chest X-rays are routinely
obtained as standard of care and can help differentiate between
different types of pneumonia (Figure S6). However, rapid radiologic interpretation of images is not always available, particularly
in the low-resource settings where childhood pneumonia has the
highest incidence and highest rates of mortality. To this end, we
also investigated the effectiveness of our transfer learning framework in classifying pediatric chest X-rays to detect pneumonia
and furthermore to distinguish viral and bacterial pneumonia to
facilitate rapid referrals for children needing urgent intervention.
We collected and labeled a total of 5,232 chest X-ray images
from children, including 3,883 characterized as depicting pneumonia (2,538 bacterial and 1,345 viral) and 1,349 normal, from
a total of 5,856 patients to train the AI system. The model was
then tested with 234 normal images and 390 pneumonia images
(242 bacterial and 148 viral) from 624 patients. After 100 epochs
(iterations through the entire dataset) of the model, the training
was stopped due to the absence of further improvement in
both loss and accuracy (Figures 6A and 6B).
In the comparison of chest X-rays presenting as pneumonia
versus normal, we achieved an accuracy of 92.8%, with a
sensitivity of 93.2% and a specificity of 90.1%. The area under
the ROC curve for detection of pneumonia from normal was
96.8% (Figure 6E). Binary comparison of bacterial and viral
pneumonia resulted in a test accuracy of 90.7%, with a sensitivity of 88.6% and a specificity of 90.9% (Figures 6C and 6D).
The area under the ROC curve for distinguishing bacterial and
viral pneumonia was 94.0% (Figure 6F).
DISCUSSION
In this study, we describe a general AI platform for the diagnosis
and referral of two common causes of severe vision loss: diabetic macular edema and choroidal neovascularization seen in
neovascular AMD. By employing a transfer learning algorithm,
our model demonstrated competitive performance of OCT image analysis without the need for a highly specialized deeplearning machine and without a database of millions of example
images (STAR Methods). Moreover, the model’s performance in
diagnosing retinal OCT images was comparable to that of human
experts with significant clinical experience with retinal diseases.
When the model was trained with a much smaller number of
images (about 1,000 from each class), it retained high performance in accuracy, sensitivity, specificity, and area under the
ROC curve for achieving the correct diagnosis and referral,
thereby illustrating the power of the transfer learning system to
make highly effective classifications, even with a very limited
training dataset.
Although our AI platform was trained and validated using
the Heidelberg Spectralis imaging system, the Digital Imaging
and Communications in Medicine (DICOM) standards make
the OCT images from different manufacturers (e.g., Zeiss
and Optovue) reasonably consistent. The goal of this preliminary
approach was to develop a system and demonstrate the
soundness of the methods. Future studies could entail the use
of images from different manufacturers in both the training and
testing datasets so that the system will be universally useful.
Moreover, the efficacy of the transfer learning technique for image analysis very likely extends beyond the realm of OCT images
Cell 172, 1122–1131, February 22, 2018 1127
and ophthalmology—in principle, the techniques we have
described here could potentially be employed in a wide range
of medical images across multiple disciplines, and in fact, we
provide a direct illustration of its wide applicability by demonstrating its efficacy in analysis of chest X-ray images.
Occlusion testing was performed to identify the areas of
greatest importance used by the model in assigning a diagnosis.
The greatest benefit of an occlusion test is that it reveals insights
into the decisions of neural networks, which are infamously
known as ‘‘black boxes’’ with no transparency. Since this test
was performed after training was completed, it demystified the
algorithm without affecting its results. The occlusion test also
confirmed that the network made its decisions using accurate
distinguishing features, which can be shared with a healthcare
professional. All areas containing drusen were recognized
correctly on all images used for testing, while the diabetic macular edema and choroidal neovascularization occlusion tests occasionally did not present a clear point of interest. This is likely
due to the lesions and fluid pockets of choroidal neovascularization and diabetic macular edema sometimes presenting
much larger than the occlusion window, while drusen tend to
be smaller in size.
Figure 5. Occlusion Maps and Longitudinal Follow-up OCT Images Comparing Retinal Structural Changes before and after Anti-VEGF
Therapy
(A) Occlusion maps highlighting areas of pathology in diabetic macular edema (left), choroidal neovascularization (middle), and drusen (right). An occlusion map
was generated by convolving an occluding kernel across the input image. The occlusion map is created after prediction by assigning the softmax probability of
the correct label to each occluded area. The occlusion map can then be superimposed on the input image to highlight the areas the model considered important in
making its diagnosis.
(B and C) Horizontal cross-section OCT images through the fovea of patients with wet AMD (B) or diabetic retinopathy with macular edema (C) before and after
three monthly intravitreal injections of bevacizumab. Both intraretinal and subretinal fluid (white arrows) lessened after treatment. Scar tissue of choroidal
neovascularization remained (arrow heads). All visual accurity (VA) was improved: 20/320 to 20/250, 5 months (patient 1); 20/40 to 20/32, 9 months (patient 2);
20/400 to 20/250, 3 months (patient 3); 20/80 to 20/50, 7 months (patient 4); 20/40 to 20/25; 7 months (patient 5); and 20/32 to 20/25, 7 months (patient 6).
See also Figure S5 and Table S3.
1128 Cell 172, 1122–1131, February 22, 2018
Although transfer learning allows the training of a highly accurate model with a relatively small training dataset, its performance would be inferior to that of a model trained from a random
initialization on an extremely large dataset of OCT images, since
even the internal weights can be directly optimized for OCT
feature detection. However, in practice, a new convolutional
neural network trained from random initialization, even with an
unlimited supply of training data, would require weeks to achieve
a good accuracy, whereas the multi-class holdout model implemented using transfer learning finished training and testing on
different data in under 2 hr. Each binary classification and the
limited model converged to a high accuracy in under 30 min.
Since medical images are difficult to collect in the large amounts
necessary to train a blank convolutional neural network, transfer
learning using a pre-trained model trained on millions of various
medical images would likely yield a more accurate model
in much less time when retraining layers for other medical
classifications.
The performance of our model depends highly on the
weights of the pre-trained model. Therefore, the performance
of this model would likely be enhanced when tested on a larger
ImageNet dataset with more advanced deep-learning techniques and architecture. Further, the rapid progression and
development of the field of convolutional neural networks
applied outside of medical imaging would also improve the
performance of our approach.
Figure 6. Plots Depicting Performance of Pneumonia Diagnosis using Chest X-Ray Images in the Training and Validation Datasets Using
TensorBoard
(A–F) Comparisons were made for pneumonia versus normal (A) with cross-entropy loss plotted against the training step (B), as well as comparisons between
bacterial pneumonia and viral pneumonia (C) and the associated cross-entropy loss (D). Plots were normalized with a smoothing factor of 0.6 in order to clearly
visualize trends. The area under the ROC curve for detecting pneumonia versus normal was 96.8% (E). The area under the ROC curve for detecting bacterial
versus viral pneumonia was 94.0% (F). Training dataset: orange. Validation dataset: blue.
See also Figure S6.
Cell 172, 1122–1131, February 22, 2018 1129
Finally, as mentioned earlier, we use OCT imaging as a
demonstration of a generalized approach in medical image
interpretation and subsequent decision making. Our framework
effectively identified potential pathology on a tissue map to
make a referral decision with performance comparable to (and
sometimes even better than) human experts, enabling timely
diagnosis of the two most common causes of irreversible severe
vision loss. OCT is particularly useful in the management of
retinal diseases because it has become critical to guiding antiVEGF treatment for the intraretinal and/or subretinal fluid seen
in many retinal conditions. This fluid often cannot be clearly visualized by the examiner’s eyes or by color fundus photography. In
addition, the OCT appearance often correlates well with visual
acuity. The presence of fluid is typically associated with worse
visual acuity, which improves once the fluid is resolved with
anti-VEGF treatment (Figure 5B). As a testament to the value of
this imaging modality, treatment decisions for exudative retinal
diseases are now guided by OCT rather than by clinical examination or fundus photography, making this demonstration of AIguided classification of images more clinically relevant than prior
studies that have analyzed retinal fundus photographs, such as
that from Gulshan et al. (2016). Given that OCT imaging has
played such a crucial role in guiding treatment, extending the
application of AI beyond diagnosis or classification of images
and into the realm of making treatment recommendations is a
promising area of future investigation.
Furthermore, our network represents a generalized platform
that can potentially be applied to a wide range of medical imaging
techniques (e.g., chest X-ray, MRI, computed tomography) to
make a clinical diagnostic decision. We demonstrated this point
by training our network on a dataset of chest X-ray images of pediatric pneumonia. Chest X-rays present a difficult classification
task due to the relatively large amount of variable objects, specifically the imaged areas outside the lungs that are irrelevant to the
diagnosis of pneumonia. The resulting high-accuracy model suggests that this AI system has the potential to effectively learn from
increasingly complicated images with a high degree of generalization using a relatively small repository of data. By demonstrating
efficacy with multiple imaging modalities and with a wide range
of pathology, this transfer learning framework presents a compelling system for further exploration and analysis in biomedical
imaging and more generalized application to an automated community-based AI system for the diagnosis and triage of common
human diseases. By providing our data and codes in a publicly
available database, we also hope that other biomedical researchers may use our work as a resource to improve the performance offuturemodels and help drive the field forward. This could
facilitate screening programs and create more efficient referral
systems in all of medicine, particularly in remote or low-resource
areas, leading to a broad clinical and public health impact.
STAR+METHODS
Detailed methods are provided in the online version of this paper
and include the following:
d KEY RESOURCES TABLE
d CONTACT FOR REAGENT AND RESOURCE SHARING
d EXPERIMENTAL MODEL AND SUBJECT DETAILS
B Images from Human Subjects
d METHOD DETAILS
B Image Labeling
B Transfer Learning Methods
B Expert Comparisons
B Occlusion Test
d QUANTIFICATION AND STATISTICAL ANALYSIS
d DATA AND SOFTWARE AVAILABILITY
SUPPLEMENTAL INFORMATION
Supplemental Information includes six figures and three tables and can be
found with this article online at https://doi.org/10.1016/j.cell.2018.02.010.
A video abstract is available at http://dx.doi.org/10.1016/j.cell.2018.02.
010#mmc2.
ACKNOWLEDGMENTS
This study was funded by the National Key Research and Development Program of China (2017YFC1104600), National Natural Science Foundation of
China (81771629 and 81700882), Guangzhou Women and Children’s Medical
Center, Guangzhou Regenerative Medicine and Health Guangdong Laboratory, the Richard Annesser Fund, the Michael Martin Fund, and the Dick and
Carol Hertzberg Fund.
AUTHOR CONTRIBUTIONS
D.S.K., M.A.L., W.C., Justin Dong, C.C.S.V., G.Y., H.L., A.M., X. Wu, F.Y., J.Z.,
S.L.B., M.K.P., J.P., A.S., M.Y.L.T., C.L., S.H., Jason Dong, R.Z., L.Z., R.H.,
W.S., X.F., Y.D., V.A.N.H., I.Z., C.W., X. Wang, E.D.Z., C.L.Z., O.L., J.X.,
A.T., X.S., M.A.S., and H.X. collected and analyzed the data. K.Z. conceived
the project. K.Z., D.S.K., M.G., and S.L.B. wrote the manuscript. All authors
discussed the results and reviewed the manuscript.
DECLARATION OF INTERESTS
The authors declare no competing interests.
Received: November 1, 2017
Revised: December 31, 2017
Accepted: February 1, 2018
Published: February 22, 2018
REFERENCES
Adegbola, R.A. (2012). Childhood pneumonia as a global health priority and the
strategic interest of the Bill & Melinda Gates Foundation. Clin. Infect. Dis. 54
(Suppl 2 ), S89–S92.
Chaudhuri, S., Chatterjee, S., Katz, N., Nelson, M., and Goldbaum, M. (1989).
Detection of blood vessels in retinal images using two-dimensional matched
filters. IEEE Trans. Med. Imaging 8, 263–269.
Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell,
T. (2013). DeCAF: A Deep Convolutional Activation Feature for Generic Visual
Recognition. Proceedings of the 31st International Conference on Machine
Learning 32, 647–655.
Ferrara, N. (2010). Vascular endothelial growth factor and age-related macular
degeneration: from basic science to therapy. Nat. Med. 16, 1107–1111.
Friedman, D.S., O’Colmain, B.J., Mun˜ oz, B., Tomany, S.C., McCarty, C.,
de Jong, P.T., Nemesure, B., Mitchell, P., and Kempen, J.; Eye Diseases Prevalence Research Group (2004). Prevalence of age-related macular degeneration in the United States. Arch. Ophthalmol. 122, 564–572.
Goldbaum, M., Moezzi, S., Taylor, A., Chatterjee, S., Boyd, J., Hunter, E., and
Jain, R. (1996). Automated diagnosis and image understanding with object
1130 Cell 172, 1122–1131, February 22, 2018
extraction, object classification, and inferencing in retinal images. Proceedings
of 3rd IEEE International Conference on Image Processing 3, 695–698.
Gulshan, V., Peng, L., Coram, M., Stumpe, M.C., Wu, D., Narayanaswamy, A.,
Venugopalan, S., Widner, K., Madams, T., Cuadros, J., et al. (2016). Development and Validation of a Deep Learning Algorithm for Detection of Diabetic
Retinopathy in Retinal Fundus Photographs. JAMA 316, 2402–2410.
Hoover, A., and Goldbaum, M. (2003). Locating the optic nerve in a retinal image using the fuzzy convergence of the blood vessels. IEEE Trans. Med. Imaging 22, 951–958.
Hoover, A., Kouznetsova, V., and Goldbaum, M. (2000). Locating blood vessels in retinal images by piecewise threshold probing of a matched filter
response. IEEE Trans. Med. Imaging 19, 203–210.
Kaiser, P.K., Brown, D.M., Zhang, K., Hudson, H.L., Holz, F.G., Shapiro, H.,
Schneider, S., and Acharya, N.R. (2007). Ranibizumab for predominantly
classic neovascular age-related macular degeneration: subgroup analysis of
first-year ANCHOR results. Am. J. Ophthalmol. 144, 850–857.
Krizhevsky, A., Sutskever, I., and Hinton, G.E. (2017). ImageNet classification
with deep convolutional neural networks. Commun. ACM 60, 84–90.
Lee, C.S., Baughman, D.M., and Lee, A.Y. (2016). Deep Learning Is Effective
for the Classification of OCT Images of Normal versus Age-Related Macular
Degeneration. Ophthamol. Retina 1, 322–327.
Mcluckie, A. (2009). Respiratory disease and its management, Volume 57
(Springer).
Razavian, A.S., Azizpour, H., Sullivan, J., and Carlsson, S. (2014). CNN
Features Off-the-Shelf: An Astounding Baseline for Recognition. In 2014
IEEE Conference on Computer Vision and Pattern Recognition Workshops,
pp. 512–519.
Rudan, I., Boschi-Pinto, C., Biloglav, Z., Mulholland, K., and Campbell, H.
(2008). Epidemiology and etiology of childhood pneumonia. Bull. World Health
Organ. 86, 408–416.
Swanson, E.A., and Fujimoto, J.G. (2017). The ecosystem that powered the
translation of OCT from fundamental research to clinical and commercial
impact [Invited]. Biomed. Opt. Express 8, 1638–1664.
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016).
Rethinking the Inception Architecture for Computer Vision. In 2016 IWWW
Conference on Computer Vision and Pattern Recognition, pp. 2818–2826.
Varma, R., Bressler, N.M., Doan, Q.V., Gleeson, M., Danese, M., Bower, J.K.,
Selvin, E., Dolan, C., Fine, J., Colman, S., and Turpcu, A. (2014). Prevalence
of and risk factors for diabetic macular edema in the United States. JAMA
Ophthalmol. 132, 1334–1340.
Wong, W.L., Su, X., Li, X., Cheung, C.M., Klein, R., Cheng, C.Y., and Wong,
T.Y. (2014). Global prevalence of age-related macular degeneration and disease burden projection for 2020 and 2040: a systematic review and meta-analysis. Lancet Glob. Health 2, e106–e116.
Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014). How transferable are
features in deep neural networks? NIPS’14 Proceedings of the 27th International Conference on Neural Information Processing Systems 2, 3320–3328.
Zeiler, M.D., and Fergus, R. (2014). Visualizing and Understanding Convolutional Networks. Lect. Notes Comput. Sci. 8689, 818–833.
Cell 172, 1122–1131, February 22, 2018 1131
STAR+METHODS
KEY RESOURCES TABLE
CONTACT FOR REAGENT AND RESOURCE SHARING
Further information and requests for resources and classifiers should be directed to and will be fulfilled by the Lead Contact,
Kang Zhang (kang.zhang@gmail.com). There are no restrictions for use of the materials disclosed.
EXPERIMENTAL MODEL AND SUBJECT DETAILS
Images from Human Subjects
Optical coherence tomography (OCT) images (Spectralis OCT, Heidelberg Engineering, Germany) were selected from retrospective
cohorts of adult patients from the Shiley Eye Institute of the University of California San Diego, the California Retinal Research Foundation, Medical Center Ophthalmology Associates, the Shanghai First People’s Hospital, and Beijing Tongren Eye Center between
July 1, 2013 and March 1, 2017. All OCT imaging was performed as part of patients’ routine clinical care. There were no exclusion
criteria based on age, gender, or race. We searched local electronic medical record databases for diagnoses of choroidal neovascularization, diabetic macular edema, drusen and normal to initially assign images. A horizontal foveal cut of OCT scans was downloaded with a standard image format according to manufacure’s softwares and instructions. Chest X-ray images (anterior-posterior)
were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children’s
Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients’ routine clinical care. Institutional Review
Board (IRB)/Ethics Committee approvals were obtained. The work was conducted in a manner compliant with the United States
Health Insurance Portability and Accountability Act (HIPAA) and was adherent to the tenets of the Declaration of Helsinki.
METHOD DETAILS
OCT examinations were interpreted to confirm a diagnosis, and referral decisions were made thereafter (‘‘urgent referral’’ for diagnoses of choroidal neovascularization or diabetic macular edema, ‘‘routine referral’’ for drusen, and ‘‘observation only’’ for normal).
The dataset represents the most common medical retina patients presenting and receiving treatment at all participating clinics.
Chest X-ray examinations were interpreted to confirm a diagnosis, and referral decisions were made thereafter (‘‘urgent referral’’
for diagnoses of bacterial pneumonia, ‘‘supportive care’’ for viral pneumonia, and ‘‘observation only’’ for normal).
Image Labeling
Before training, each image went through a tiered grading system consisting of multiple layers of trained graders of increasing expertise for verification and correction of image labels. Each image imported into the database started with a label matching the most
recent diagnosis of the patient. The first tier of graders consisted of undergraduate and medical students who had taken and passed
an OCT interpretation course review. This first tier of graders conducted initial quality control and excluded OCT images containing
severe artifacts or significant image resolution reductions. The second tier of graders consisted of four ophthalmologists who
independently graded each image that had passed the first tier. The presence or absence of choroidal neovascularization (active
or in the form of subretinal fibrosis), macular edema, drusen, and other pathologies visible on the OCT scan were recorded. Finally,
a third tier of two senior independent retinal specialists, each with over 20 years of clinical retina experience, verified the true labels for
each image. The dataset selection and stratification process is displayed in a CONSORT-style diagram in Figure 2B. To account for
human error in grading, a validation subset of 993 scans was graded separately by two ophthalmologist graders, with disagreement
in clinical labels arbitrated by a senior retinal specialist.
For the analysis of chest X-ray images, all chest radiographs were initially screened for quality control by removing all low quality or
unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI
system. In order to account for any grading errors, the evaluation set was also checked by a third expert.
REAGENT or RESOURCE SOURCE IDENTIFIER
Deposited Data
OCT and Chest X-Ray images and codes https://data.mendeley.com/datasets/rscbjbr9sj/2 N/A
Software and Algorithms
TensorFlow https://www.tensorflow.org/ N/A
ImageNet www.image-net.org N/A
e1 Cell 172, 1122–1131.e1–e2, February 22, 2018
Transfer Learning Methods
Using the Tensorflow we adapted an Inception V3 architecture pretrained on the ImageNet dataset (Szegedy et al., 2016). Retraining
consisted of initializing the convolutional layers with loaded pretrained weights and retraining the final, softmax layer to recognize
our classes from scratch. In this study, the convolutional layers were frozen and used as fixed feature extractors. The convolutional
‘‘bottlenecks’’ are the values of each training and testing images after they have passed through the frozen layers of our model and
since the convolutional weights are not updated, these values are initially calculated and stored in order to reduce redundant
processes and speed up training. The newly initialized network, then, takes the image bottlenecks as input and retrains to classify
our specific categories. Attempts at ‘‘fine-tuning’’ the convolutional layers by unfreezing and updating the pretrained weights on
our medical images using backpropagation tended to decrease model performance due to overfitting (Figure 1).
The Inception model was trained on an Ubuntu 16.04 computer with 2 Intel Xeon CPUs, using a NVIDIA GTX 1080 8Gb GPU for
training and testing, with 256Gb available in RAM memory. Training of layers was performed by stochastic gradient descent in
batches of 1,000 images per step using an Adam Optimizer with a learning rate of 0.001. Training on all categories was run for
10,000 steps, or 100 epochs, since training of the final layers will have converged by then for all classes. Holdout method testing
was performed after every step using a test partition containing images from patients independent of the patients represented in
the training partition by passing each image through the network without performing gradient descent and backpropagation, and
the best performing model was kept for analysis.
Expert Comparisons
In order to evaluate our model in the context of clinical experts, a validation set of 1000 images (633 patients), independent of the
patients in the training set, was used to compare our network referral decisions with the decisions made by human experts. Weighted
error scoring was used to reflect the fact that a false negative result (failing to refer) is more detrimental than a false positive result
(making a referral when it was not warranted). Using these weighted penalty points, error rates were computed for the model and
for each of the human experts.
Occlusion Test
Similarly to the methods described by Lee et al. and Zeiler and Fergus, an occlusion test was performed to identify the areas
contributing the most to the neural network’s assignment of the predicted diagnosis(Lee et al., 2016; Zeiler and Fergus, 2014). A blank
20x20 pixel box was systematically moved across every possible position in the image and the probabilities of the disease were
recorded. The highest drop in the probability represents the region of interest that contributed the highest importance to the deep
learning algorithm (Figure 5A, see also Figure S5 for additional examples).
QUANTIFICATION AND STATISTICAL ANALYSIS
The 207,130 images collected were reduced to the 108,312 OCT images (from 4686 patients) and used for training the AI platform.
Another subset of 633 patients not in the training set was collected based on a sample size requirement of 583 patients to detect
sensitivity and specificity at 0.05 marginal error and 95% confidence. The test images (n = 1000) were used to evaluate model
and human expert performance. Receiver operating characteristics (ROC) curves plot the true positive rate (sensitivity) versus the
false positive rate (1 – specificity). ROC curves were generated using classification probabilities of urgent referral versus otherwise
and the true labels of each test image and the ROC function of the Python scikit-learn library. The area under the ROC curve is a
measure of performance and the true positive rate (TPR or sensitivity) at some chosen true negative rate (TNR or specificity) on
the ROC curve is the probability that the classifier will rank a randomly chosen ‘‘urgent referral’’ instance higher than a randomly
chosen normal or drusen instance. Accuracy was measured by dividing the number of correctly labeled images by the total number
of test images. Sensitivity and specificity were determined by dividing the total number of correctly labeled urgent referrals and the
total number of correctly labeled non-urgent referrals, respectively, by the total number of test images.
DATA AND SOFTWARE AVAILABILITY
All deep learning methods were implemented using either TensorFlow (https://www.tensorflow.org). ImageNet, a public database of
images, can be found at https://www.image-net.org. Dataset on high resolution JPEG OCT and chest X-ray images are deposited
into the public Mendeley database (https://doi.org/10.17632/rscbjbr9sj.3).
Cell 172, 1122–1131.e1–e2, February 22, 2018 e2
Supplemental Figures
Figure S1. Plots Showing Binary Performance in the Training and Validation Datasets Using TensorBoard, Related to Figure 3
Comparisons were made for choroidal neovascularization (CNV) versus normal (A), diabetic macular edema (DME) versus normal (B), and drusen versus
normal (C). Plots were normalized with a smoothing factor of 0.6 in order to clearly visualize trends. The validation accuracy and loss shows better performance
since images with more noise and lower quality were also included in the training set to reduce overfitting and help generalization of the classifier. Training
dataset: orange. Validation dataset: blue.
Figure S2. Receiver Operating Characteristic Curves for Binary Classifiers, Related to Figure 4
The corresponding area under the ROC curve (AUROC) for the graphs are 100% for choroidal neovascularization (CNV) versus normal (A), 99.87% for diabetic
macular edema (DME) versus normal (B), and 99.96% for drusen versus normal (C). The straight vertical and horizontal lines in (A) and the nearly straight lines in
(B) and (C) demonstrate that the binary convolutional neural network models have a near perfect classification performance.
Figure S3. Plots Depicting the Positive and Negative Likelihood Ratios with Their Corresponding 95% Confidence Intervals Marked, Related
to Figure 4
(A) The positive likelihood ratio is defined as the true positive rate over the false positive rate, so that an increasing likelihood ratio greater than 1 indicates
increasing probability that the predicted result is associated with the disease.
(B) The negative likelihood ratio is defined as the false negative rate over the true negative rate, so that a decreasing likelihood ratio less than 1 indicates increasing
probability that the predicted result is associated with the absence of disease.
The confidence intervals show that the best trained model demonstrated statistically similar screening performance in when compared to human experts.
(legend on next page)
Figure S4. Proposed Penalties for Incorrect Labeling during Weighted Error Calculations and Confusion Matrix of Experts Grading OCT
Images, Related to Figure 4
(A) The penalties include an error score of 4 for ‘‘urgent referrals’’ scored as normal and an error score of 2 for ‘‘urgent referrals’’ scored as drusen. All other
incorrect answers carry an error score of 1.
(B) The results for each of the human experts is depicted here, comparing the true labels and the predicted labels for each individual grader.
Figure S5. Occlusion Maps of Diabetic Macular Edema, Choroidal Neovascularization, and Drusen, Related to Figure 5
(Top) Diabetic macular edema (DME), (middle) choroidal neovascularization (CNV), and (bottom), drusen. Additional examples of occlusion test images, illustrating how an occluding kernel was convolved across the input image to identify areas contributing to the algorithm’s determination of the diagnosis.
Figure S6. Illustrative Examples of Chest X-Rays in Patients with Pneumonia, Related to Figure 6
The normal chest X-ray (left panel) depicts clear lungs without any areas of abnormal opacification in the image. Bacterial pneumonia (middle) typically exhibits a
focal lobar consolidation, in this case in the right upper lobe (white arrows), whereas viral pneumonia (right) manifests with a more diffuse ‘‘interstitial’’ pattern in
both lungs.

ORIGINAL RESEARCH • SPECIAL REPORT
Despite widespread utilization of infection control measures, severe acute respiratory syndrome coronavirus 2
continues to cause frequent recurring outbreaks of coronavirus disease 2019 (COVID-19) throughout the world
(1,2). To date, COVID-19 remains clinically unpredictable, demonstrating significant potential to rapidly overwhelm health care infrastructure (1,2). The inability to
quickly detect outbreaks, mobilize testing and medical
care resources, and guide treatment—including predicting
which patients will experience a mild course and which patients will require advanced life support—remains a major
challenge for health care systems worldwide (1,3). Investments into infrastructure capable of maximizing the data
and information generated by practice-based evidence for
COVID-19 detection, prognostication, and management,
including diagnostic imaging, will be critical to optimize
the use of limited resources and guide global strategy (4).
Chest imaging, including both radiography and CT,
is a vital tool for the detection, quantification, and clinical management of COVID-19 (5–7). At the same time,
the field of machine learning has demonstrated potential
value for rapid medical imaging analysis in COVID-19
(8), leveraging imaging data biomarkers to help diagnose,
quantify, and predict disease severity in patients with COVID-19 (9–11). Although routine screening by means of
chest imaging alone for the identification of COVID-19
is not recommended by most radiology societies (12), its
frequent use in practice for both patients with respiratory
symptoms or persons under investigation for COVID-19
and in management of those confirmed to have the
The RSNA International COVID-19 Open Radiology
Database (RICORD)
Emily B. Tsai, MD* • Scott Simpson, DO* • Matthew P. Lungren, MD, MPH • Michelle Hershman, MD •
Leonid Roshkovan, MD • Errol Colak, MD • Bradley J. Erickson, MD, PhD • George Shih, MD •
Anouk Stein, MD • Jayashree Kalpathy-Cramer, PhD • Jody Shen, MD • Mona Hafez, MD • Susan John, MD •
Prabhakar Rajiah, MD • Brian P. Pogatchnik, MD • John Mongan, MD, PhD • Emre Altinmakas, MD •
Erik R. Ranschaert, MD, PhD • Felipe C. Kitamura, MD, MSc • Laurens Topff, MD • Linda Moy, MD •
Jeffrey P. Kanne, MD • Carol C. Wu, MD
From the Department of Radiology, Stanford University, Stanford, Calif (E.B.T., J.S., B.P.P.); Department of Radiology, University of Pennsylvania Hospital, Philadelphia,
Pa (S.S., M. Hershman, L.R.); Department of Radiology, Stanford University School of Medicine, Stanford University Medical Center, 725 Welch Rd, Room 1675, Stanford, CA 94305-5913 (M.P.L.); Department of Medical Imaging, University of Toronto, Unity Health Toronto, Toronto, Canada (E.C.); Department of Radiology, Mayo
Clinic, Rochester, Minn (B.J.E., P.R.); Department of Radiology, Weill Cornell Medicine, New York, NY (G.S.); MD.ai, New York, NY (A.S.); Department of Radiology,
Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Harvard Medical School, Charlestown, Mass (J.K.C.); Department of Diagnostic
and Interventional Radiology, Cairo University Kasr Alainy Faculty of Medicine, Cairo, Egypt (M. Hafez); Department of Radiology, The Ottawa Hospital, Ottawa,
Canada (S.J.); Department of Radiology and Biomedical Imaging, Center for Intelligent Imaging, San Francisco, Calif (J.M.); Department of Radiology, Koç University
School of Medicine, Koç University Hospital, Istanbul, Turkey (E.A.); Department of Radiology, ETZ Hospital, Tilburg, the Netherlands (E.R.R.); Department of Radiology, University of Ghent, Ghent, Belgium (E.R.R.); Department of Diagnostic Imaging, Universidade Federal de São Paulo, São Paulo, Brazil (F.C.K.); Department of Radiology, Netherlands Cancer Institute, Amsterdam, the Netherlands (L.T.); Department of Radiology, NYU Grossman School of Medicine, Center for Advanced Imaging
Innovation and Research, Laura and Isaac Perlmutter Cancer Center, New York, NY (L.M.); Department of Radiology, University of Wisconsin School of Medicine and
Public Health, Madison, Wis (J.P.K.); and Department of Thoracic Imaging, University of Texas MD Anderson Cancer Center, Houston, Tex (C.C.W.). Received October
9, 2020; revision requested October 27; revision received November 10; accepted December 8. Address correspondence to M.P.L. (e-mail: mlungren@stanford.edu).
* E.B.T. and S.S. contributed equally to this work.
Conflicts of interest are listed at the end of this article.
See also the editorial by Bai and Thomasian in this issue.
Radiology 2021; 299:E204–E213 • https://doi.org/10.1148/radiol.2021203957 • Content codes:
The coronavirus disease 2019 (COVID-19) pandemic is a global health care emergency. Although reverse-transcription polymerase
chain reaction testing is the reference standard method to identify patients with COVID-19 infection, chest radiography and CT
play a vital role in the detection and management of these patients. Prediction models for COVID-19 imaging are rapidly being
developed to support medical decision making. However, inadequate availability of a diverse annotated data set has limited the
performance and generalizability of existing models. To address this unmet need, the RSNA and Society of Thoracic Radiology
collaborated to develop the RSNA International COVID-19 Open Radiology Database (RICORD). This database is the first
multi-institutional, multinational, expert-annotated COVID-19 imaging data set. It is made freely available to the machine learning community as a research and educational resource for COVID-19 chest imaging. Pixel-level volumetric segmentation with
clinical annotations was performed by thoracic radiology subspecialists for all COVID-19–positive thoracic CT scans. The labeling
schema was coordinated with other international consensus panels and COVID-19 data annotation efforts, the European Society of
Medical Imaging Informatics, the American College of Radiology, and the American Association of Physicists in Medicine. Studylevel COVID-19 classification labels for chest radiographs were annotated by three radiologists, with majority vote adjudication by
board-certified radiologists. RICORD consists of 240 thoracic CT scans and 1000 chest radiographs contributed from four international sites. It is anticipated that RICORD will ideally lead to prediction models that can demonstrate sustained performance
across populations and health care systems.
©RSNA, 2021
Online supplemental material is available for this article.
This copy is for personal use only. To order printed copies, contact reprints@rsna.org
Tsai and Simpson et al
Radiology: Volume 299: Number 1—April 2021  n  radiology.rsna.org E205
open medical imaging data set for COVID-19 thoracic imaging
made available initially through The Cancer Imaging Archive
(TCIA). Additional data will continue to be collected, annotated, and shared, including additional imaging modalities and
accompanying clinical data, as part of an expanded effort collaboration with the American College of Radiology and the American Association of Physicists in Medicine to create a new Medical Imaging and Data Resource Center. These data will continue
to be made freely available for general research and education.
Materials and Methods
The primary goal of creating the RICORD data set was to
achieve a large and heterogeneous database focused on COVID-19 chest imaging that represents a diversity of variables across patient populations, imaging equipment, and
protocols, which introduced many regulatory and logistical
challenges. A second goal was to focus on streamlining imaging data de-identification and transmission such that data
aggregation was possible. These goals required custom-built
open-source engineering tools to facilitate the process toward
successful uniform Digital Imaging and Communications in
Medicine (DICOM) study de-identification and submission
to a central resource.
A third goal of the RICORD data set was to include multimodal imaging, both planar imaging in the form of chest radiography and volumetric data (CT image series). Multimodal
images were collected along with relevant clinical and demographic data, presenting unique challenges due to data set complexity. In RICORD, we sought to include expert annotation
in the setting of a new disease, with many available and nascent
classification schemas requiring both education and several
rounds of consensus and coordination with similar data annotation efforts by other large organizations around the world. Moreover, the size and complexity of the data set and use of image series required assembling, coordinating, training, and monitoring
a dedicated group of expert annotators to complete the project
in a short time frame. Finally, data set hosting and data vending required infrastructure capable of handling large volumes of
imaging studies, including experience with public data sets
and requisite privacy and security mechanisms. An international
task force composed of imaging, biostatistical, and data science
experts was assembled to create a staged deployment approach
(Fig 1) completed in a relatively short time frame to provide
value during the ongoing pandemic.
Data Inclusion Criteria
An international call for COVID-19 medical imaging data submission was issued by means of an open survey by the RSNA
in April 2020. More than 200 individual responses from 20
countries were received. As part of an aggressive release timeline, four initial sites from four countries were selected for this
first release of RICORD based on the availability of curated
COVID-19 data and presence of existing data sharing agreements with the RSNA. As of this writing, dozens of additional
sites are engaged in active data use agreement review with the
RSNA. International laws, institutional regulations and permissions, ethics review, and the availability of access to curated
Abbreviations
COVID-19 = coronavirus disease 2019, DICOM = Digital Imaging
and Communications in Medicine, RICORD = RSNA International
COVID-19 Open Radiology Database, TCIA = The Cancer Imaging
Archive
Summary
The RSNA International COVID-19 Open Radiology Database, or
RICORD, is the first multi-institutional, multinational, expert-annotated coronavirus disease 2019 imaging data set made freely available
and designed for the machine learning community.
Key Results
n The RSNA International COVID-19 Open Radiology Database
(RICORD) is a multi-institutional, multinational, expert-annotated coronavirus disease 2019 (COVID-19) imaging data set made
openly available to the research community.
n In RICORD, pixel-level volumetric segmentation with clinical
annotations was performed by thoracic radiology subspecialists
for all COVID-19–positive thoracic CT imaging studies, and
COVID-19 classification labeling for chest radiographs was performed by three board-certified radiologists.
n At this time, RICORD consists of 240 thoracic CT images and
1000 chest radiographs.
infection has nonetheless increased worldwide (13). In addition, given the prevalence of asymptomatic and presymptomatic infection, as well as early evidence for limited immunity in
exposed populations, COVID-19 is likely to be a globally endemic disease in the future. This underscores the importance of
identifying outbreaks and public health planning, particularly
as patients will have incidentally detected findings at routine
imaging that could be attributable to COVID-19 pneumonia
and could expose others in the health care system, especially
vulnerable populations (1).
Since the outbreak of the virus, there have been many publications related to COVID-19 (14), including dozens of automated imaging analysis models; however, the majority have
underperformed in validation experiments (15,16). This failure
to generalize to new data sets is often multifactorial, owing in
part to limited single-site data, lack of expert annotation, and
heterogeneous or inconsistent labeling schemas (15,16). The
shortcomings of these early efforts highlight the importance of
shared research and education resources for COVID-19. Large
data sharing collaborations are an effective strategy to pool medical data to address shortcomings related to data availability, generalization, and coordinating annotation frameworks (17,18).
Unfortunately, even in the face of a health care crisis, incentives
for collecting, annotating, and sharing medical data, especially
imaging data, are not well established. Prior experience curating
large, annotated, multisite medical imaging data sets has been
related primarily to hosting machine learning challenges or as
part of clinical trials or individual institution efforts (19–23).
New initiatives are needed that can better operationalize the collection, preparation, annotation, and access to large medical imaging data sets to address public health crises.
The purpose of this work is to describe the RSNA International COVID-19 Open Radiology Database (RICORD) data
set as the first multinational, multimodality, expert-annotated 
The RSNA International COVID-19 Open Radiology Database (RICORD)
E206 radiology.rsna.org  n Radiology: Volume 299: Number 1—April 2021
Portability and Accountability Act. Selection criteria included
(a) patients at least 18 years of age who underwent chest imaging (CT and/or radiography) for suspected COVID-19 infection (eg, International Classification of Diseases, 10th Revision,
Clinical Modification code U07.1 among discharge diagnosis)
and (b) positive infection confirmed with one or more of the
COVID-19 imaging data serve as ongoing barriers that require
time to resolve.
Institutional review board (ethics committee) approval was
obtained from all four sites for this retrospective study. For
the United States site, a waiver of informed consent was obtained, and processes were compliant with the Health Insurance
Figure 1: Stepwise pathway for coronavirus disease 2019 (COVID-19) imaging data contribution. CTP = clinical trials processor, RICORD = RSNA International COVID-19 Open Radiology Database.
Figure 2: Pathway for the annotation and curation for the RSNA International COVID-19 Open Radiology Database. COVID-19 = coronavirus disease 2019,
DICOM = Digital Imaging and Communications in Medicine, TCIA = The Cancer Imaging Archive.
Tsai and Simpson et al
Radiology: Volume 299: Number 1—April 2021  n  radiology.rsna.org E207
the hospitalization and had a confirmed test available after hospital discharge.
For chest CT examinations, the axial series was requested with
a section thickness of 2.5–5.0 mm and any protocol or kernel,
with preference for soft-tissue kernel. The section thickness was
set partly to facilitate the section-by-section segmentation and
annotation of data. Prior studies have successfully used varied
following: reverse-transcription polymerase chain reaction test,
immunoglobulin M antibody test, or clinical diagnosis using
hospital-specific criteria. For the purposes of this data set, criteria
for positive infection encompassed patients who were diagnosed
before hospitalization but remained symptomatic during the
hospitalization, patients with a positive test or diagnosis during
hospital admission, and patients who were symptomatic during
Figure 3: Example of CT scans and chest radiographs in the RSNA International COVID-19 Open Radiology Database. (a) Annotated axial CT image shows segmentation of characteristic bilateral multifocal ground-glass opacities in predominantly peripheral distribution (orange regions of interest). The CT image was classified as
having typical appearance of coronavirus disease 2019 (COVID-19) pneumonia. (b) Annotated axial CT image shows segmentation of bilateral multifocal ground-glass
opacities with diffuse distribution (orange regions of interest). The CT image was classified as having indeterminate appearance of COVID-19 pneumonia. (c) Thoracic
CT image shows bilateral nodular and patchy opacities with peripheral and lower lung predominance involving four lung zones, annotated as typical for COVID-19 with
moderate severity. (d) Thoracic CT image shows bilateral nodular and patchy opacities with peripheral and lower lung predominance involving more than four lung zones,
annotated as typical appearance for COVID-19 and severe lung involvement. (e) Bedside chest radiograph with bilateral patchy and nodular opacities (arrows) with
upper lung predominance involving more than four lung zones, annotated as indeterminate appearance for COVID-19 and severe lung involvement. (f) Bedside chest
radiograph shows left lower lobe opacities (arrows) with small left pleural effusion involving a single lung zone, annotated as atypical appearance for COVID-19 and mild
lung involvement. (g) Bedside chest radiograph shows bilateral patchy and nodular opacities (arrows) with upper lung predominance involving more than four lung zones,
annotated as indeterminate appearance for COVID-19 and severe lung involvement. (h) Bedside chest radiograph shows left lower lobe opacity (arrow) with small left
pleural effusion involving a single lung zone, annotated as atypical appearance for COVID-19 and mild lung involvement.
The RSNA International COVID-19 Open Radiology Database (RICORD)
E208 radiology.rsna.org  n Radiology: Volume 299: Number 1—April 2021
need to homogenize the de-identification approach as a way
to mitigate problematic heterogeneity in de-identification
methodologies. Unique pseudonymous identifiers are created
for each patient such that subsequent imaging studies for the
same patient could be assigned to the same anonymous patient
identification number.
A customized, free, open-source DICOM de-identification
software solution named DICOM Anonymizer was created by
experienced DICOM experts at the RSNA based on recognized
standards and best practices implementing the RICORD Data
De-identification Protocol. The anonymizer generates a unique
numeric site identification parameter, or SITEID, as a way to
distinguish data sets contributed by different sites without exposing the actual identity of the contributing site. The specific
elements modified in the anonymizer script and selected relevant sections of the DICOM standard can be found here: https://
www.rsna.org/-/media/Files/RSNA/Covid-19/RICORD/RSNACovid-19-Deidentification-Protocol.pdf.
To facilitate multiple workflows, three options were set up to
import DICOM objects into DICOM Anonymizer, including
(a) importing from a local storage location, (b) direct picture archiving and communication system query (through a DICOM
C-MOVE transfer), and (c) the use of accession numbers to allow a user to enter a list of accession numbers by hand, copy and
section thicknesses to train machine learning models (24). For
chest radiography, any frontal radiograph and method of acquisition (portable, anteroposterior, posteroanterior) was accepted.
The following data elements were obtained for each examination: (a) Unique Patient identifier, (b) Unique Imaging Examination identifier, (c) age, (d) sex, and (e) COVID-19 testing
method (polymerase chain reaction, serological, clinical). These
data elements were standardized with the de-identification tool
(see following section). In the frequent case that a given patient
underwent multiple thoracic imaging examinations, relative
time of acquisition was maintained with the de-identification
to enable timeline preservation in the aggregate data set. COVID-19 testing methods and other supporting clinical information, if available, were provided in a spreadsheet and linked with
the patient-unique identification number and imaging study
identification number.
De-Identification
Contributing sites were responsible for de-identifying imaging
examinations and associated clinical information for inclusion
in RICORD with expectations for use as a public research and
educational database (noncommercial use). Although all sites
followed their institutional policies and procedures, adhering to
locally applicable regulations and best practices, there was a
Table 1: Descriptive Image-Level Labels for CT Annotations
Image-Level
Annotation Label
Examination-Level Annotation Labels
COVID-19
Classification
Specific and
Additional
Imaging Findings
Type of Lung
Disease
IV Contrast
Material
Quality
Control
Presence
of Support
Apparatus
Infectious opacity Typical Halo sign Normal lung With IV contrast
material
Adequate Endotracheal
tube
Infectious tree-in-bud
and/or micronodules
Indeterminate Reversed halo sign Infectious lung
disease
Without IV
contrast
material
Inadequate:
motion and/or
breathing
Central venous
and/or
arterial line
Infectious cavity Atypical Reticular pattern
without parenchymal
opacity
Emphysema Inadequate:
insufficient
inspiration
Nasogastric
tube
Noninfectious nodule
or mass
Negative for
pneumonia
Perilesional vessel
enlargement
Oncologic lung
disease
Inadequate:
low spatial resolution
Sternotomy
wires
Atelectasis Bronchial wall
thickening
Noninfectious
inflammatory
lung disease
Inadequate:
incomplete
lungs
Pacemaker
Other noninfectious
opacity
Bronchiectasis Noninfectious
interstitial
lung disease
Inadequate:
wrong body part
and/or modality
Other support
apparatus
Subpleural
curvilinear
Fibrotic lung
disease
Effusion, pleural thickening, pneumothorax,
pericardial, lymphadenopathy, pulmonary
embolism
Other lung
disease
Note.—Image-level annotations were applied as free-form shapes. Examination-level annotations were used in a “choose all that apply” approach such that annotations could include more than one of the labels. COVID-19 = coronavirus disease 2019, IV = intravenous.
Tsai and Simpson et al
Radiology: Volume 299: Number 1—April 2021  n  radiology.rsna.org E209
download. The data are organized as collections, typically images related by a common disease (eg, lung cancer), image modality or type (MRI, CT, digital histopathologic examination,
etc), and/or research focus. DICOM is the primary file format
used by TCIA for medical imaging. A DICOM file stores the
digital image along with a series of tags that contain metadata
about the image, such as patient identification number, study
identification number, patient weight, and anatomic site. More
information about DICOM is available at http://medical.nema.
org. Supporting data related to the images, such as patient outcomes, treatment details, genomics, and expert analyses, are
also provided when available. This public-facing data hosting
and vending workflow enables the wide distribution and use of
this imaging resource, and RICORD will be listed as a unique
collection within TCIA infrastructure. Any potential user can
then access TCIA to search for and download images and associated annotation files.
The terms outline that ownership of the submitted data is
retained by the submitting institution and, in agreeing to
participate, the submitting organization grants the RSNA a
nonexclusive, royalty-free, sublicensable, worldwide, perpetual,
irrevocable license to the submitted data available for commercial, scientific, and educational purposes, including rights
necessary for the RSNA to make the Curated Submission Data
available to the public pursuant to the Creative Commons Attribution 4.0 International License. Each submitting center must
attest that they have the authority and rights to grant the RSNA
paste a list, or open a text file containing a list and then start the
DICOM import process. Each participating site was provided
with documentation on how to install and use the DICOM
Anonymizer to prepare DICOM objects for submission to RICORD. Although successful use of the DICOM Anonymizer
required technical knowledge of the configuration of the picture
archiving and communication system and local networks, a webinar was hosted by members of the RSNA COVID-19 task
force as a technical walkthrough demonstration format followed
by an open question session from the audience. A discussion
board for users was also made available so that troubleshooting
and guidance could be more easily accessed by participating sites
with asynchronous interactions from RSNA technical staff and
volunteers.
Data Use Agreement and Hosting
Over the course of several iterations, a data submission agreement was crafted with the intent of setting the terms by which
the RSNA would collect and submit anonymized imaging data
from medical centers and research institutions. The agreement
describes the collaboration with existing National Institutes of
Health resources through TCIA to facilitate the submission,
processing, and publication of data. The agreement outlines
the process for data set review, confirmation of de-identification, and encrypted transmission. Housed within the National
Institutes of Health, TCIA is a service that de-identifies and
hosts a large archive of medical images accessible for public
Figure 4: Examples of image-level annotations on axial CT images indicated with orange regions of interest. (a) Infectious opacity segmented in the left upper lobe.
(b) Infectious tree-in-bud and/or micronodules segmented in the right lower lobe. (c) Infectious cavity segmented in the right upper lobe. (d) Noninfectious nodule or mass
segmented in the posterior left pleura. (e) Atelectasis segmented in the left lower lobe. (f) Other noninfectious opacity segmented in the right lower lobe.
The RSNA International COVID-19 Open Radiology Database (RICORD)
E210 radiology.rsna.org  n Radiology: Volume 299: Number 1—April 2021
utable to COVID-19 pneumonia, including standardized
study-level annotation to reduce variability. This multisociety,
international, expert annotation consensus will help detail the
imaging appearance of COVID-19 pneumonia and aid in further data set enrichment and curation as well as model development across data sets. Examination-level binary annotations
were performed to indicate the presence of a support apparatus (endotracheal tube, central venous catheter, pacemaker,
feeding tube) as well as to describe global study image quality (adequate, motion artifact, incomplete). Each examination
was classified as having typical, indeterminate, atypical, or
negative appearance for COVID-19 pneumonia based on the
RSNA consensus statement (25) (Fig 3). Table 1 describes the
image-level and examination-level annotation labels. Examples
of possible image-level and examination-level annotations are
shown in Figure 4 and Figure 5, respectively.
Several decisions among a panel of five senior thoracic
radiologists (J.P.K., C.C.W., E.B.T., S.S., and G.S.; average
experience, 15 years) were made in developing the annotation schema around specific findings and coordinated with
leadership in other similar international parallel efforts. Only
opacities and nodules greater than 1 cm were annotated, and
clinical judgment as to whether a nodule or mass was infectious was made by the annotating specialist radiologist and
annotated as “infectious opacity.” For infectious micronodules and tree-in-bud, a region of interest was added that
a license to publish and otherwise use the Submission Data and
Curated Submission Data in accordance with the terms of the
agreement. A fully executed signed agreement was required of
all participating sites prior to data submission and can be found
in its entirety at the link included here: https://www.rsna.org/-/
media/Files/RSNA/Covid-19/RICORD/RSNA-Data-SubmissionAgreement-Form.pdf.
Annotation
The annotation strategy was focused on balancing the richness
of the annotations with the effort required to perform annotations. In aggregate for RICORD, 240 unique chest CT scans
and 1000 unique chest radiographs were selected from the
initial contributing institutions. The entire process from data
solicitation to collation of the final data set is summarized in
the workflow diagram (Fig 2).
Chest CT scan annotation.—For the chest CT segmentation
task, a small portion of each data set from contributing institutions was identified for full hand segmentation and annotation
by six subspecialist thoracic radiologists (average experience, 6
years). The annotation schema was designed in coordination
with Society of Thoracic Radiology and European Society of
Medical Imaging Informatics leadership to ensure consistency.
Annotation was focused on current understanding and guidance in reporting chest imaging findings potentially attribFigure 5: Examples of examination-level annotations on axial CT images. (a) Ground-glass opacities surrounding a nodular opacity (arrow) in the left lower
lobe (halo sign). (b) Bilateral ground-glass opacities (arrows) with central clearing (reversed halo sign). (c) Reticular pattern without parenchymal opacity in
the left upper lobe (arrows). (d) Perilesional vessel enlargement associated with bilateral ground-glass opacities (arrows). (e) Bronchial wall thickening most
evident in the right lung (arrows). (f) Bronchiectasis in the left upper lobe (arrows). (Fig 5 continues.)
Tsai and Simpson et al
Radiology: Volume 299: Number 1—April 2021  n  radiology.rsna.org E211
encompassed the entire region of nodularity rather than as
individual nodules. In studies where atelectasis was associated with infectious opacities, atelectasis was annotated as
an “infectious opacity.” Alveolar edema was labeled as “other
noninfectious opacity.” Lymphadenopathy, regardless of location, was defined as lymph nodes larger than 1 cm short axis.
Multiple examination-level labels for type of lung disease
were allowed if deemed appropriate to provide a differential
diagnosis based on clinical judgment.
Chest radiograph annotation.—Selection of the annotation
schema for chest radiographs was based on a study-level scoring system and suggested reporting language developed by
subspecialist thoracic radiologists (J.P.K. and C.C.W.) (26).
Each chest radiograph was classified as typical, indeterminate,
atypical, or negative for findings of COVID-19 pneumonia.
Figure 5 (continued). (g) Bilateral subpleural curvilinear lines (arrows). (h) Small bilateral pleural effusions
(arrows). (i) Right pleural thickening (arrows). (j) Right pneumothorax (arrows). (k) Pericardial effusion (arrow).
(l) Mediastinal lymphadenopathy (arrows) in the prevascular and bilateral lower paratracheal stations. (m)
Pulmonary emboli (arrows) in the right lower and middle lobar pulmonary arteries.
In all except the negative cases, the number of regions with
abnormal opacities were assessed and classified as showing
mild, moderate, or severe disease. As there is subjectivity in
this labeling schema, triple annotation was performed, and
majority consensus was selected as the final label when at least
two of three annotators selected the same classifications. In
cases without a majority consensus, final adjudication was
performed by one of two experienced subspecialist thoracic
radiologists (average experience, 15 years). Annotation was
performed using a commercial browser-based application
(MD.ai). Annotators were solicited from membership of the
Society of Thoracic Radiology and the RSNA, and three (J.S.,
P.R., and M. Hafez, with 5, 7, and 8 years of experience, respectively) were selected based on their performance on previous chest imaging annotation exercises conducted by these
organizations. Each of the three annotators were blinded to
each other but were able to work on the data at the same
time. Adjudicators (J.P.K. and C.C.W.) were able to view the
annotations from the three initial readers.
The RSNA International COVID-19 Open Radiology Database (RICORD)
E212 radiology.rsna.org  n Radiology: Volume 299: Number 1—April 2021
characteristics, and other included metadata will be critical when
generating cohorts with RICORD, particularly as additional
public COVID-19 imaging data sets are made available through
complementary and parallel efforts. It is important to emphasize
that there are limitations to the clinical ground truth, as severe
acute respiratory syndrome coronavirus 2 reverse-transcription
polymerase chain reaction tests have widely documented limitations and are subject to both false-negative and false-positive
results (1,4), which impacts the distribution of the included
imaging data and may have led to an unknown epidemiologic
distortion of patients based on the inclusion criteria. These
limitations notwithstanding, RICORD has achieved the stated
objectives for data complexity, heterogeneity, and high-quality
expert annotations as a comprehensive COVID-19 thoracic imaging data resource.
The 240 CT scans and 1000 radiographs represent a subset of
images uploaded by the contributing sites. Because of human resource limitations and time constraints, annotation was limited
to 120 COVID-19–positive CT scans and 1000 COVID-19–
positive chest radiographs. Cases were randomly selected from
each site. Additional data from these sites and other contributing sites will be included in subsequent releases. Because making
COVID-19–related cases available quickly was a high priority,
and because several public chest radiograph data sets providing cases of non–COVID-19 pneumonia and other conditions
for differential analysis are already available, only COVID-19–
positive chest radiography studies have been published initially.
There are plans to supplement these with COVID-19–negative
chest radiography cases from contributing sites. Users could also
train their model on the 2018 RSNA Pneumonia Challenge data
set (27). Finally, the Medical Imaging and Data Resource Center
project achieved its target goal of providing 10000 COVID-19–
related studies before the end of 2020.
This initial RSNA International COVID-19 Open Radiology
Database (RICORD) data set is the first of a larger coordinated
effort called the Medical Imaging and Data Resource Center,
which is an open-source medical imaging database and machine
learning effort co-led by the RSNA, the American College of
Radiology, the American Association of Physicists in Medicine,
and the National Institute of Biomedical Imaging and Bioengineering at the National Institutes of Health. The Medical Imaging and Data Resource Center will create a separate platform to
Data Set Description
The RICORD data set is composed of 240 thoracic CT scans
and 1000 chest radiographs contributed from four international sites (details in Appendix E1 [online]). Care was taken to
curate a balanced data set across the contributing sites. Patient
demographics collected included sex, age, and COVID-19
testing status and method. Both CT scan and chest radiograph
data included multiple studies for some patients, with a higher
prevalence of such studies in the chest radiography data set.
Date of study was randomized, but intervals between studies
for a given patient were maintained. Descriptive statistics and
demographics are listed in Tables 2 and 3.
Discussion
The RSNA International COVID-19 Open Radiology Database is the largest publicly available expert-annotated data set
of chest CT scans and chest radiographs formatted in accordance with the Digital Imaging and Communications in Medicine standard. The data sets were acquired from institutions
in four countries, representing diverse patient populations and
heterogeneous imaging protocols and vendors. The radiologic
examinations were performed for various clinical indications
related to coronavirus disease 2019 (COVID-19): to obtain diagnostic criteria for COVID-19, to determine disease severity,
or to assess treatment response. Hundreds of hours were spent
by volunteer radiologists to compile, curate, and annotate this
initial working data set, which is made available through The
Cancer Imaging Archive, a service sponsored by the National
Institutes of Health that hosts a large archive of medical images
accessible for public download.
Because this is a public data set, RICORD is available for
noncommercial use (and further enrichment) by research and
education communities, which may include the development of
educational resources for COVID-19 and use of RICORD to
create artificial intelligence systems for diagnosis and quantification, benchmarking performance for existing solutions, exploration of distributed and/or federated learning, further annotation
or data augmentation efforts, and evaluation of the examinations
for disease entities beyond COVID-19 pneumonia. Deliberate
consideration of the detailed annotation schema, demographic
Table 2: Descriptive Demographic Information for RICORD
Studies by Collection
Parameter Collection 1a Collection 1b Collection 1c
Modality CT CT Radiography
No. of patients 110 117 361
No. of studies 120 120 1000
No. of COVID-19–
positive studies
120 0 1000
No. of female patients 41 (37) 61 (52) 148 (41)
Note.—Numbers in parentheses are percentages. Collection 1a
comprises COVID-19–positive CT scans; collection 1b, COVID-19–negative CT scans; and collection 1c, COVID-19–positive chest radiographs. COVID-19 = coronavirus disease 2019,
RICORD = RSNA International COVID-19 Open Radiology
Database.
Table 3: Positive and Negative Labels Referring to RT-PCR
Confirmation of COVID-19 Status
Site Collection 1a Collection 1b Collection 1c
Site A 28 30 76
Site B 19 30 0
Site C 58 30 433
Site D 15 30 491
Note.—Data are numbers of studies by contributing site. Collection 1a comprises COVID-19–positive CT scans; collection
1b, COVID-19–negative CT scans; and collection 1c, COVID-19–positive chest radiographs. COVID-19 = coronavirus
disease 2019, RT-PCR = reverse-transcription polymerase chain
reaction.
Tsai and Simpson et al
Radiology: Volume 299: Number 1—April 2021  n  radiology.rsna.org E213
collect, annotate, store, and share coronavirus disease 2019–related medical images via open-access Gen3 data commons, beginning with RICORD. Engagement with multiple subspecialty
societies to leverage their unique expertise in developing clinical
use cases and high-quality annotated data sets is an effective and
useful model to follow for future collaborations. This joint effort will allow streamlining and consistency in methods of data
collection and provide broader aggregation, better organization,
and more convenient access to data for researchers and educators.
Acknowledgments: The authors extend their immense gratitude to the following
contributors without whom this work would not have been possible: Chris Carr,
RSNA; John Perry, RSNA; Hakan Dogan, MD, Koç University Hospital, Istanbul,
Turkey; Hui-Ming Lin, Unity Health Toronto, Toronto, Canada; Hojjat Salehinejad,
Unity Health Toronto, Toronto, Canada; Nitamar Abdala, the Federal University of
São Paulo (UNIFESP); Henrique Carrete Jr, UNIFESP; Ernandez Santos, UNIFESP;
Henrique Souza Alves, UNIFESP; Natalia Dutra Cavalcante, UNIFESP.
Author contributions: Guarantors of integrity of entire study, E.B.T., S.S., M.P.L.,
F.C.K.; study concepts/study design or data acquisition or data analysis/interpretation, all authors; manuscript drafting or manuscript revision for important intellectual content, all authors; approval of final version of submitted manuscript, all authors; agrees to ensure any questions related to the work are appropriately resolved,
all authors; literature research, E.B.T., M.P.L., E.C., G.S., M. Hafez, E.R.R., L.M.,
C.C.W.; clinical studies, S.S., M.P.L., M. Hershman, L.R., J.S., M. Hafez, S.J., P.R.,
E.A., E.R.R., F.C.K.; experimental studies, E.B.T., M. Hershman, E.C., A.S., P.R.,
B.P.P., F.C.K.; statistical analysis, E.C., A.S.; and manuscript editing, E.B.T., S.S.,
M.P.L., M. Hershman, L.R., E.C., B.J.E., G.S., J.K.C., M. Hafez, S.J., P.R., J.M.,
E.R.R., F.C.K., L.T., L.M., J.P.K., C.C.W.
Disclosures of Conflicts of Interest: E.B.T. disclosed no relevant relationships. S.S.
disclosed no relevant relationships. M.P.L. Activities related to the present article: disclosed no relevant relationships. Activities not related to the present article: received
compensation for board membership from Nines Radiology, Segmed, BunkerHill, and
Carestream; received compensation from Bayer for consulting work; has stock or stock
options in Nines Radiology, Segmed, BunkerHill, and Centaur. Other relationships:
disclosed no relevant relationships. M. Hershman disclosed no relevant relationships.
L.R. disclosed no relevant relationships. E.C. disclosed no relevant relationships. B.J.E.
disclosed no relevant relationships. G.S. Activities related to the present article: disclosed
no relevant relationships. Activities not related to the present article: is a board member
and consultant for MD.ai (no compensation received to date); has stock or stock options
in MD.ai. Other relationships: disclosed no relevant relationships. A.S. disclosed no
relevant relationships. J.K.C. Activities related to the present article: institution received
a grant from the National Institutes of Health. Activities not related to the present article: institution received or will receive grants from the National Institutes of Health,
National Science Foundation, and GE; received reimbursement for travel and meeting
expenses from IBM; serves as deputy editor of Radiology: Artificial Intelligence. Other
relationships: disclosed no relevant relationships. J.S. disclosed no relevant relationships.
M. Hafez disclosed no relevant relationships. S.J. disclosed no relevant relationships.
P.R. Activities related to the present article: disclosed no relevant relationships. Activities
not related to the present article: receives royalties from Elsevier/Amirsys. Other relationships: disclosed no relevant relationships. B.P.P. disclosed no relevant relationships. J.M.
Activities related to the present article: disclosed no relevant relationships. Activities not
related to the present article: institution received or will receive grants from GE and Nuance. Other relationships: disclosed no relevant relationships. E.A. disclosed no relevant
relationships. E.R.R. disclosed no relevant relationships. F.C.K. Activities related to the
present article: disclosed no relevant relationships. Activities not related to the present
article: received compensation from MD.ai for consulting; is employed by Diagnósticos
da América. Other relationships: disclosed no relevant relationships. L.T. disclosed no
relevant relationships. L.M. Activities related to the present article: disclosed no relevant
relationships. Activities not related to the present article: received compensation from
Lunit and iCAD for board membership; institution received or will receive grant(s)
from Siemens; has stock or stock options in Lunit. Other relationships: disclosed no
relevant relationships. J.P.K. Activities related to the present article: disclosed no relevant
relationships. Activities not related to the present article: received compensation from
Parexel International for consulting. Other relationships: disclosed no relevant relationships. C.C.W. Activities related to the present article: disclosed no relevant relationships.
Activities not related to the present article: institution received or will receive grants.
Other relationships: disclosed no relevant relationships.
References
1. Wiersinga WJ, Rhodes A, Cheng AC, Peacock SJ, Prescott HC. Pathophysiology,
Transmission, Diagnosis, and Treatment of Coronavirus Disease 2019 (COVID-19):
A Review. JAMA 2020;324(8):782–793.
2. Nadkarni GN. An ounce of public health for COVID-19? Sci Transl Med
2020;12(541):eabb5675.
3. Black A, MacCannell DR, Sibley TR, Bedford T. Ten recommendations for supporting
open pathogen genomic analysis in public health. Nat Med 2020;26(6):832–841.
4. Tromberg BJ, Schwetz TA, Pérez-Stable EJ, et al. Rapid Scaling Up of Covid-19
Diagnostic Testing in the United States - The NIH RADx Initiative. N Engl J Med
2020;383(11):1071–1077.
5. Bernheim A, Mei X, Huang M, et al. Chest CT Findings in Coronavirus Disease-19
(COVID-19): Relationship to Duration of Infection. Radiology 2020;295(3):200463.
6. Pan F, Ye T, Sun P, et al. Time Course of Lung Changes at Chest CT during Recovery
from Coronavirus Disease 2019 (COVID-19). Radiology 2020;295(3):715–721.
7. Chung M, Bernheim A, Mei X, et al. CT Imaging Features of 2019 Novel Coronavirus (2019-nCoV). Radiology 2020;295(1):202–207.
8. Mei X, Lee HC, Diao KY, et al. Artificial intelligence-enabled rapid diagnosis of
patients with COVID-19. Nat Med 2020;26(8):1224–1228.
9. Ai T, Yang Z, Hou H, et al. Correlation of Chest CT and RT-PCR Testing for Coronavirus Disease 2019 (COVID-19) in China: A Report of 1014 Cases. Radiology
2020;296(2):E32–E40.
10. Li L, Qin L, Xu Z, et al. Using Artificial Intelligence to Detect COVID-19 and
Community-acquired Pneumonia Based on Pulmonary CT: Evaluation of the Diagnostic Accuracy. Radiology 2020;296(2):E65–E71.
11. Liu Z, Jin C, Wu CC, et al. Association between Initial Chest CT or Clinical Features and Clinical Course in Patients with Coronavirus Disease 2019 Pneumonia.
Korean J Radiol 2020;21(6):736–745.
12. Rubin GD, Ryerson CJ, Haramati LB, et al. The Role of Chest Imaging in Patient
Management during the COVID-19 Pandemic: A Multinational Consensus Statement from the Fleischner Society. Radiology 2020;296(1):172–180.
13. Wan Y-L, Schoepf UJ, Wu CC, et al. Preparedness and Best Practice in Radiology
Department for COVID-19 and Other Future Pandemics of Severe Acute Respiratory Infection. J Thorac Imaging 2020;35(4):239–245.
14. Chen Q, Allot A, Lu Z. Keep up with the latest coronavirus research. Nature
2020;579(7798):193.
15. Wynants L, Van Calster B, Collins GS, et al. Prediction models for diagnosis and
prognosis of covid-19 infection: systematic review and critical appraisal. BMJ
2020;369:m1328 [Published correction appears in BMJ 2020;369:m2204.].
16. Suchá D, van Hamersvelt RW, van den Hoven AF, de Jong PA, Verkooijen HM. Suboptimal Quality and High Risk of Bias in Diagnostic Test Accuracy Studies on Chest
Radiography and Computed Tomography in the Acute Setting of the COVID-19
Pandemic: A Systematic Review. Radiol Cardiothorac Imaging 2020;2(4):e200342.
17. Cosgriff CV, Ebner DK, Celi LA. Data sharing in the era of COVID-19. Lancet
Digit Health 2020;2(5):e224.
18. Gates B. Responding to Covid-19 - A Once-in-a-Century Pandemic? N Engl J Med
2020;382(18):1677–1679.
19. Flanders AE, et al. Construction of a Machine Learning Dataset through Collaboration: The RSNA 2019 Brain CT Hemorrhage Challenge. Radiol Artif Intell
2020;2(3):e190211.
20. Halabi SS, Prevedello LM, Kalpathy-Cramer J, et al. The RSNA Pediatric Bone Age
Machine Learning Challenge. Radiology 2019;290(2):498–503.
21. Prevedello LM, et al. Challenges Related to Artificial Intelligence Research in Medical Imaging and the Importance of Image Analysis Competitions. Radiol Artif Intell
2019;1(1):e180031https://doi.org/10.1148/ryai.2019180031.
22. Prior FW, Clark K, Commean M, et al. TCIA: An information resource to enable
open science. In: 2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Osaka, Japan, July 3–7, 2013.
Piscataway, NJ: IEEE, 2013; 1282–1285.
23. Irvin J, Rajpurkar P, Ko M, et al. CheXpert: A Large Chest Radiograph Dataset with
Uncertainty Labels and Expert Comparison. In: Proceedings of the AAAI Conference on Artificial Intelligence 2019;33(1):590–597.
24. Lin Li, Lixin Qin, Zeguo Xu, et al. Using Artificial Intelligence to Detect COVID-19 and Community-acquired Pneumonia Based on Pulmonary CT: Evaluation
of the Diagnostic Accuracy. Radiology 2020;296(2):E65–E71.
25. Simpson S, Kay FU, Abbara S, et al. Radiological Society of North America Expert
Consensus Statement on Reporting Chest CT Findings Related to COVID-19. Endorsed by the Society of Thoracic Radiology, the American College of Radiology, and
RSNA - Secondary Publication. J Thorac Imaging 2020;35(4):219–227.
26. Litmanovich DE, Chung M, Kirkbride RR, Kicska G, Kanne JP. Review of Chest
Radiograph Findings of COVID-19 Pneumonia and Suggested Reporting Language.
J Thorac Imaging 2020. 10.1097/RTI.0000000000000541. Published online June 9,
2020.
27. Shih G, Wu CC, Halabi SS, et al. Augmenting the National Institutes of Health
Chest Radiograph Dataset with Expert Annotations of Possible Pneumonia. Radiol
Artif Intell 2019;1(1):e180041.

RIDER (Reference Database to Evaluate Response)
Committee Combined Report, 9/25/2008
Sponsored by NIH, NCI, CIP, ITDB
Causes of and Methods for Estimating/Ameliorating Variance
in the Evaluation of Tumor Change in Response to Therapy
CT Volumetrics subcommittee members:
Sam Armato
Reinhard Beichel
Luc Bidaut
Larry Clarke
Barbara Croft
Chuck Fenimore
Marios Gavrielides
Hyun (Grace) Kim
Lisa Kinnard
Geoffrey McLennan
Chuck Meyer
Nick Petrick
Anthony Reeves
Binsheng Zhao
Mike McNitt-Gray, Chair
MR subcommittee members:
Daniel Barboriak
Luc Bidaut
Larry Clarke
Barbara Croft
Chuck Meyer
Ed Jackson, Chair
PET-CT subcommittee members:
Sam Armato
Edwin JR van Beek
Luc Bidaut
Larry Clarke
Barbara Croft
Geoffrey McLennan
Paul Kinahan, Chair
This project has been funded in whole or in part with federal funds from the National
Cancer Institute, National Institutes of Health, under contract N01-CO-12400. The
content of this publication does not necessarily reflect the views or policies of the
Department of Health and Human Services, nor does mention of trade names,
commercial products, or organizations imply endorsement by the U.S. Government.
Introduction
Early detection of tumor response to therapy is a key goal. Finding a measurement tool capable
of early detection of tumor response could individualize therapy treatment as well as reduce the
cost of bringing new drugs to market. On an individual basis the urgency arises from the desire
to prevent continued treatment of the patient with a high cost, high risk regimen with no
demonstrated individual benefit, as well as the need to rapidly switch the patient to another
therapy that may increase treatment efficacy for that patient. Regarding the process of bringing
new drugs to market such tools could demonstrate efficacy in much smaller populations
allowing phase III trials to be have smaller populations and thus arrive at statistically significant
decisions in shorter durations. Such studies would be much less costly in time and dollars spent
to bring the drug to market.
The emphasis placed on the word “Early” implies that we are interested in the measurement
regime of zero change, i.e. the detection of truly small changes from whatever tool and
parameter set we measure. From detection theory we understand that our ability to do so rests
on the ratio of signal to noise, alternatively described as effect size to variance. As this ratio
increases we migrate from the condition being able to detect changes in large populations by
averaging, to the condition of using fewer subjects until we are able to detect such changes in
an individual with clinically useful statistical accuracy.
Given the large task required to implement these measurements across a broad spectrum of
tools and measurement parameters in search of optimal tools, this RIDER group has focused on
ways of estimating a measurement tool’s noise, i.e. variance, under the condition of no change
across several modalities and measurement techniques. Arguably the most realistic and useful
datasets representing zero change come from patients harboring tumors who are imaged,
removed from scanner, allowed to drink, snack, etc, and then are rescanned. We affectionately
refer to these interval exams as “coffee-break” exams. These datasets then represent all of the
realities of short interval imaging with whatever modality was used, i.e. tissue contrast to noise,
patient motion artifacts, repositioning errors, etc., that will be encountered in the real world.
Additionally because the time interval between scans is on the order of hours or less, we can
safely assume that there are no macroscopic changes to the tumor in the interim. Note that
datasets with expert annotations were not used due their demonstrated variability in
segmentation and thus lack of certainty in the change assumptions associated with using these
datasets 1 . An alternative to collecting these coffee-break experiments which describe all
sources of noise in the null hypothesis against which treatment effects can be compared, is the
collection of a large database of treatment trials along with clinical endpoints which can be
modeled to determine the sources of multiparametric covariance; we suggest that the collection
of coffee-break data may be far more efficacious at much lower collection cost.
Most importantly, in addition to the following body of text several groups have provided deidentified coffee-break datasets to NCIA (http://ncia.nci.nih.gov), NCI’s Archive web site, for
public downloading of this invaluable data, along with descriptions of their measurements, postprocessing analyses and results, so that their methodologies and results can be compared with
the results of others to follow. While some of the RIDER participants were partially funded for
these efforts by NCI sponsored contracts, others volunteered their time and data to participate.
We are indebted to all for their expertise, collegial participation and many hours of work.
crm

1 Meyer, CR, TD Johnson, G McLennan, DR Aberle, EA Kazerooni, H MacMahon, BF Mullan, DF
Yankelevitz, EJR van Beek, SG Armato III, MF McNitt-Gray, AP Reeves, D Gur, PH Bland, CI Henschke,
EA Hoffman, G Laderach, R Pais, A Starkey, D Qing, C Piker, J Guo, D Max, BY Croft, LP Clarke (2006)
Evaluation of lung MDCT nodule annotations across radiologists and methods, Acad Radiol 13(10):1254-
1265.
RIDER Project
CT Volumetrics Subcommittee
Summary Report v2
1. Research Questions to be asked regarding CT Volumetrics as a Biomarker for
response and how RIDER can contribute to this.
There are several open research questions that will help us establish whether CT
Volumetrics can be useful in determining a patient’s response to therapy (i.e. can serve as a
Biomarker of response). These include:
a. Patient Repeatability – How much variation can be expected when the same
acquisition, image analysis and image-derived measurement calculation are repeated
on the same patient over a very short time period?
b. Accuracy, Bias and Reproducibility using a test object (Phantom) – How close can
one match a known value (size, function, etc) using a specific image acquisition,
image analysis and image-derived measurement calculation? What are the effects of
changing parameters on the ability to obtain an accurate result? If the same phantom
is scanned on the same device, how much variance can we observe? Now move the
phantom to another device or another institution and measure variance, etc.
c. Patient Change – When a patient is imaged over time, is the change observed
predictive of clinical outcome?
These questions can also be asked in terms of a comparison with the clinically used RECIST
criteria.
The RIDER CT Volumetrics subcommittee has focused on the above questions specifically
in the context of the use of tumor volumes obtained from CT scans (primarily focusing on
thoracic CT and the measurement of lung tumor volumes). This subcommittee has
identified both an extended list of potential sources of bias and variance that could be
encountered in the measurement of tumor volumes using CT as well as sources of data that
could be used to help answer some of these questions. These are described below.
2. Potential Sources of Bias and Variance
There are many potential sources of bias and variance when measuring tumor volumes using
CT, especially for lesions in the lung. These have been broken into three major groups, with
the understanding that there may be significant interactions between sources of error (e.g.
covariance) ) linked to various effects. These include:
a. Patient Factors: (controllable factors in a patient or possibly controllable in a
phantom)
i. Breathhold (ability to hold breath during a single scan)
1. breathing motion
ii. Patient motion
1. voluntary (e.g. just not holding still during scan)
2. involuntary (e.g. cardiac motion)
3. cyclic v. non-cyclic motion (e.g. related to the possibility of gating)
iii. Inspiratory level (such as the consistency of breathold for all serial scans)
iv. Patient orientation/positioning
1. Supine or prone (gravity dependent effects)
2. Feet first or head first
v. Patient Size (e.g. body habitus)
vi. Implants (e.g. metallic implants, pacemakers, etc.)
vii. Abnormalities
1. Resected lungs
2. Scarring
3. Competing/Concomitant disease (e.g. inflammation)
4. High-density tissues (see also vi)
5. Girth of patients (e.g. obesity)
viii. Other Desirable Information
1. Patient medical history – such as treatment history
b. Lesion Factors (possibly controllable in a phantom)
i. nodule size
ii. nodule shape (e.g. speculation, lobulation)
iii. nodule density
iv. nodule texture
v. nodule margin
vi. nodule internal composition and homogeneity (see also vii)
vii. nodule margin
viii. nodule radiological solidity – solid or ground glass or mixed
ix. location – juxtapleural vs. mediastinal vs. juxtavascular vs. near bone (e.g. rib
or vertebra) or soft (e.g. muscle, heart) tissue
x. size and density of attaching vessel
c. Imaging Modality – specific factors that vary within CT
i. Differences between imaging hardware MDCT
1. different number of detector rows
2. different number and size of detector elements in each row
3. different number of sources (e.g. dual source CT)
4. different detector material
5. different x-ray spectrum/beam filtration (e.g softer vs. harder x-ray
beam)
6. different bowtie (compensating) filter
7. different acquisition, correction or reconstruction software principles
and versions (e.g. some features enabled)
ii. Differences in imaging protocol
1. different collimation setting
2. different mAs
3. different pitch
4. reconstructed slice thickness and spacing/overlap
5. varying slice thickness/spacing in different regions (e.g 5 mm
thickness through upper and lower lungs, 3 mm through hilar region)
6. reconstruction kernel
7. correction and reconstruction algorithms (e.g. different interpolation
scheme)
8. different kVp
9. Reconstructed Field of View
10. contrast vs. non-contrast
a. injection rate
b. volume of contrast used
i. including whether a weight based scheme is used to
determine total contrast amount
ii. concentration of contrast material used (there are
different concentrations sold commercially)
iii. type/physiology of contrast material
c. delay (e.g. delay time used to determine when scan should
start following IV injection)
11. Overlap or non-overlapped acquisitions (full chest, then abdomen or
through the chest down to the dome of the diaphragm, then delay and
then from diaphragm down to abdomen)
12. With larger detector row arrays providing extended z-axis coverage,
sequential scans may be performed, and lesions may would span - or
be at the interface of - several acquisitions FOVs
iii. Scanner Calibration
1. Phantom QA
2. Calibration to water
3. Calibration to air
4. HU consistency serially and across FOV
5. Contrast Scale
iv. Scanner maintenance schedule/parts replacement (e.g. parts like x-ray tubes,
and detectors may degrade or at least change performance over time, which
may affect image quality)
d. Image Analysis – image segmentation, registration and calculation
i. Segmentation vs. non-segmentation
ii. Non-segmentation method (such as registration method for change analysis
or template-method)
iii. Manual vs. semi-automated vs. fully automated analysis
iv. Region growing factors
1. Thresholding
2. Algorithm (e.g. criterion)
v. Calculating Volume
1. Indirect estimates/surrogates of volume
a. 1D measurement (RECIST)
b. 2D measurement (Product of Diameters, WHO)
2. Summing voxels
3. Other estimates of volumes (e.g. taking into account partial volume
effects or tumor models) that may be patient related
vi. Software version (again, depending on which features are implemented in
software being used and how they are being implemented).
e. Interactions between Source of Bias and Variance
The CT Volumetrics subcommittee recognizes that there will be some instances of
strong interaction between these sources of error. This means that analyses will
likely have to take into account covariances and not just consider univariate analyses.
A few examples include;
i. A segmentation method that uses a threshold value to determine lesion
boundary extent may adapt this value when different slice thicknesses are
used due to different degrees of partial volume effects at the tumor’s edge.
ii. Based on the sampling theory, there likely is a strong interaction between the
size of a lesion and the slice thickness used to reconstruct images when
volume is to be determined
f. Mitigation measures
These were discussed briefly by the CT Volumetrics committee and a few mitigation
measures were prospectively identified. These include:
i. When possible, keep all potential sources of variance the same, including:
1. The same device (same scanner)
2. Using the same technical parameter settings as previous exams for
any given patient (though some thought to adjusting parameter
settings due to change in patient size, such as weight gain or weight
loss, may be appropriate).
3. Use the same patient factors (positioning, breathing instructions, etc.)
4. Perform analysis with the same software
5. Perform analysis with the same software settings (threshold, etc.) as
previous exams.
6. Perform analysis with the same software version if updates were to
add another source of variance (i.e. change absolute measurements)
without improving the change analysis (e.g. when seeking relative
rather than absolute changes)
3. Data to be made available and how it contributes to each question
There will be several important sets of image data that will be made publicly available,
either through the RIDER project directly or through other data collection efforts that are
being performed outside of RIDER, but that share the same goals in answering questions
related to CT Volumetrics as a Biomarker for response. These are described below:
a. As part of the RIDER Project
i. The Patient Repeat CTs (“Coffee Break Experiment”) performed at
Memorial Sloan Kettering.
1. In this study, 32 lung cancer (NSCLC) patients were imaged twice on
the same scanner in an interval less than 15 minutes apart.
2. Imaging was done using identical technical parameter settings on the
same machine (GE LightSpeed 16 for 28 subjects, GE VCT(64 slice
scanner) for 4 subjects). LightSpeed 16 settings (VCT settings in
parentheses where different) were:
a. 120 kVp
b. detector configuration of 16 x 1.25 mm (64x0.625mm)
c. pitch of 1.375:1 (0.984)
d. rotation time of 0.5 second.
e. The standard-dose thoracic images were obtained without
intravenous contrast during a breath hold.
f. Thin-section images of 1.25 mm slice thickness were
reconstructed using the Lung convolution kernel.
3. Expected image-derived metric would be tumor volume, but this
could be expanded to other metrics.
4. Data will be made available to the public through NCIA by late
September.
ii. Unmarked Repeat CT lung studies at different time intervals from MD
Anderson
1. In this study, many cases of patients (not sure of the current or
ultimate number) with known nodules or masses in the lungs (both
primary and metastatic lesions were included) were submitted to
NCIA.
2. Each case had at least 2 image data sets from different time points;
many had 3 or more time points (information on time interval
between scans will be available) .
3. No truth about tumor volume was provided – no reader segmentation
or volume estimate.
4. For several cases, two readers at NCIA (C. Jaffe and R. E) made
RECIST measurements, but these are subjective ratings and would
not be considered as an “externally determined truth”.
5. Typically, scans were done with the same acquisition parameters
and/or on similar scanners, but there is no implicit guarantee thereof
as this was not one of the inclusion criteria (though technical
parameter information will be made available through DICOM
headers).
b. Outside RIDER
i. CT lung nodule phantom from FDA (Nick Petrick, Marios Gavrielides,
Lisa Kinnard).
1. In this study, several synthetic nodules of known size, composition
and shape were placed inside an anthropomorphic phantom and
attached to lung vasculature.
2. This phantom has been scanned multiple times under different
combinations of dose (low and high mAs), slice thickness (thin and
thick slices), pitch, and reconstruction filter (smooth and sharp
kernels). This includes, effectively, a “phantom coffee break”
experiment.
3. These datasets will be available to RIDER working group sites to
have them use their own segmentation and volume calculation
software to estimate volume of each lesion.
4. The phantom has also been scanned at Mallinckrodt Institute of
Radiology (Washington University, St. Louis) and similar imaging
has been performed on a Siemens scanner. This phantom is also being
scanned on a GE system at the National Institutes of Health. Thus,
the variability between scans obtained on different platforms can be
investigated.
5. Again, main focus here was on estimating the volume of simulated
nodules and analyzing the effects of different technical parameters,
especially as a function of nodule size and composition.
6. All of these scans will eventually be archived on the NCIA website
for general public use.
ii. Cornell Coffee Break
1. In this study, several patients were scanned multiple times during the
course of a clinically indicated needle biopsy study. These are
multiple scans of the same lesion (the one about to be biopsied).
2. More information at http://www.via.cornell.edu/crpf.html
iii. Biochange 2008 experiment
1. In this study, colleagues at NIST performed a nodule volume change
pilot study in which they collected several patient studies and some
studies from the FDA’s phantom with simulated nodules and had
multiple investigative teams perform measurements.
2. More information is available at:
http://www.itl.nist.gov/iad/894.05/biochange2008/Biochange2008-
webpage.htm
4. Planned Analyses of Bias and Variance
a. Planned Analyses that will be performed as part of RIDER include:
i. From patient repeatability under no change condition (“Coffee Break”)
experiment, the MSK group will perform an analysis of variance of the
tumor volume measurements made for each patient between the two scans
performed. See Appendix 1 for MSKCC resulted submitted for RIDER.
b. Planned Analyses that will be performed outside of RIDER include:
i. From FDA scans of anthropomorphic phantom:
1. an analysis of the effects of different acquisition parameters (such as
slice thickness, mAs level, beam collimation, reconstruction
algorithm, pitch and scanner manufacturer) and possibly image
analysis parameters on tumor volumes will be performed.
2. Their analysis will also include investigating the effects of lesion
related parameters (such as the differences between spherical and
non-spherical nodules as well isolated lesions vs. lesions contacting a
simulated vessel) on tumor volumes.
ii. Analyses similar to the MSK Coffee Break experiment are expected to be
performed on the Cornell Coffee Break experiment image data as well.
iii. The NIST Biochange 2008 experiment will be reporting the results of its
Pilot Study at SPIE 2009 in which it will investigate changes in tumor
volumes from repeat studies of patients and phantoms using different
observers/methods.
5. Roadmap for future data collection and analysis efforts
a. Other analyses/experiments to perform on RIDER datasets
i. While tumor volumetrics are the primary subject of these investigations, there
are several other metrics that the RIDER CT Volumetrics subcommittee
would suggest warrant future investigation. These include:
1. Other changes in tumor characteristics besides just volume, such as
changes in tumor density and composition. Some analysis may be
possible with RIDER datasets acquired for volumetric purposes (such
as Coffee Break experiment data).
b. It is recognized that with RIDER and related datasets, a unique opportunity exists
both to perform analyses and to provide guidance as to how future analyses should
be performed. This may include describing the perceived strengths and weaknesses
of various approaches (say, univariate analysis of variance to multivariance analysis
of variance, etc.) and describing in detail each analysis method how results may vary
depending on the kind of data under scrutiny, and also depending on the question
that needs to be answered.
c. The RIDER group may also consider describing barriers to either scenario:
i. short-term issues (such as barriers to sharing data due to IRB issues) and
ii. longer terms issues such as
1. workflow issues about how tumor volumetric issues would be
performed in a clinical setting
2. local cultural or logistical issues such as who would actually perform
those measurements
3. infrastructure issues such as how those results would be obtained,
communicated, stored and reported both for regulatory purposes and
for clinical patient management issues
4. economic issues such as:would trial sponsors be willing to bear the
cost of the increased effort required to perform tumor volumetric
measurements (and reporting, etc.)? would they be willing to do so
only if the efficacy of these methods were demonstrated
unequivocally?
d. Other Data sets that would be nice to have and the analyses that would be great to do
i. The FDA is considering expanding the range of simulated nodules to those
that may be more peripherally located and of different size and composition.
This would permit a stratified analysis by different kinds of tumors that are
seen clinically (albeit under ideal phantom conditions). For example, if
ground glass tumors were to be physically simulated, investigations could be
performed into the effects of technical parameters on estimating volume for
ground glass tumors. Alternately, a similar setup could also help in assessing
the effects of having different software packages to estimate the volume of
ground glass tumors
ii. Of course, the obvious extension of RIDER would be to collect image data
sets that have corresponding clinical outcomes, such as progression-free
survival (PFS) or some other “truth” measure. Ideally, these image datasets
would be from the same patient acquired over multiple time points and the
timing with regard to therapy received would be known along with the
clinical outcome. Other supplementary information could be the ground truth
on imaged lesions through biopsies, or through complete surgical samples
collected shortly after the last imaging session for this lesion.
Appendix 1: MSKCC Repeat CT Study – Summary on methods and results
Background:
The CT scan is the most widely used assessment of response to treatment for lung tumors.
However, little is known about the reproducibility of the CT scan measurements obtained.
The purpose of this study was to evaluate the variability of tumor unidimensional,
bidimensional and volumetric measurements on same-day repeat CT scans in patients with
non-small cell lung cancer (NSCLC).
Methods:
Thirty-two consecutive patients with measureable NSCLC, each underwent two CT scans of
the chest within 15 minutes using the same imaging protocol, were evaluated. We applied
our home-grown segmentation method to assisting in calculation of the two greatest
diameters and the volume of each lung lesion on both scans of thin-section images. An
experienced radiologist visually inspected all segmentation results and corrected a result if it
was suboptimal. Concordance correlation coefficient (CCC) and Bland-Altman plots were
used to assess the agreement between the measurements on the two repeat scans.
Results:
Lesions had a mean diameter of 3.8 +/- 2.0 cm. The computer method successfully
segmented 19 (59%) lesions on both scans. The remaining 13 (41%) lesions required
radiologist’s manual corrections. The CCCs and relatively narrow 95% limits of agreement
have demonstrated that CT scan measurements are highly reproducible (Table 1). Taking the
volume as an example, differences measured on the two repeat scans falling outside the
range of -12.1% and 13.4% could be considered a true change in tumor volume.
Concordance
correlation coefficient
 Mean
% relative
difference
95% Limits of
agreement
ρc 95% CI

Uni-dimensional 1.00 (1.00, 1.00) -0.6% -7.3 %, 6.2 %
Bi-dimensional 1.00 (0.99, 1.00) 1.1% -17.6 %, 19.8 %
Volume 1.00 (1.00, 1.00) 0.7% -12.1 %, 13.4 %
Table 1. Computer-generated measures of reproducibility on repeat CT scans
Page 1 of 7
RIDER Project
MR SUBCOMMITTEE SUMMARY REPORT
1. Types of MR imaging biomarkers that would be useful in providing change/response
assessment.
Of the large number of morphological and functional MR measures that can be obtained and
might be useful in assessment of response to therapy, the MR Subcommittee focused on the
following aspects:
• Phantom data obtained on four scanners from two vendors and at two field strengths to
address 1) repeatability (short-term and long term) of T1 measurements, 2) contrast-tonoise ratio and signal intensity stability, 3) limits of agreement of T1 measurements
using two acquisition techniques, and 4) between vendor contrast differences and
effect on computed contrast agent concentration results (M.D. Anderson Cancer
Center).
• Repeat (assumed zero pathological change) dynamic contrast enhanced MRI (DCEMRI) (Duke University).
• Repeat (assumed zero pathological change) diffusion MRI, including diffusionweighted imaging and diffusion tensor imaging (University of Michigan and Duke
University).
These three foci of effort provided deidentified source data, transferred to the NCIA, to be
used by other researchers interested in change analysis of DCE-MRI and diffusion imaging
biomarkers, as well as data analyses and derived parametric maps, also transferred to NCIA.
The individual contract reports address the specific data analyses performed on each of the
data sets outlined above. This summary document only briefly reviews the general goals of
the subcommittee’s efforts, data acquisition and submission, perceived limitations of the
current data and analyses, difficulties encountered in the efforts to share such clinical research
data, and suggestions for future studies.
2. Specific goals of the MR subcommittee efforts.
The initial studies described herein were undertaken to begin the process of addressing the
following general goals identified by the full RIDER committee:
• Accuracy, Bias and Reproducibility Using a Test Object (Phantom) – How closely can
one match known value (contrast response, relaxation times, etc.) using a specific
image acquisition technique and image analysis methodology? If the same phantom is
scanned on the same device, how much variance is observed? If the phantom is
scanned on a different device (same vendor as well as different vendor) how much
variance is observed? How closely can acquisitions on one vendor’s scanner be
matched on a scanner from a different vendor?
• Patient Repeatability – If repeated scans are obtained on a given patient, how much
variation is observed when the same acquisition, image analysis, and image-derived
measurement calculation are used?
• Patient Change – When a patient is imaged over time, how much change can reliably
be observed, and is such change observed predictive of clinical outcome? (This
ultimate goal of the RIDER efforts is not addressable within the limited RIDER
initiative lifetime.)
Page 2 of 7
3. Identification of sources of bias and variance
Although MR provides a potential wealth of functional and morphological information, there
are many sources of variance when using MR techniques to measure imaging biomarker
response. These have been broken into three major groups, with the understanding that there
may be significant interactions between sources of variance (e.g. covariances) between
effects. These include:
a. Patient Factors:
i. Patient motion
1. voluntary (e.g., not holding still during scanning, swallowing)
2. involuntary (e.g., respiratory and cardiac motion, coughing,
swallowing, spasm)
3. cyclic (e.g., “gatable”) vs. non-cyclic
ii. Physiological variation independent of disease
1. variation in cardiac output
2. modulation of blood volume and flow due to, for example, prandial
status and caffeine consumption.
iii. Lesion size and location
1. lesion size vs. spatial resolution provided by scanner hardware and
acquisition protocol (partial volume)
2. lesion location proximal to sources of signal variation (e.g., mediastinal
lesions in DCE-MRI applications)
iv. Patient positioning / repositioning / immobilization
v. Implants (e.g., metallic implants, pacemakers, etc.)
vi. Effects due to therapies other than the target therapy
1. Prior therapies
2. Concomitant therapies (e.g., steroids, antivascular/antiangiogenic
therapies, etc.)
b. Acquisition-Dependent Factors:
i. Differences due to imaging hardware design and performance characteristics
1. differences in gradient subsystem performance and image acquisition
rates
2. differences in B0 and B1 homogeneity
3. phased array coil sensitivity characteristics
ii. Differences in imaging protocols (extensive list!)
1. differences in pulse sequence implementation, across vendors and
within vendors, which can strongly affect contrast response
2. differences in acquisition parameter selections available for a given
pulse sequence across vendors and within vendors
3. corrections for image intensity non-uniformity
4. parallel imaging implementation and acceleration factors
5. the “upgrade dilemma” – software upgrades that can, in unanticipated
and unadvertised ways, modify pulse sequences and affect contrast
response, temporal resolution, etc.
Page 3 of 7
iii. Scanner characteristics and calibration
1. magnetic field homogeneity
2. RF subsystem calibration and stability
3. gradient subsystem calibration and stability
4. spatial accuracy (over specific volume or volumes) - affected by
gradient nonlinearity corrections (in-plane and, for some scanners and
pulse sequences, through-plane)
5. contrast response
6. signal stability (signal intensity, SNR, CNR, and ghosting)
7. B1 homogeneity and receiver coil sensitivity characteristics
c. Image analysis factors
i. Strongly application-dependent.
1. Correction for motion, if required, including deformable registration.
2. Correction for image intensity non-uniformity (B1 transmit / RF coil
sensitivity characteristics)
3. DCE-MRI requirements:
a. vascular input function assessment
b. native tissue T1 measurements required if contrast agent
concentration assessment required
c. temporal sampling characteristics
ii. General
1. repeatability of analysis technique
a. interobserver
b. intraobserver
c. (semi-)automatic
2. sensitivity to CNR and other input data characteristics
Interactions between sources of variance
As noted by the other RIDER subcommittees, there are known (and, undoubtedly, as yet
unknown) interactions between some of these sources of variance and these will need to
be addressed in an application specific manner.
Basic steps for mitigation of patient-dependent variance
While such techniques were not discussed specifically by the MR Subcommittee, common
concepts for mitigation include:
• Use the same device (same scanner) for each patient, when possible, and have
rigorous QC processes in place to assure consistent performance for a given
scanner and, if applicable, across scanners.
• Use stored, and “user locked”, if possible, protocols on scanners to make sure the
same technical parameter settings are used.
• Use consistent patient positioning and preparation procedures (IV placement,
breathing instructions and/or feedback, etc.).
• Use consistent contrast agent doses and rates of delivery (and consistent use of
saline flush) when techniques call for intravascular contrast agents.
Page 4 of 7
• Carefully monitor the scanners for any software and/or hardware upgrades. If
either occurs, completely reassess baseline scanner performance characteristics.
• Perform all data analyses with the same equipment and software release versions,
if possible. If upgrades or changes occur, verify consistency of results from one or
more test cases with those obtained using prior versions of the hardware and/or
software.
4. Data made available by MR Subcommittee members and how they contribute to each
question above
Several sets of image data will be made publicly available the RIDER project and can be used
to partially address one or more of the three major categories of variance identified above.
These are described below:
a. Phantom data to assess system bias and variance (E. Jackson, M.D. Anderson Cancer
Center).
i. In this study, data were obtained using an 18-compartment EuroSpin TO5
contrast response phantom (Diagnostic SONAR, Ltd, Livingston, West
Lothian, Scotland). Data were obtained in “coffee break” fashion on the same
day as well as one week later. In the coffee break measurement setting, the RF
coil and phantom were positioned and data obtained, the phantom was then
removed and positioned and scanned on a second nearby scanner. The
phantom was subsequently returned to the first scanner, the RF coil and
phantom repositioned, and data obtained using a new exam ID but using the
same stored data acquisition protocol. Data were obtained at all three time
points from three 1.5T scanners (two, with differing gradient subsystems, from
a single vendor and one from a second vendor) and from one 3.0T scanner.
Data acquired included T1 measurements using a 2D inversion recovery (IR)
sequence (once at 1.5T and twice at 3.0T) and a 3D multiple flip angle fast
spoiled gradient recalled echo (MF-FSPGR) sequence (each time on each
scanner). An approximately 7 min DCE-MRI data set was also acquired each
time on each scanner using a 3D FSPGR sequences with sections matched
those acquired for the MF-FPSGR T1 measurements..
For the fast spin echo IR T1 measurements, 10 inversion times were used (50,
100, 250, 500, 750, 1000, 1500, 2000, 2500, and 3000 ms). For the MFFSPGR T1 measurements, 7 flip angles were used (2, 5, 10, 15, 20, 25, 30
degrees). For the DCE-MRI acquisitions, the total scan time was
approximately 7 min with a temporal resolution of approximately 9 sec
(dependent on specific scanner). Echo and repetition times were matched as
closely as possible across scanners, with TE ranging from 0.90 to 1.35 ms and
TR ranging from 4.09 to 5.10 ms. The flip angle was 30 degrees for all DCEMRI scans.
ii. All source data will be transferred to NCIA by 9/25/2008.
b. Repeated DCE-MRI and Diffusion Data in Brain Tumor Patients (D. Barboriak, Duke
University Medical Center)
i. In this study, repeat DCE-MRI datasets in 19 patients with recurrent
glioblastoma multiforme and repeat diffusion tensor imaging (DTI) datasets in
17 of the 19 patients were performed. The interval between scans was 2 days
Page 5 of 7
or less. The technical parameters are summarized in the Appendix. All
images were obtained on the same 1.5T imager (Siemens Avanto). In brief,
for DCE-MRI, T1 mapping using a multi-flip angle approach and 6 flip angles
was performed. Dynamic imaging was performed every 4.8 seconds during
the intravenous infusion of 0.1mmol/kg of Gd-DTPA at 3ccs/second. DTI
imaging was performed using a 12 direction sequence, TR=6000ms, TE=100
ms, 90 degree flip angle, 4 signal averages, 128 x 128 acquisition matrix, 1.72
x 1.72 x 5 mm voxel size, and a b-value of 1000 sec/mm2
.
ii. All source data have been transferred to NCIA. In addition, parameter maps
for fractional plasma volume, volume of the extracellular extravascular space,
and Ktrans (for DCE-MRI data) and apparent diffusion coefficient and fractional
anisotropy (for DTI data) have been transferred to NCIA. Segmentations of
tumor-related enhancement and FLAIR signal abnormality volumes have also
been transferred.
c. Repeated ADC Diffusion of Breast Cancer (C. Meyer, University of Michigan)
i. In this study, during the first cycle of neoadjuvant chemotherapy the following
imaging protocol was implemented:
• Two pre-initiation (pre-therapy) baseline MRI scans were typically
obtained within 15 minutes, where initiation of neoadjuvant
chemotherapy was no later than one week from the last baseline MRI,
and
• One post-initiation MRI was obtained within 8-11 days after initiation
of neoadjuvant chemotherapy
ii. The second cycle of neoadjuvant chemotherapy was implemented with no
further imaging studies.
iii. Axial DWI parameters on our Philips 3T scanner: FOV=350mm; acquisition
matrix = 196x86; SENSE factor 2; 30 slices; 4mm thick; 2D SE-EPI 3-axis
DWI at b=0 and 800s/mm2
; STIR (fat suppression), TR/TE/TI = 12000 / 62 /
150 ms; 2 signal average; acquisition time = 4:41 min.
iv. All source data will be transferred to NCIA by 9/25/2008.

5. Analyses of bias and variance
a. Phantom data collection (M.D. Anderson Cancer Center):
Limits of agreement and coefficients of repeatability, as proposed by Bland and
Altman (Lancet, 1986), were computed for the T1 measurements obtained from each
scanner using the IR-based and multiple flip angle-based data (limits of agreement)
or multiple flip angle measurements at each time point (coefficients of repeatability).
T1 measurement correlation analysis was also performed using the data obtained at
each time point on each scanner. Contrast response was assessed at each time point
on each scanner using the multiple compartment phantom data, and stability of the
CNR and signal intensity was assessed from each of the DCE scans obtained at each
time point on each scanner. Simulated DCE uptake curves were also generated for
each scanner using measured data from the multiple compartment phantom and
commonly assumed signal intensity response for an ideal fast spoiled gradient echo
sequence.
Page 6 of 7
b. Repeated DCE-MRI and Diffusion Data in Brain Tumor Patients (Duke University
Medical Center):
A similar analysis of repeatability was performed on both the DCE-MRI and DTI
datasets from brain tumor patients. For DCE-MRI parameters, mean values were
obtained at both time points in areas of tumor-related enhanced segmented from 3D
volumetric isotropic T1-weighted contrast-enhanced FLASH images. The
coefficient of repeatability (Bland and Altman) and the 95% confidence interval for
percent change in parameter (Roberts, Roberts C, Issa B, Stone A, Jackson A,
Waterton JC, Parker GJ Comparative study into the robustness of compartmental
modeling and model-free analysis in DCE-MRI studies. J Magn Reson Imaging.
2006;23(4):554-63). For DTI parameters, mean values were obtained at both time
points in both the areas of tumor-related enhanced, as well as in areas FLAIR signal
abnormality segmented from 3D volumetric isotropic contrast-enhanced FLAIR
images.
c. Repeated ADC Diffusion of Breast Cancer (University of Michigan)
Registration of the tumor in the interval exams was implemented using MIAMI
Fuse©. Tumor volumes of interest (VOI) were drawn on the anatomical image
volume and were warped from the anatomical volume onto one of the pre-therapy
diffusion volumes denoted as the reference; warping is necessary due to the
susceptibility artifacts in the diffusion acquisitions not present in the anatomical
volumes. Subsequent registrations, either between the two pre-therapy exams or the
two pre- and post-therapy scans, are also warped to account for repositioning
deformations to the breast as well as any small compartmental changes to the tumor.
Warping is accomplished using thin plate splines where the degrees of freedom
(DOF) of the warp are related to the volume of the tumor. The user only needs to
pick the location of three control points in the homologous tumor volume that
approximate their loci in the reference tumor volume. The multiscale registration
first implements rigid body registration, then low DOF warping, and finally full
DOF warping.
Apparent diffusion coefficient (ADC) volumes are computed from the interleaved
b=0 s/mm2
 and b=800 s/mm2
 acquisitions. For each pair of registered ADC images,
a 128x128 joint density histogram (JDH) is constructed by incrementing the count of
the 2D histogram defined by the two ADC values of the registered tumors. For the
JDH of the two pre-therapy exams, bias is removed from this realization and
variance is generalized, i.e., increased, by adding the transpose of its JDH to itself.
Then linear regression is performed after rotating the JDH onto its principal
component axes. The resulting linear estimate (green), the estimates of its 95%
confidence limits (red) and the 95% confidence limits of the histogram (yellow) are
computed and plotted on the modified joint density histogram. The means of both
joint density histograms are computed and plotted. Note that pre-therapy/pretherapy JDF represents estimates of sources of measurement covariance variance
associated with the null hypothesis, i.e., the presence of all sources of noise, but no
change in the tumor. 6. Recommendations for future data collection and analysis efforts and guidance to users
of MR-based imaging biomarkers
Page 7 of 7
a. Once the MR data analyses are fully available on NCIA, the MR Subcommittee and
parent RIDER Committee should carefully evaluate the complete data set and describe
perceived strengths and weaknesses of various approaches utilized. Once this process
is complete, the RIDER Committee should be in a unique position, one based on
multiple sources of commonly shared data, to make recommendations to the imaging
biomarker community at large regarding source of variance and bias as well as means
of mitigating such sources.
b. The RIDER Committee should also, in communication of its findings and
recommendations for future efforts, consider describing barriers to the implementation
of quantitative imaging biomarkers, including:
i. barriers to sharing of data, such as those due to IRB compliance and review
and sponsor-specific concerns of the proprietary nature of clinical trial data
ii. workflow-related issues, including 1) the volume of data to be stored and
analyzed for single or multiple time points, 2) the time-commitment
requirement for not just the data analysis, but also the associated QC required
to assure optimal data collection and analysis, 3) the funding required for such
data acquisition and analyses and sources of such funding (or lack thereof),
and 4) Structured Reporting and further extensions of current DICOM
standards to allow for the appropriate data storage and reconciliation, the
communication of such quantitative imaging biomarker findings in a
consistent manner, and to allow further data mining and meta analyses.
c. Patient outcome studies should be strongly encouraged in any RIDER Committee
report. The data obtained during the very limited lifetime of the RIDER initiative is
clearly only the initial phase of a process that begins with a cultural change in the very
nature of the data acquired (which currently are obtained using techniques optimized
for qualitative image quality, not quantitative image data integrity) and progresses
through the identification and minimization of sources of bias and variance at the
device, patient preparation and scanning, and data analysis levels to come to the stage
where imaging biomarkers can be utilized as true surrogate markers of therapy
response. The ultimate proof of the validity and utility of such non-invasive imaging
biomarkers, of course, lies in the ability of such measures to predict patient outcome
earlier than currently possible. A key future effort must focus on such outcome
measure assessments as well as continued efforts to identify and quantify sources of
bias and variance in order to be able to properly determine sample size requirements
for single subject response and group response assessments to novel therapies.
RIDER MR Subcommittee members:
Daniel Barboriak, MD – Duke University Medical Center
Luc Bidaut – M.D. Anderson Cancer Center
Edward Jackson, Chair – M.D. Anderson Cancer Center
Charles Meyer – University of Michigan
Submitted to the RIDER Committee and Laurence Clarke on 9/24/2008.
Duke DCE‐MRI / DTI – Appendix – Page 1 of 4
Appendix 2
Guide to Duke submissions to NCIA
Imaging data on 19 patients with recurrent glioblastoma who underwent repeat imaging sets was
submitted to NCIA.  These images were obtained approximately 2 days apart (with the exception of patient
786, whose images were obtained one day apart).   
DICOM images
DCE‐MRI:   
All 19 patients had repeat dynamic contrast‐enhanced MRI (DCE‐MRI) datasets on the same 1.5T imaging
magnet.  On the basis of T2‐weighted images, technologists chose 16 image locations using 5mm thick
contiguous slices for the imaging.  For T1 mapping, multi‐flip 3D FLASH images were obtained using flip
angles of 5, 10, 15, 20, 25 and 30 degrees, TR of 4.43 ms, TE of 2.1 ms, 2 signal averages.  Dynamic images
were obtained during the intravenous injection of 0.1mmol/kg of Magnevist intravenous at 3ccs/second,
started 24 seconds after the scan had begun.  The dynamic images were acquired using a 3D FLASH
technique, using a flip angle of 25 degrees, TR of 3.8 ms, TE of 1.8 ms using a 1 x1 x 5mm voxel size.  The 16
slice imaging set was obtained every 4.8 sec.
DTI:
Seventeen of the 19 patients also obtained repeat diffusion tensor imaging (DTI) sets.  Whole brain DTI
were obtained using TR 6000ms, TE 100 ms, 90 degree flip angle, 4 signal averages, matrix 128 x 128,  1.72
x 1.72 x 5 mm voxel size, 12 tensor directions, iPAT 2, b value of 1000 sec/mm2
.
Contrast‐enhanced 3D FLASH:
All 19 patients underwent whole brain 3D FLASH imaging in the sagittal plane after the administration of
Magnevist.  For this sequence, TR was 8.6 ms, TE 4.1 ms, 20 degree flip angle, 1 signal average, matrix 256 x
256; 1mm isotropic  voxel  size.
Contrast‐enhanced 3D FLAIR:
All 17 patients who had repeat DTI sets also had 3D FLAIR sequences in the sagittal plane after the
administration of Magnevist.  For this sequence, the TR was 6000 ms, TE 353 ms, and TI 2200ms; 180
degree flip angle, 1 signal average, matrix 256 x 216; 1 mm isotropic voxel size.
Note: before transmission to NCIA, all image sets with 1mm isotropic voxel size were “defaced” using
MIPAV software or manually.
Non‐DICOM images
A set of images which are not in DICOM format were submitted to provide an example or instance of an
analysis of the submitted imaging datasets.  The steps used to obtain the imaging and guide to the sample
Duke DCE‐MRI / DTI – Appendix – Page 2 of 4
images are given below.  All image analysis steps, unless specified, were performed using in house software
created using as plugins to the ImageJ platform.
DCE‐MRI analysis:
The first step in DCE‐MRI analysis was to obtain maps of T1 and equilibrium magnetization from the multi‐
flip 3D FLASH images.  This was accomplished using a non‐linear simplex fitting technique of the data to the
standard signal intensity equation.
The second step was using motion correction algorithms to minimize the effects of patient motion during
the dynamic acquisition.  In brief, an algorithm based on measurements of correlation ratios were applied
to groups of 5 time points that were averaged and edge filtered to determine to which image sets motion
correction would be applied.  Sets that were motion corrected were subjected to two algorithms.  First,
image registration with itk (www.itk.org) using MultiResMIReg (a multi‐resolution mutual information
algorithm) were performed.  These registrations were kept if the correlation ratios measured as described
above were improved by the technique.  If there was no improvement, a Lucas‐Kanade image stabilizing
algorithm was performed.  Again, the registrations were kept if the correlation ratios improved.
The third step was to use the T1 and equilibrium magnetization images to convert signal intensities to
estimates of gadolinium concentrations.  For this purpose, the standard equation as formulated in Li, et al.1
was used.  This resulted in contrast agent concentration‐time curves generated for each pixel location.
The fourth step was to derive a vascular region‐of‐interest from the imaging data.  Candidate vascular
voxels were selected using an automated technique from the signal intensity data on the basis of timing
and heights of peaks.   An experienced neuroradiologist then selected subsets of these voxels on a single
slice on the basis of resulting height and plateau of the concentration agent‐time curves.
The fifth step was to derive parameter maps using the extended Tofts model from the data above.  To
derive Ktrans (the volumetric transfer constant for contrast agent transit from the plasma space to the
extracellular extravascular space),  fPV (the fractional plasma volume, or percent of each voxel thought to
represent plasma space) and the ve (the percent of each voxel thought to represent extracellular
extravascular space), the matrix‐based technique described by Murase2 was used.   
Of note, these maps were submitted to NCIA.  They are under the non‐DICOM datasets directory
designated by patient number.  The maps are then designated Ktrans‐mmddyyyy.tif , FPV‐
mmddyyyy and Ve‐mmddyyy, respectively, where mmddyyyy is the anonymized date.  These
images are 32‐bit TIFF stacks of 16 images.
The sixth step was to determine the location of tumor that was repetitively imaged.  The contrast‐enhanced
3D FLASH images obtained at the first imaging session were filtered using anisotropic diffusion filtering,
then candidate pixels were determined by thresholding all pixels whose signal intensity was more than two
standard deviations above the mean obtain in a large VOI obtained in normal appearing areas in the corpus
callosum.  An experienced neuroradiologist then selected those pixels in this pixel set that appeared to be
related to tumor contrast enhancement rather than non‐tumor objects such as fat, vessels, dura or choroid
plexus.
Duke DCE‐MRI / DTI – Appendix – Page 3 of 4
Of note, these segmentations of tumor‐related enhancement (TRE) were submitted to NCIA.  They
are under the non‐DICOM datasets directory designated by patient number.  The segmentations
are then designated TRE_segmentation‐mmddyyyy.tif , , where mmddyyyy is the anonymized date.  
These images are 8‐bit TIFF stacks.  These images were originally processed in the axial plane, but
have been reformatted into the sagittal plane in order to correspond exactly with the sagittal
source DICOM images.
The seventh step was to place the parameter maps into a single parameter space by registering a
summation of the dynamic image stack to the isotropic whole brain 3D FLASH images obtained at the same
time point.  By registering the 3D FLASH obtained at the second imaging session to that obtained in the
first, both sets of parameter maps could be registered to the isotropic whole brain 3D FLASH images
obtained at the first imaging session.  MultiResMIReg was used to perform these registrations.
The eighth step was to derive mean parameters in areas of tumor.  The first and last two images in the
parameter maps were discarded due to wrap‐around artifact.  The resultant parameter maps registered to
the isotropic whole brain 3D FLASH images obtained at the first imaging session were interpolated to 1 mm
isotropic images using a tri‐linear technique.  Mean parameter values were obtained for points that were
segmented as tumor‐related enhancement in the sixth step and were imaged in the middle 12 slices on
both imaging occasions.   Main repeatability indices derived were the repeatability coefficient and the 95%
confidence interval for percent change3
.
DTI analysis:
The first step in the DTI analysis was to create pixel‐by‐pixel maps of apparent diffusion coefficient (ADC)
and fractional anisotropy (FA) from the DTI image sets.  These images were obtained using JDTI, a Java‐
based plugin for ImageJ based on the JAMA matrix package (this software is available for free download
from http://dblab.duhs.duke.edu).  Of note, the fractional anisotropy equation used in this software
package is:
2
3
2
2
2
1
2
3
2
2
2
1 ( ) ( ) ( )
λ λ λ
λ λ λ
+ +
− + − + − = MD MD MD
FA
Where MD is the mean diffusivity and λ1, λ2, and λ3 are the three eigenvalues from the diffusion tensor.  
(ADC is set equal to the mean diffusivity).  The values obtained by this plugin may need to be multiplied by
2
3
in order to match other formulations of FA in the literature.
Of note, these maps were submitted to NCIA.  They are under the non‐DICOM datasets directory
designated by patient number.  The maps are then designated ADC‐mmddyyyy.tif and FA‐
mmddyyy, respectively, where mmddyyyy is the anonymized date.  These images are 32‐bit TIFF
stacks.
The second step was to create segmentations of TRE and FLAIR signal abnormality (FSA) from the
corresponding isotropic 3D image sets.  The TRE segmentations obtained in step 6 of the DCE‐MRI analysis
Duke DCE‐MRI / DTI – Appendix – Page 4 of 4
above were used for DTI analysis.  To obtain FSA segmentations, the contrast‐enhanced 3D FLAIR images
obtained at the first imaging session were filtered using anisotropic diffusion filtering, then candidate pixels
were determined by thresholding all pixels whose signal intensity was more than two standard deviations
above the mean obtain in a VOI obtained in normal appearing areas in the caudate head contralateral to
the bulk of tumor.  An experienced neuroradiologist then selected those contiguous pixels in this pixel set
that appeared to be involved with or surrounding the tumor‐related contrast enhancement.
Of note, these segmentations of FLAIR signal abnormality (FSA) were submitted to NCIA.  They are
under the non‐DICOM datasets directory designated by patient number.  The segmentations are
then designated FSA_segmentation‐mmddyyyy.tif , , where mmddyyyy is the anonymized date.  
These images are 8‐bit TIFF stacks.  These images were originally processed in the axial plane, but
have been reformatted into the sagittal plane in order to correspond exactly with the sagittal
source DICOM images.
The third step was to place the parameter maps into a single parameter space by registering a summation
of the dynamic image stack to the B0 images to the isotropic whole brain 3D FLASH images obtained at the
same time point.  By registering the 3D FLASH obtained at the second imaging session to that obtained in
the first, both sets of parameter maps could be registered to the isotropic whole brain 3D FLASH images
obtained at the first imaging session.  Similarly, registrations of the 3D FLAIR sequences to the isotropic
whole brain 3D FLASH images obtained at the first time point allowed these images and their corresponding
segmentations to be placed in the same anatomical frame of reference.  MultiResMIReg was used to
perform these registrations.
The fourth step was to derive mean parameters in areas of tumor.  The parameter maps registered to the
isotropic whole brain 3D FLASH images obtained at the first imaging session were interpolated to 1 mm
isotropic images using a tri‐linear technique.  Mean parameter values were obtained for points that were
segmented as TRE and FSA on the first study.  Main repeatability indices derived were the repeatability
coefficient and the 95% confidence interval for percent change3
.
References
1. Li KL, Zhu XP, Waterton J, Jackson A. Improved 3D quantitative mapping of blood volume and
endothelial permeability in brain tumors. J Magn Reson Imaging 2000;12(2):347‐357.
2. Murase K. Efficient method for calculating kinetic parameters using T1‐weighted dynamic contrast‐
enhanced magnetic resonance imaging. Magn Reson Med 2004;51(4):858‐862.
3. Galbraith SM, Lodge MA, Taylor NJ, et al. Reproducibility of dynamic contrast‐enhanced MRI in
human muscle and tumours: comparison of quantitative and semi‐quantitative analysis. NMR
Biomed 2002;15(2):132‐142.
Summary of RIDER PET/CT Subcommittee Results and Recommendations
September 21, 2008
PET/CT RIDER subcommittee members:
Luc Bidaut (MD Anderson Cancer Center)
Larry Clarke (NCI)
Barbara Croft (NCI)
Robert Doot (U. Washington)
Paul Kinahan (chair) (U. Washington)
Geoffrey McLennan (U. Iowa)
Charles Meyer (U. Michigan)
Edwin van Beek (U. Iowa)
Brian Zimmerman (NIST)
The following reports the results and recommendations of the RIDER PET/CT
subcommittee. The PET/CT subgroup was responsible for: (1) archiving de-identified
DICOM serial PET/CT phantom and lung cancer patient data in the National Cancer
Imaging Archive (NCIA) to provide a resource for the testing and development of
algorithms and imaging tools used for assessing response to therapy, (2) conducting
multiple serial imaging studies of a long half-life phantom to assess systemic variance in
serial PET/CT scans that is unrelated to response, and (3) identifying and
recommending methods for quantifying sources of variance in PET/CT imaging with the
goal of defining the change in PET measurements that may be unrelated to response to
therapy, thus defining the absolute minimum effect size that should be used in the
design of clinical trials using PET measurements as end points.
Figure 1: Serial PET/CT images of an lung cancer patient before and after therapy. Black and
white arrow heads respectively indicate the tumor in the PET and CT images.
1. Archived PET/CT Images
De-identified DICOM images of serial scans of 28 lung cancer patients (with a total of 65
scans) of patients were uploaded to the National Cancer Imaging Archive (NCIA) using
the RSNA’s Medical Imaging Resource Center (MIRC) open source clinical trials
software. DICOM images from repeat studies of a long half-life 68Ge calibration
phantom were also uploaded. These image sets can now be downloaded by users for
the testing and development of algorithms and imaging tools for assessing response to
therapy. Sample PET/CT images of a lung cancer patient and the phantom included in
the NCI archive are in Figures 1 and 2 and derived results are discussed in the next
section.
2. Serial imaging studies of patients and a phantom filled with long half-life 68Geepoxy compound
De-identified DICOM files from serial PET and PET/CT imaging of 10 lung cancer
patients were retrieved from the National Cancer Imaging Archive and analyzed to verify
the ability to process the archived files. Sample results from an analysis of the retrieved
files using GE PET VCAR software are shown in Figure 3.

Figure 2: NEMA NU-2 IQ phantom filled with solid, 9-month half-life 68Ge-epoxy compound. Air
gaps (white regions) in the main chamber and stems, and to some extent in the spheres
themselves, are evident on the CT images from PET/CT scans at the University of Washington.
Reproducibility studies of the phantom shown in Figure 2 assessed the instrumentation
variability of measurements in PET/CT scans by repeatedly measuring the activity
concentration of phantom hot spheres with fixed spatial locations and a true
target/background ratio of 4:1. Phantom spheres and background volumes were filled
with long half-life 68Ge-germanium epoxy to allow repeated measurements.
Several sets of 20 scans were acquired serially and reconstructed using various PET/CT
scanners (e.g., GE’s Discovery STE - DSTE, Siemens’ Biograph Hi-REZ and Philips’
Gemini TF) to determine the variability in stationary and ‘coffee-break’ repeat scans of
the same 9-month half-life calibration phantom. Standard deviations for maximum and
mean values of absolute recovery coefficients (equal to measured activity divided by true
activity) ranged from 0.9% to 4.3% for repeat GE DSTE scans of a stationary phantom,
depending on acquisition and reconstruction methods, after averaging the standard
deviations for all 6 sphere diameters as shown in Table 1. Sample results in Figure 4
show similar findings for ‘coffee-break’ imaging studies using one PET/CT from each of
the 3 major vendors. The GE, Philips, and Siemens PET/CTs were respectively located
at the University of Washington, at the University of Pennsylvania, and at the Huntsman
Cancer Institute at the University of Utah.
Figure 3: Maximum SUV from analysis of retrieved DICOM files using commercial analysis
software versus maximum SUV from clinical reports from serial PET scans of 10 lung cancer
patients archived in the NCIA.
The standard deviations of the absolute recovery coefficients for the coffee-break sets in
Figure 4 are shown in Table 2. The standard deviations from the first set of GE DSTE
coffee-break scans were significantly lower than the determined standard deviations
from the Siemens Biograph Hi-REZ and Philips Gemini TF PET/CTs with the exception
of the standard deviation of maximum absolute recovery coefficient for the Philips
Gemini TF PET/CT whose maximum value of 4.3% exhibited only a trend towards
statistical significance (p = 0.06). After adjusting the maximum and mean absolute
recovery coefficients to compensate for differences in total counts, the adjusted absolute
recovery coefficients for the subsequent coffee-break experiments were no longer
significantly different from the DSTE PET/CT set of standard deviations with the
exception of the adjusted standard deviation of the mean absolute recovery coefficient
for the Gemini TF PET/CT whose average value of 1.8% was significantly lower that the
corresponding DSTE PET/CT value of 2.4 (p = 0.04) as found in Table 2.
Table 1: Standard deviations of recovery coefficients for 2D & 3D reconstructions. Average
standard deviations (SD) of absolute maximum and mean recovery coefficients (RC) for 5 minute
GE DSTE PET/CT scans acquired in 2D and 3D modes. The 3D-FBP method was 3DReProjection (3DRP) (n = 20).
Algorithm: 2D-FBP* 2D-OSEM* 3D-FBP* 3D-OSEM*
SD of Max RC for 7-mm Resolution 4.3% 3.8% 2.4% 2.4%
SD of Max RC for 10-mm Resolution 3.2% 2.7% 1.7% 1.6%
SD of Max RC for 13-mm Resolution 2.3% 2.0% 1.2% 1.1%
SD of Mean RC for 7-mm Resolution 2.8% 2.5% 1.4% 1.4%
SD of Mean RC for 10-mm Resolution 2.3% 1.9% 1.1% 1.1%
SD of Mean RC for 13-mm Resolution 1.8% 1.6% 0.9% 0.9%
*Standard deviation (SD) value is mean of SDs of recovery coefficients (RC) for all six spheres.

Figure 4: Recovery coefficients vs. size for PET/CT scans of repositioned phantom. Maximum (A)
and mean (B) absolute recovery coefficients - with standard deviation error bars - versus sphere
diameter for the same phantom repositioned and imaged 20 times using PET/CTs from the 3
major vendors and reconstructed via 3D iterative algorithms with 7-mm post-reconstruction
Gaussian smoothing for the GE and Siemens scanners, and with no post-reconstruction
smoothing for the Philips scanner with standard deviation error bars.
Table 2: Reproducibility of PET/CT ‘coffee-break’ scans of repositioned phantom. Reproducibility
of 20 PET/CT scans of repositioned phantom with 3D iterative reconstruction and 7-mm Gaussian
smoothing for GE DSTE and Siemens Biograph Hi-REZ, and a “sharp” Philips filter for the Gemini
TF PET/CTs (n = 20).
 SD of
Max RC*
(unitless)
SD of
Mean
RC*
(unitless)
PET Slice
Thickness
(mm)
Time
since
DSTE
Scan (d)
Scan
Time
(min)
GE DSTE-1 (1/28/07)† 2.9% 2.4% 3.3 0 5
Siemens Biograph Hi-REZ (10/13/07)† 4.8%‡ 3.9%‡ 2.0 258 6
Philips Gemini TF (12/20/07)† 4.3% 3.2%‡ 4.1 326 3
Adjusted Siemens Biograph Hi-REZ 2.9%§ 2.4%§ 3.3§ 0§ 5§
Adjusted Philips Gemini TF 2.4%§ 1.8%‡§ 3.3§ 0§ 5§
* Standard deviation (SD) value is mean of SDs of recovery coefficients (RC) for all six spheres.
† Date the same phantom filled with solid 68Ge-germanium epoxy was scanned.
‡ These SDs were significantly different from corresponding DSTE-1 PET/CT SDs (p ≤ 0.04).
§ SDs adjusted via Equation 4 to compensate for differences in total counts.
3. Recommended methods to quantify variance in PET/CT imaging
Response of disease to treatment can be quantified using serial PET scans to measure
changes in concentrations of tracer uptake throughout diseased tissue [1]. FDG-PET
estimates of therapy response have been reported to predict outcome in non-small cell
lung cancer [2, 3], esophageal squamous cell carcinoma [4], ovarian cancer [5],
metastatic breast cancer [6], locally advanced adenocarcinoma [7], and neoadjuvant
locally advanced breast cancer [8]. Changes are due to response to therapy, natural (i.e.
biologic) variability including lesion size, variability in patient preparation [1], and
systemic variability measurement from differences in hardware, reconstruction, ROI
analysis, and subject positioning or movement (i.e. respiration).
A growing number of clinical trials are considering using quantitative FDG-PET
measurements as endpoints [1, 9] or to guide treatment [7], which increases the
importance of understanding the bias and variability intrinsic to serial PET scans and
their impact on multi-center clinical trials. The reproducibility of PET quantification must
be assessed to enable determination of a threshold change required for classifying
patient response to therapy and to aid estimation of variance for clinical trial design.
While reproducibility of patient imaging [10-13] and relative merits of different
reconstruction methods [12, 14-16] and ROI analysis techniques [12, 15, 17] have been
published, the impact of PET/CT hardware and subject positioning on the reproducibility
of tracer uptake quantification have not been as well studied.
3.1. What measures would be useful in measuring meaningful change/response?
Currently most clinically relevant: Relative change in SUV [standardized uptake value =
(measured ROI value) / ((injected amount)/(volume of distribution)) ], i.e. % change in
SUV - typically maximum SUV - in a ROI. In particular due to physical limitations of
FDG-PET, lesions less than 3 cm in diameter are prone to significant partial volume
effects (PVE). This leads to errors in the initial value for such lesions, which can then
propagate to even larger errors in the % change calculation.
What about dynamic imaging (c.f. pharmacokinetics)? Analyses of dynamic data sets
may provide better accuracy and more relevant information [8, 18, 19]. For metabolic
changes measured with FDG-PET, the use of SUV may suffice, but, it will likely not be
so for new tracers, or if the initial SUV is low (e.g. tumor to background ratio less than 5
[20]), and any subsequent response is producing only a small absolute change . In
addition to such considerations, recent publications suggest that a change in blood flow
may be a better metric than tracer metabolic rates to predict patient outcome [8].
There are many barriers to dynamic imaging in the clinic: arterial blood sampling,
scanner time plus cost, technical issues for dynamic imaging at multiple axial FOVs (e.g.
for lesions that require various bed positions).
Solutions for arterial sampling may include: (1) image-based measures of input function
from heart or aorta in FOV etc.; (2) population based curves w/ venous sample,
Solutions for scanner time plus cost may include: (1) plan into trial from beginning; (2)
only use for phase 0 or first in human (FIH) studies to determine potential differences in
measured effect sizes between SUV and PET measures determined from dynamic
imaging.
3.2. What is the basic variability in these measures, and what are the sources of
these variations?
Quantitation in PET is relatively well understood, but a major component thereof remains
dose calibrators and long term stability or compensation. In other words, the scanners
are stable [22] unless they are modified in some way such as hardware or software
updates or recalibration or sensor drift.
There are three major categories of sources that might be modality (and task) specific:
a. Patient Factors – motion (e.g. respiration), physiologic function, etc.
b. Imaging Modality - specific factors that vary for PET and PET/CT equipment
and design
c. Image Analysis - image segmentation, registration and calculation methods
Some of the related variable effects are summarized in recent presentations and
conference records (e.g. [22,23]).
Physical sources of error (real, estimated and their correction)
a. Attenuation
b. Scattered and Random coincidences
c. Detector efficiency variations, scanner dead-time
Sources of variability
1. Patent specific
a. Biological changes
b. Lesion size (partial volume effect), location and environment
c. Patient and/or lesion motion
2. Imaging protocol
a. Patient preparation
b. Uptake period and environment
c. Scan protocol: amount of FDG (or other tracer) injected, scan duration,
2D or 3D mode,
3. Processing specific
a. Accuracy of corrections for physical sources of error
b. Reconstruction method
c. Image smoothness vs. resolution tradeoff
d. ROI definition method
e. Standardized Uptake Value (SUV) calculation
One solution to the variability introduced by image reconstruction protocols is to
prescribe extra reconstructions with specific and controlled parameters (e.g. FBP)
whenever quantitation is required.
3.3. Are there any mitigating measures to reduce variation?
a. Attention should be paid to the positioning - i.e. centering in the FOV – of the
lesion to minimize truncation effects
b. Dose calibrator standard – one should be available next month from NIST –
that is cross calibrated with 18F’s different branching ratios and additional Xrays from 68Ge.
c. What about dose calibrator reproducibility? NIST has not seen significant
variability (i.e. < 2%) but there have been reports of up to 5% variability and
results of internal studies will be presented at ENM and SNM.
3.4. Can we separate out actual biological/physiological/pathophysiological
change from variability introduced by the measurement process i.e. perform an
analysis of the components of variance?
Likely yes: it is generally easier to separate out variability than bias. BUT depends on the
details:
1. Patient characteristics (e.g. diabetes, other functional states)
2. Type of scan
a. single PET scan
b. serial PET scans (on same equipment and with same protocol) will tend
to cancel out the effect of reproducible bias on change analysis
c. mixing various scanner types and/or generations (e.g. at large PET
centers or see also d)
d. multi-center studies (clinical trials versus clinical care, esp. at tertiary
centers)
3. Calibration phantoms are important and have a real role for ensuring that
machine and results are maintained within their workable – and reliable range
4. Can we scan a calibration phantom alongside each patient (“a la” QCT)?
5. In general, 3-4 may be more workable with NIST-traceable calibrated sources
3.5. Given the variability in image acquisition and image analysis, what is the
minimum change we can detect using a given modality and image analysis
method, i.e. how large must the “effect” size ultimately be in order to detect a
change in a single individual with significant statistical accuracy?
What we don't know in general:
1. We do not know how the measured changes relate to defined standards such
as the EORTC standards [21]. This can be answered with entirely controlled
virtual simulations and through physical phantom studies. Hybrid simulation
(patient+phantom) methods may also help in this regard.
2. We don't know about coffee-break type variability (e.g. due to a combination of
patient and scanner issues) with extended time between imaging sessions.
Could we use variability of normal tissue variability from patients? Recall Weber
1999 [11] recommended a minimum limits of 20% change in FDG parameter
based on a measured standard deviation of approximately 10% since changes
of more than 20% were outside the observed 95% range for spontaneous
fluctuations for the fifty tumors examined in his study (within restriction of study,
e.g. no Rx therapy and also no change16 patients with less than 10 days
between serial scans). It is important to note that figure 3 in the Weber 1999
article [11] implies study of lesions with different average values of initial FDG
uptake would require different minimum effect sizes.
3.6. How do the answers to question 3.5 change for multicenter clinical trials?
1. Will need a larger effect size or sample size to compensate for the increased
variation
2. Important to identify sources of variation and prospectively control them
3. Introduce calibration phantom as daily QC (could replace doing this with each
patient)
4. Important to repeat with a calibration scan to understand bias, then other
sources of variability will dominate
3.7. Can we identify software tools that can adequately quantify treatment-induced
changes?
1. Scanner vendors have generally still limited tools, e.g. for
visualization/comparison, SUV and ROIs: GE PET VCAR, Philips (?), Siemens
True3D
2. Similar tools are also available from vendors not directly associated with a
specific scanner: Hermes, MimVista, Mirada, Vital
3. Other tools are available through other vendors that may better fit the
quantitation niche: Pmod (kinetic analysis)
4. And finally, there are open source tools that may - or generally may not - fit the
requirements for a full clinical exploitation: e.g. from caBIG (XIP, AIM)
This list is not exhaustive.
3.8. Can we establish standards that will eventually lead to the acceptance of
PET/CT image analysis software by regulatory agencies as surrogate end-points
in new drug applications?
1. Establish benchmarks with databases (RIDER) and known phantoms.
2. Would like to separate acquisition from image analysis, so there is an important
role for digital reference objects and simulation paradigms.
3. RIDER/NCIA could be a source of data but standards definition/certification
needs to be taken up by a standards organization such as
MITA/NEMA/ACR/ABSNM
4. RIDER may not be a large enough dataset.
3.9. Datasets
1. What data is needed in general?
a. Simulation of scanners and phantoms for multiple vendors and images
supporting multiple parameters
b. Serial patient studies with meta-data (disease, therapy, outcomes, ground
truth)
2. What data does RIDER have?
a. Multi-vendor and multi-parameter calibration phantoms
b. Serial patient studies (but without meta-data).
3. What data is in the literature and/or in progress from other efforts?
a. Minn 1995, Weber 1999, Young 1999, Shankar 2006, and Boellaard 2008
4. What additional data is needed?
a. NIST 68Ge calibration data
b. Access to current trials data with meta-data and outcomes. These need to
be accessible publicly and in a useful format.
References
[1] L. K. Shankar, J. M. Hoffman, S. Bacharach, M. M. Graham, J. Karp, A. A. Lammertsma, S.
Larson, D. A. Mankoff, B. A. Siegel, A. Van den Abbeele, J. Yap, and D. Sullivan,
"Consensus recommendations for the use of 18F-FDG PET as an indicator of therapeutic
response in patients in National Cancer Institute Trials," J Nucl Med, vol. 47, pp. 1059-66,
Jun 2006.
[2] M. R. MacManus, R. Hicks, R. Fisher, D. Rischin, M. Michael, A. Wirth, and D. L. Ball,
"FDG-PET-detected extracranial metastasis in patients with non-small cell lung cancer
undergoing staging for surgery or radical radiotherapy--survival correlates with metastatic
disease burden," Acta Oncol, vol. 42, pp. 48-54, 2003.
[3] W. A. Weber, V. Petersen, B. Schmidt, L. Tyndale-Hines, T. Link, C. Peschel, and M.
Schwaiger, "Positron emission tomography in non-small-cell lung cancer: prediction of
response to chemotherapy by quantitative assessment of glucose use," J Clin Oncol, vol.
21, pp. 2651-7, Jul 15 2003.
[4] H. A. Wieder, B. L. Brucher, F. Zimmermann, K. Becker, F. Lordick, A. Beer, M. Schwaiger,
U. Fink, J. R. Siewert, H. J. Stein, and W. A. Weber, "Time course of tumor metabolic
activity during chemoradiotherapy of esophageal squamous cell carcinoma and response
to treatment," J Clin Oncol, vol. 22, pp. 900-8, Mar 1 2004.
[5] N. Avril, S. Sassen, B. Schmalfeldt, J. Naehrig, S. Rutke, W. A. Weber, M. Werner, H.
Graeff, M. Schwaiger, and W. Kuhn, "Prediction of response to neoadjuvant chemotherapy
by sequential F-18-fluorodeoxyglucose positron emission tomography in patients with
advanced-stage ovarian cancer," J Clin Oncol, vol. 23, pp. 7445-53, Oct 20 2005.
[6] F. Cachin, H. M. Prince, A. Hogg, R. E. Ware, and R. J. Hicks, "Powerful prognostic
stratification by [18F]fluorodeoxyglucose positron emission tomography in patients with
metastatic breast cancer treated with high-dose chemotherapy," J Clin Oncol, vol. 24, pp.
3026-31, Jul 1 2006.
[7] F. Lordick, K. Ott, B. J. Krause, W. A. Weber, K. Becker, H. J. Stein, S. Lorenzen, T.
Schuster, H. Wieder, K. Herrmann, R. Bredenkamp, H. Hofler, U. Fink, C. Peschel, M.
Schwaiger, and J. R. Siewert, "PET to assess early metabolic response and to guide
treatment of adenocarcinoma of the oesophagogastric junction: the MUNICON phase II
trial," Lancet Oncol, vol. 8, pp. 797-805, Sep 2007.
[8] L. K. Dunnwald, J. R. Gralow, G. K. Ellis, R. B. Livingston, H. M. Linden, J. M. Specht, R. K.
Doot, T. J. Lawton, W. E. Barlow, B. F. Kurland, E. K. Schubert, and D. A. Mankoff, "Tumor
Metabolism and Blood Flow Changes by Positron Emission Tomography: Relation to
Survival in Patients Treated With Neoadjuvant Chemotherapy for Locally Advanced Breast
Cancer," J Clin Oncol, Jul 14 2008.
[9] M. E. Juweid and B. D. Cheson, "Positron-emission tomography and assessment of cancer
therapy," N Engl J Med, vol. 354, pp. 496-507, Feb 2 2006.
[10] H. Minn, K. R. Zasadny, L. E. Quint, and R. L. Wahl, "Lung cancer: reproducibility of
quantitative measurements for evaluating 2-[F-18]-fluoro-2-deoxy-D-glucose uptake at
PET," Radiology, vol. 196, pp. 167-73, Jul 1995.
[11] W. A. Weber, S. I. Ziegler, R. Thodtmann, A. R. Hanauske, and M. Schwaiger,
"Reproducibility of metabolic measurements in malignant tumors using FDG PET," J Nucl
Med, vol. 40, pp. 1771-7, Nov 1999.
[12] M. Westerterp, J. Pruim, W. Oyen, O. Hoekstra, A. Paans, E. Visser, J. van Lanschot, G.
Sloof, and R. Boellaard, "Quantification of FDG PET studies using standardised uptake
values in multi-centre trials: effects of image reconstruction, resolution and ROI definition
parameters," Eur J Nucl Med Mol Imaging, vol. 34, pp. 392-404, Mar 2007.
[13] R. Boellaard, W. J. Oyen, C. J. Hoekstra, O. S. Hoekstra, E. P. Visser, A. T. Willemsen, B.
Arends, F. J. Verzijlbergen, J. Zijlstra, A. M. Paans, E. F. Comans, and J. Pruim, "The
Netherlands protocol for standardisation and quantification of FDG whole body PET studies
in multi-centre trials," Eur J Nucl Med Mol Imaging, Aug 15 2008.
[14] R. Boellaard, N. C. Krak, O. S. Hoekstra, and A. A. Lammertsma, "Effects of noise, image
resolution, and ROI definition on the accuracy of standard uptake values: a simulation
study," J Nucl Med, vol. 45, pp. 1519-27, Sep 2004.
[15] N. C. Krak, R. Boellaard, O. S. Hoekstra, J. W. Twisk, C. J. Hoekstra, and A. A.
Lammertsma, "Effects of ROI definition and reconstruction method on quantitative outcome
and applicability in a response monitoring trial," Eur J Nucl Med Mol Imaging, vol. 32, pp.
294-301, Mar 2005.
[16] V. Bettinardi, P. Mancosu, M. Danna, G. Giovacchini, C. Landoni, M. Picchio, M. C. Gilardi,
A. Savi, I. Castiglioni, M. Lecchi, and F. Fazio, "Two-dimensional vs three-dimensional
imaging in whole body oncologic PET/CT: a Discovery-STE phantom and patient study," Q
J Nucl Med Mol Imaging, vol. 51, pp. 214-23, Sep 2007.
[17] S. M. Larson, Y. Erdi, T. Akhurst, M. Mazumdar, H. A. Macapinlac, R. D. Finn, C. Casilla,
M. Fazzari, N. Srivastava, H. W. Yeung, J. L. Humm, J. Guillem, R. Downey, M. Karpeh, A.
E. Cohen, and R. Ginsberg, "Tumor Treatment Response Based on Visual and Quantitative
Changes in Global Tumor Glycolysis Using PET-FDG Imaging. The Visual Response Score
and the Change in Total Lesion Glycolysis," Clin Positron Imaging, vol. 2, pp. 159-171, May
1999.
[18] N. M. Freedman, S. K. Sundaram, K. Kurdziel, J. A. Carrasquillo, M. Whatley, J. M. Carson,
D. Sellers, S. K. Libutti, J. C. Yang, and S. L. Bacharach, "Comparison of SUV and Patlak
slope for monitoring of cancer therapy using serial PET scans," Eur J Nucl Med Mol
Imaging, vol. 30, pp. 46-53, Jan 2003.
[19] R. K. Doot, L. K. Dunnwald, E. K. Schubert, M. Muzi, L. M. Peterson, P. E. Kinahan, B. F.
Kurland, and D. A. Mankoff, "Dynamic and static approaches to quantifying 18F-FDG
uptake for measuring cancer response to therapy, including the effect of granulocyte CSF,"
J Nucl Med, vol. 48, pp. 920-5, Jun 2007.
[20] G. M. McDermott, A. Welch, R. T. Staff, F. J. Gilbert, L. Schweiger, S. I. Semple, T. A.
Smith, A. W. Hutcheon, I. D. Miller, I. C. Smith, and S. D. Heys, "Monitoring primary breast
cancer throughout chemotherapy using FDG-PET," Breast Cancer Res Treat, vol. 102, pp.
75-84, Mar 2007.
[21] H. Young, R. Baum, U. Cremerius, K. Herholz, O. Hoekstra, A. A. Lammertsma, J. Pruim,
and P. Price, "Measurement of clinical and subclinical tumour response using [18F]-
fluorodeoxyglucose and positron emission tomography: review and 1999 EORTC
recommendations. European Organization for Research and Treatment of Cancer
(EORTC) PET Study Group," Eur J Cancer, vol. 35, pp. 1773-82, Dec 1999.
[22] Doot RK, Christian PE, Mankoff DA, Kinahan PK. Reproducibility of Quantifying Tracer
Uptake with PET/CT for Evaluation of Response to Therapy. In: 2007 IEEE Nuclear
Science Symposium and Medical Imaging Conference, Honolulu, Hawaii Oct 27 - Nov 3,
Vol. , pp 2880-2884, 2007.
[23] Kinahan PE, Doot RK, Christian PE, Karp JS, Scheuermann JS, Zimmerman RE, Saffer
JR, McEwan AJ. Multi-center comparison of a PET/CT calibration phantom for imaging
trials. Journal of Nuclear Medicine, vol. 49, pp P. (abstract), 2008 

Special Section Guest Editorial: LUNGx Challenge for computerized lung nodule classification: reflections and lessons learned
Challenges, in the context of medical imaging, are valuable in that they allow for a direct comparison of different algorithms designed for a specific radiologic task, with all algorithms abiding by the same set of rules, operating on a common set of images, and being evaluated with a uniform performance assessment paradigm. The variability of system performance based on database composition and subtlety, definition of “truth,” and scoring metric is well-known;1–3 challenges serve to level the differences across these various dimensions. The medical imaging community has hosted a number of successful thoracic imaging challenges that have spanned a wide range of tasks,4,5 including lung nodule detection,6 lung nodule change, vessel segmentation,7 and vessel tree extraction.8 Each challenge presents its own unique set of circumstances and considerations; however, important common themes exist. Future challenge organizers (and participants) could benefit from an open discussion of successes achieved, pitfalls encountered, and lessons learned from each completed challenge.

The LUNGx Challenge, formally known as the SPIE-AAPM-NCI Lung Nodule Classification Challenge, was a collaborative effort sponsored and supported by the International Society for Optics and Photonics (SPIE), American Association of Physicists in Medicine (AAPM), and National Cancer Institute (NCI) along with investigators from University of Chicago, University of Michigan, and Oak Ridge National Laboratory. The Challenge was conducted as part of the SPIE Medical Imaging Symposium held in Orlando, Florida from February 22 to 26, 2015. The focus of the LUNGx Challenge was the computerized classification of lung nodules as benign or malignant in diagnostic computed tomography (CT) scans. This communication provides an overview of the LUNGx Challenge and addresses the “lessons learned” during the conceptualization, conduct, and analysis of the Challenge.

1.Approach
The Challenge included a calibration phase and a testing phase. A calibration set of 10 thoracic CT scans, five with a single confirmed benign nodule and five with a single confirmed malignant nodule, was made available through the NCI’s The Cancer Imaging Archive (TCIA)9 on November 26, 2014. Along with the complete set of DICOM images for these 10 clinical CT scans, a spreadsheet was included that contained the spatial coordinates of the approximate center of each nodule and the diagnosis (the “truth”) for each nodule. All information within the DICOM headers remained intact with the exception of protected health information, which had been removed by the organizers prior to the upload of images to TCIA; this anonymization approach and the public release of the CT scans for this purpose had been approved by the local Institutional Review Board. The nodules in the calibration set (and the test set) had been determined by a radiologist to be either primary lung cancer or benign based on pathologic assessment and/or follow-up imaging examinations. As stated in the Challenge announcement,10 the calibration set was meant to be representative of the technical aspects (e.g., scanner type, acquisition parameters, file format) associated with images in the test set so that participants could become familiar with the image acquisition parameters and DICOM file structure of the one institution (University of Chicago) that supplied the clinical cases for the LUNGx Challenge; participants were not to consider the lung nodules present in the calibration set to be representative of the difficulty level expected in the test set. It is important to note that the calibration set was not intended to serve as a development or training set, since the expectation was that participating groups already would have developed a trained system.

The test set became available through TCIA on January 12, 2015. The test set contained 60 thoracic CT scans with a total of 73 nodules (13 scans contained two nodules each). Along with the complete set of DICOM images for these 60 clinical CT scans, a spreadsheet was included that contained the spatial coordinates of the approximate center of each of these 73 nodules. All information within the DICOM headers remained intact with the exception of protected health information, which had been removed by the organizers prior to the upload of images to TCIA. Participants applied their algorithms to these 73 lung nodules to assign a probability of malignancy to each nodule.

Fifteen sets of results from participants’ algorithms (in the form of a single numeric value estimate of the probability of malignancy for each nodule) were submitted to the organizers by February 6, 2015. With knowledge of the truth, the organizers evaluated the performance of each of these 15 sets of submitted results using receiver operating characteristic (ROC) analysis. Each group that submitted results was invited to prepare a poster for display at the SPIE Medical Imaging Symposium, and the two groups with the best performance (greatest area under the ROC curve) were invited to participate in a panel discussion at the Symposium entitled, “CAD grand challenge: present and future.” In addition, one member of the highest-performing group was awarded complimentary registration to the Symposium.

2.Lessons Learned
2.1.Establishing a Challenge
The organizers of a challenge contribute time, effort, and resources, along with the ability to anticipate unforeseen situations. Organizers must establish the necessary controlled environment that includes a focused, well-vetted set of (clinical) cases on the front end and, on the back end, a performance assessment process that specifies the manner in which participants are to report results to the organizers and the scoring metric through which the results will be evaluated. Essential to the successful implementation of a challenge is a robust infrastructure for case dissemination, communication, and upload of consistently formatted results. The need for communication begins with the initial announcement; appropriate venues for “advertising” an upcoming challenge should be identified early in the planning process. The LUNGx Challenge made use of the SPIE Medical Imaging e-mail distribution list and the Grand Challenges in Biomedical Image Analysis web site.11 During the Challenge, all questions from participants to the organizers were managed through Google Groups so that all participants received the same information and no one participant solely benefitted from clarifying information that might be conveyed in answer to a question.

Despite diligent planning and effort on the part of the organizers, a challenge will not succeed without the active and dedicated participation of groups willing to “accept the challenge.” The phrase “build it and they will come” certainly applies in the setting of challenges; indeed, the burden on organizers is to “build it SO they will come” by offering an activity that participants consider reputable and one they consider worthy of their time, effort, and involvement; also attractive is some type of incentive (e.g., participation on a conference panel or co-authorship on an eventual publication). In the case of the LUNGx Challenge, sponsorship by three major organizations (SPIE, AAPM, and NCI) and affiliation with the SPIE Medical Imaging Symposium provided the legitimacy, while participation in the Symposium along with the potential for complimentary registration provided an incentive. Groups that choose to participate in a challenge deserve much credit for subjecting their algorithms to circumstances that might differ substantially from those under which the algorithms were developed.

2.2.Database Considerations
The single most important component of a medical imaging challenge is the set of images (the “database”); therefore, the effort involved in database collection should not be underestimated or undervalued. The organizers typically need to provide both a set of training cases (or a more limited set of calibration cases as was done for the LUNGx Challenge) and a set of test cases, which leads to the ubiquitous question, “How many cases do we need?” The answer to this question is complicated. The balance between the effort (cost) required to obtain relevant clinical images (which increases with increasing numbers of cases) and the statistical power that may be achieved by the challenge (which also increases with increasing numbers of cases) can be elusive, and often practicality emerges as the deciding factor, with the number of cases being determined by the (limited) volume of cases available to the organizers.

Collecting cases for a challenge requires consideration of a number of factors that impose constraints on the selected cases, thus increasing the burden of database collection—but with the expected benefit of a more scientifically or clinically relevant challenge. Two general aspects of the collected cases must be considered: (1) the distribution of image-acquisition parameters represented by the images and (2) the range of disease states or anatomic variation captured by the images. The LUNGx Challenge used clinical cases from one institution to minimize the inherent variability of scan technical parameters, although other challenges could benefit from the greater heterogeneity in imaging parameters and patient demographics captured by cases from multiple institutions. The organizers must determine the level of consistency across all image-acquisition parameters that is required by the challenge task; for some challenges, a specific image reconstruction algorithm, pixel size, section thickness, or contrast-enhancement protocol might be desired, while for other challenges, a clinically realistic distribution of these parameters could be more appropriate. For challenges that involve evaluation of abnormalities, the range of lesion size and subtlety is an important consideration, and the distributions of factors such as gender and age should not be overlooked, since, depending on the challenge task, these factors can have a substantial impact on the results. If the challenge task is one of discrimination between two conditions, gender and/or age matching between groups might be necessary. The nodules between the two groups in the LUNGx Challenge test set were size matched (although this fact was not disclosed to participants), since nodule size is a well-known predictor of malignancy; had size matching not been implemented, participants potentially could have achieved a high level of performance simply by calculating some metric of nodule size alone.12 Organizers should verify that data available in the DICOM image headers or in any supplemental documentation does not incidentally yield a high performance in the specified task. Other clinical factors might be relevant to the challenge task, such as smoking history, race, or genetic information, and the organizers must decide whether such additional clinical or demographic data is essential to the challenge. For the LUNGx Challenge, some participants may have reasonably desired smoking history, the distribution of cell type among the malignant nodules, or the processes represented by the benign nodules, but this information was not provided.

2.3.Clarity of Challenge Rules
Based on their own preconceptions and extensive experiences in the field, challenge organizers likely will have certain expectations with regard to the manner in which participants should approach the challenge task; these expectations, without a doubt, will be unwittingly violated by some participants who come from different technical backgrounds/cultures or who have a different interpretation of the challenge rules or even the fundamental task that the challenge seeks to address. In the LUNGx Challenge, for example, the 10 calibration cases were intended to assist groups evaluate the compatibility of the Challenge cases with their algorithms and were not intended for algorithm development or for classifier training; nevertheless, some groups attempted to use this intentionally small set of calibration cases for development or training. As another example, despite the expectation that the nodule classification systems would be fully computerized with no human involvement, some groups sought out and incorporated local radiologist input: radiologist-constructed nodule outlines, radiologist semantic characterization of nodules, and radiologist ratings of nodule malignancy were used as input to the systems that some participants applied to the Challenge test set (the first two uses of human involvement were ruled to be acceptable—although unexpected—systems, while the system that used radiologist malignancy ratings was withdrawn). The rules of a challenge must be crafted as completely, as clearly, and as logically as possible, with the organizers attempting to anticipate any confusion and misinterpretations the rules might cause. After numerous inquiries from groups working with the LUNGx calibration set, the Challenge rules were expanded to explicitly define the spatial coordinate system conventionally used for CT scan images and to describe basic elements of the DICOM file format.

Any group that downloaded cases while the challenge was active should be allowed to retain those images; the alternative (i.e., requesting that groups delete all downloaded data at the conclusion of the challenge) is impractical and unenforceable. Accordingly, plans for the continued public availability of challenge cases should be made, since cases from a challenge can provide a valuable resource. With this in mind, the LUNGx Challenge instructions stated that “anyone wishing to use the downloaded images for presentation or publication purposes outside of the LUNGx Challenge should acknowledge the SPIE, the NCI, the AAPM, and the University of Chicago. The LUNGx Challenge cases and associated data may be downloaded from Ref. 13. One lingering issue with the LUNGx Challenge going forward is whether the truth information for the test cases will be made public; knowledge of the nodule diagnosis of each test case would greatly enhance the value of these cases as a resource for medical imaging researchers, but disclosure of this information would exclude the potential incorporation of these test cases in a future challenge. Digital object identifiers (DOIs) have received growing interest as a more permanent mechanism through which to foster reproducible imaging research;14 a recent lung segmentation challenge conducted by the Quantitative Imaging Network made use of the DOI approach to maintain challenge data for future use.15

Participants should be made aware of their potential involvement in any presentation or publication expected to result from a challenge. The ability of participants to remain anonymous in a publication should be stated in the challenge rules. Anonymity could take one of two general forms: (1) complete omission of the names and affiliations of participating groups or (2) a listing of participants that remains disassociated from the results of individual systems. A publication preferably should report the general methodology of each system linked with the performance results of that system to convey the relative merits of different approaches to the challenge task; under this approach, however, it may be impractical to maintain separation of participants’ identities from their methods, and hence their results. Ultimately, full disclosure, full credit, and full responsibility are always preferred in a scientific communication—an argument that favors no level of anonymity at all.

2.4.Participant Responsibility
Groups that choose to participate in a challenge have a responsibility to approach the challenge with commitment and scientific rigor. Participants must adhere to the rules of the challenge, which should be clearly specified by the organizers. The point of commitment also should be specified. Download of the cases should not commit a group to participation in the challenge; once downloaded and assessed, the challenge images may be determined by a group to comprise, for example, technical parameters or concomitant disease for which the group’s system was not designed. With submission of its system’s output to the challenge organizers for evaluation, however, a group becomes a fully vested challenge participant and should accept the final results and performance analysis of the organizers. The spirit of a challenge is compromised (and resources of the organizers are wasted) if groups are allowed to withdraw their participation if they find their system’s performance is not to their satisfaction. Participants must be mindful of the educational, friendly competition, and community-building nature of a challenge.

To summarize, the LUNGx Challenge was a successful scientific challenge for the computerized classification of lung nodules on CT scans. Despite careful planning and an attempt to think through potential pitfalls, some aspects of the Challenge yielded unexpected outcomes, but these situations provide valuable lessons for members of the medical imaging community who would organize (and participate in) future challenges. A scientific paper analyzing the individual and collective performances of algorithms submitted to the Challenge, along with ancillary analyses on clinically relevant subsets of the LUNGx database and radiologists’ classification performance, is being prepared. The Challenge was given a prominent role in the CAD Conference of the 2015 SPIE Medical Imaging Symposium. Conference organizers plan to leverage the success of the LUNGx Challenge into a related challenge to be conducted in association with SPIE Medical Imaging 2016.

Acknowledgments
The authors would like to express their appreciation to Angela Keyser from the AAPM and Diane Cline and Sandy Hoelterhoff from the SPIE for their assistance in making this challenge a reality. The LUNGx Challenge was supported by the SPIE, NCI, and AAPM. This project has been funded in whole or in part with Federal funds from the National Cancer Institute, National Institutes of Health, under Contract No. HHSN261200800001E. The content of this publication does not necessarily reflect the views or policies of the Department of Health and Human Services, nor does mention of trade names, commercial products, or organizations imply endorsement by the U.S. Government. S.G.A. and M.L.G. receive royalties and licensing fees through The University of Chicago related to computer-aided diagnosis. This manuscript has been authored in part by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a nonexclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan.16

References
1. R. M. Nishikawa et al., “Effect of case selection on the performance of computer‐aided detection schemes,” Med. Phys., 21 265 –269 (1994). http://dx.doi.org/10.1118/1.597287 Google Scholar2. R. M. Nishikawa et al., “Variations in measured performance of CAD schemes due to database composition and scoring protocol,” Proc SPIE, 3338 840 –844 (1998). http://dx.doi.org/10.1117/12.310894 Google Scholar3. G. Revesz et al., “The effect of verification on the assessment of imaging techniques,” Invest. Rad., 18 194 –198 (1983). http://dx.doi.org/10.1097/00004424-198303000-00018 INVRAV 0020-9996 Google Scholar4.  http://grand-challenge.org/Why_Challenges/ Google Scholar5. K. Murphy, “Development and evaluation of automated image analysis techniques in thoracic CT,” (2011). Google Scholar6. B. van Ginneken et al., “Comparing and combining algorithms for computer-aided detection of pulmonary nodules in computed tomography scans: the ANODE09 study,” Med. Image Anal., 14 707 –22 (2010). http://dx.doi.org/10.1016/j.media.2010.05.005 Google Scholar7. R. D. Rudyanto et al., “Comparing algorithms for automated vessel segmentation in computed tomography scans of the lung: the VESSEL12 study,” Med. Image Anal., 18 1217 –32 (2014). http://dx.doi.org/10.1016/j.media.2014.07.003 Google Scholar8. P. Lo et al., “Extraction of airways from CT (EXACT’09),” IEEE Trans. Med. Imaging, 31 2093 –2107 (2012). http://dx.doi.org/10.1109/TMI.2012.2209674 ITMID4 0278-0062 Google Scholar9. K. Clark et al., “The Cancer Imaging Archive (TCIA): maintaining and operating a public information repository,” J. Digital Imaging, 26 1045 –1057 (2013). http://dx.doi.org/10.1007/s10278-013-9622-7 JDIMEW Google Scholar10. . http://spie.org/Documents/ConferencesExhibitions/MI/LUNGxChallengeFormat.pdf Google Scholar11. . http://grand-challenge.org/ Google Scholar12. A. C. Jirapatnakul et al., “Characterization of pulmonary nodules: effects of size and feature type on reported performance,” Proc. SPIE, 6915 69151E (2008). http://dx.doi.org/10.1117/12.770514 Google Scholar13. S. G. ArmatoIII et al., “SPIE-AAPM-NCI lung nodule classification challenge dataset,” Cancer Imaging Arch., (2015). http://dx.doi.org/10.7937/K9/TCIA.2015.UZLSU3FL Google Scholar14. P. E. Bourne, “DOIs for DICOM raw images: enabling science reproducibility,” Radiology, 275 3 –4 (2015). http://dx.doi.org/10.1148/radiol.15150144 RADLAX 0033-8419 Google Scholar15. J. Kalpathy-Cramer et al., “QIN multi-site collection of Lung CT data with nodule segmentations,” Cancer Imaging Arch., (2015). http://dx.doi.org/10.7937/K9/TCIA.2015.1BUVFJR7 Google Scholar16. . http://energy.gov/downloads/doe-public-access-plan Google Scholar